

══════════════════════════════════════════════════════════════
 FILE: core/extension.rs
══════════════════════════════════════════════════════════════

use super::types::*;
use anyhow::{bail, Result};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;

/// Compute the sequential hash chain, collecting checkpoints along the way.
/// Used by both create_extension and mine_extension.
fn compute_chain(midstate: &[u8; 32], nonce: u64) -> ([u8; 32], Vec<[u8; 32]>) {
    let mut x = hash_concat(midstate, &nonce.to_le_bytes());
    let mut checkpoints = Vec::with_capacity((EXTENSION_ITERATIONS / CHECKPOINT_INTERVAL) as usize + 1);
    checkpoints.push(x);

    for i in 1..=EXTENSION_ITERATIONS {
        x = hash(&x);
        if i % CHECKPOINT_INTERVAL == 0 {
            checkpoints.push(x);
        }
    }

    (x, checkpoints)
}

/// Derive which segments to spot-check from the final hash.
/// Deterministic: all nodes check the same segments for the same block.
/// Unpredictable: attacker must complete the full chain to learn which are checked.
fn spot_check_indices(final_hash: &[u8; 32], num_segments: usize, count: usize) -> Vec<usize> {
    let count = count.min(num_segments);
    let mut indices = Vec::with_capacity(count);
    let mut seed = *final_hash;

    while indices.len() < count {
        seed = hash(&seed);
        let raw = u64::from_le_bytes(seed[..8].try_into().unwrap());
        let idx = (raw as usize) % num_segments;
        if !indices.contains(&idx) {
            indices.push(idx);
        }
    }

    indices
}

/// Create an extension by doing sequential work
pub fn create_extension(midstate: [u8; 32], nonce: u64) -> Extension {
    let (final_hash, checkpoints) = compute_chain(&midstate, nonce);
    Extension { nonce, final_hash, checkpoints }
}

/// Verify an extension by spot-checking random checkpoint segments.
/// Cost: O(SPOT_CHECK_COUNT * CHECKPOINT_INTERVAL) instead of O(EXTENSION_ITERATIONS).
///
/// When checkpoints have been pruned (empty vec), falls back to full-chain
/// recomputation: O(EXTENSION_ITERATIONS). This is ~1ms per block with BLAKE3
/// and happens only for deeply finalized historical blocks during sync.
pub fn verify_extension(midstate: [u8; 32], ext: &Extension, target: &[u8; 32]) -> Result<()> {
    // 1. Difficulty check (instant)
    if ext.final_hash >= *target {
        bail!("Extension doesn't meet difficulty target");
    }

    // 2. Pruned checkpoints — full-chain recomputation
    if ext.checkpoints.is_empty() {
        return verify_extension_full(midstate, ext);
    }

    let num_segments = (EXTENSION_ITERATIONS / CHECKPOINT_INTERVAL) as usize;
    let expected_checkpoints = num_segments + 1;

    // 3. Structural check
    if ext.checkpoints.len() != expected_checkpoints {
        bail!(
            "Wrong checkpoint count: got {}, expected {}",
            ext.checkpoints.len(),
            expected_checkpoints
        );
    }

    // 4. First checkpoint must match midstate + nonce
    let expected_start = hash_concat(&midstate, &ext.nonce.to_le_bytes());
    
    // --- LOGGING START ---
    if ext.checkpoints[0] != expected_start {
        tracing::error!("VERIFY ERROR DEBUG:");
        tracing::error!("  Input Midstate: {}", hex::encode(midstate));
        tracing::error!("  Nonce: {}", ext.nonce);
        tracing::error!("  Expected Checkpoint[0] (hash(midstate+nonce)): {}", hex::encode(expected_start));
        tracing::error!("  Actual Extension Checkpoint[0]: {}", hex::encode(ext.checkpoints[0]));
    }
    // --- LOGGING END ---

    if ext.checkpoints[0] != expected_start {
        bail!("First checkpoint doesn't match midstate+nonce");
    }

    // 5. Last checkpoint must equal final_hash
    if ext.checkpoints[num_segments] != ext.final_hash {
        bail!("Last checkpoint doesn't match final_hash");
    }

    // 6. Spot-check segments
    let indices = spot_check_indices(&ext.final_hash, num_segments, SPOT_CHECK_COUNT);

    for seg in indices {
        let mut x = ext.checkpoints[seg];
        for _ in 0..CHECKPOINT_INTERVAL {
            x = hash(&x);
        }
        if x != ext.checkpoints[seg + 1] {
            bail!("Checkpoint verification failed at segment {}", seg);
        }
    }

    Ok(())
}

/// Full-chain recomputation for pruned extensions.
///
/// Recomputes the entire sequential hash chain from `hash(midstate || nonce)`
/// through EXTENSION_ITERATIONS steps and verifies the result matches `final_hash`.
/// Cost: O(EXTENSION_ITERATIONS) ≈ 1M BLAKE3 hashes ≈ 1ms.
fn verify_extension_full(midstate: [u8; 32], ext: &Extension) -> Result<()> {
    let mut x = hash_concat(&midstate, &ext.nonce.to_le_bytes());
    for _ in 0..EXTENSION_ITERATIONS {
        x = hash(&x);
    }
    if x != ext.final_hash {
        bail!("Full-chain verification failed: recomputed hash != final_hash");
    }
    Ok(())
}

/// Mine: try nonces until one produces a final_hash below target.
/// Spawns one worker per available core, each trying independent nonces.
/// Uses an AtomicBool to instantly abort if a peer solves the block first.
pub fn mine_extension(midstate: [u8; 32], target: [u8; 32], cancel: Arc<AtomicBool>) -> Option<Extension> {
    let num_threads = std::thread::available_parallelism()
        .map(|n| n.get())
        .unwrap_or(1);

    if num_threads <= 1 {
        return mine_extension_single(midstate, target, cancel);
    }

    let found = Arc::new(AtomicBool::new(false));
    let (tx, rx) = std::sync::mpsc::channel::<(Extension, u64)>();

    let threads: Vec<_> = (0..num_threads)
        .map(|_| {
            let cancel = Arc::clone(&cancel);
            let found = Arc::clone(&found);
            let tx = tx.clone();
            std::thread::spawn(move || {
                let mut attempts = 0u64;
                loop {
                    if cancel.load(Ordering::Relaxed) || found.load(Ordering::Relaxed) {
                        return;
                    }

                    attempts += 1;
                    let nonce: u64 = rand::random();
                    let (final_hash, checkpoints) = compute_chain(&midstate, nonce);

                    if final_hash < target {
                        found.store(true, Ordering::Relaxed);
                        let _ = tx.send((Extension { nonce, final_hash, checkpoints }, attempts));
                        return;
                    }
                }
            })
        })
        .collect();

    // Drop our copy so rx terminates when all threads finish
    drop(tx);

    let result = rx.recv().ok();

    // Ensure all threads exit before returning
    for t in threads {
        let _ = t.join();
    }

    if let Some((ext, attempts)) = result {
        tracing::info!(
            "Found valid extension! nonce={} attempts={} hash={} threads={}",
            ext.nonce, attempts, hex::encode(ext.final_hash), num_threads
        );
        Some(ext)
    } else {
        tracing::debug!("Mining cancelled after all threads exited ({} threads)", num_threads);
        None
    }
}

/// Single-threaded fallback.
fn mine_extension_single(midstate: [u8; 32], target: [u8; 32], cancel: Arc<AtomicBool>) -> Option<Extension> {
    let mut attempts = 0u64;

    loop {
        if cancel.load(Ordering::Relaxed) {
            tracing::debug!("Mining cancelled by network event after {} attempts", attempts);
            return None;
        }

        attempts += 1;
        let nonce: u64 = rand::random();

        let (final_hash, checkpoints) = compute_chain(&midstate, nonce);

        if final_hash < target {
            tracing::info!(
                "Found valid extension! nonce={} attempts={} hash={}",
                nonce, attempts, hex::encode(final_hash)
            );
            return Some(Extension { nonce, final_hash, checkpoints });
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn easy_target() -> [u8; 32] {
        [0xff; 32] // accepts everything
    }

    // ── create_extension ────────────────────────────────────────────────

    #[test]
    fn create_extension_deterministic() {
        let ms = hash(b"test midstate");
        let e1 = create_extension(ms, 42);
        let e2 = create_extension(ms, 42);
        assert_eq!(e1.final_hash, e2.final_hash);
        assert_eq!(e1.checkpoints, e2.checkpoints);
    }

    #[test]
    fn create_extension_different_nonces_differ() {
        let ms = hash(b"test midstate");
        let e1 = create_extension(ms, 0);
        let e2 = create_extension(ms, 1);
        assert_ne!(e1.final_hash, e2.final_hash);
    }

    #[test]
    fn create_extension_checkpoint_count() {
        let ms = hash(b"test midstate");
        let ext = create_extension(ms, 0);
        let expected = (EXTENSION_ITERATIONS / CHECKPOINT_INTERVAL) as usize + 1;
        assert_eq!(ext.checkpoints.len(), expected);
    }

    #[test]
    fn create_extension_first_checkpoint_is_hash_of_midstate_nonce() {
        let ms = hash(b"test midstate");
        let nonce = 99u64;
        let ext = create_extension(ms, nonce);
        let expected = hash_concat(&ms, &nonce.to_le_bytes());
        assert_eq!(ext.checkpoints[0], expected);
    }

    #[test]
    fn create_extension_last_checkpoint_equals_final_hash() {
        let ms = hash(b"test midstate");
        let ext = create_extension(ms, 0);
        assert_eq!(*ext.checkpoints.last().unwrap(), ext.final_hash);
    }

    // ── verify_extension ────────────────────────────────────────────────

    #[test]
    fn verify_valid_extension() {
        let ms = hash(b"verify test");
        let ext = create_extension(ms, 7);
        assert!(verify_extension(ms, &ext, &easy_target()).is_ok());
    }

    #[test]
    fn verify_rejects_wrong_midstate() {
        let ms = hash(b"correct");
        let ext = create_extension(ms, 0);
        let wrong_ms = hash(b"wrong");
        assert!(verify_extension(wrong_ms, &ext, &easy_target()).is_err());
    }

    #[test]
    fn verify_rejects_above_target() {
        let ms = hash(b"target test");
        let ext = create_extension(ms, 0);
        let impossible_target = [0u8; 32]; // nothing can be below all zeros
        assert!(verify_extension(ms, &ext, &impossible_target).is_err());
    }

    #[test]
    fn verify_rejects_wrong_checkpoint_count() {
        let ms = hash(b"bad checkpoint");
        let mut ext = create_extension(ms, 0);
        ext.checkpoints.push([0u8; 32]); // extra checkpoint
        assert!(verify_extension(ms, &ext, &easy_target()).is_err());
    }

    #[test]
    fn verify_rejects_tampered_checkpoint() {
        let ms = hash(b"tamper test");
        let mut ext = create_extension(ms, 0);
        // Flip a byte in a middle checkpoint
        let mid = ext.checkpoints.len() / 2;
        ext.checkpoints[mid][0] ^= 0xFF;
        // This may or may not be caught depending on spot-check sampling,
        // but the last checkpoint won't match final_hash anymore
        // OR a spot-checked segment will fail.
        // At minimum, if the last checkpoint is tampered it's caught:
        let last = ext.checkpoints.len() - 1;
        let mut ext2 = create_extension(ms, 0);
        ext2.checkpoints[last][0] ^= 0xFF;
        assert!(verify_extension(ms, &ext2, &easy_target()).is_err());
    }

    #[test]
    fn verify_rejects_tampered_final_hash() {
        let ms = hash(b"final hash tamper");
        let mut ext = create_extension(ms, 0);
        ext.final_hash[0] ^= 0xFF;
        assert!(verify_extension(ms, &ext, &easy_target()).is_err());
    }

    // ── verify_extension (pruned / full-chain fallback) ─────────────────

    #[test]
    fn verify_pruned_extension_valid() {
        let ms = hash(b"prune test");
        let ext = create_extension(ms, 7);
        let pruned = Extension {
            nonce: ext.nonce,
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        assert!(verify_extension(ms, &pruned, &easy_target()).is_ok());
    }

    #[test]
    fn verify_pruned_extension_wrong_midstate() {
        let ms = hash(b"prune correct");
        let ext = create_extension(ms, 0);
        let pruned = Extension {
            nonce: ext.nonce,
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        let wrong = hash(b"prune wrong");
        assert!(verify_extension(wrong, &pruned, &easy_target()).is_err());
    }

    #[test]
    fn verify_pruned_extension_wrong_nonce() {
        let ms = hash(b"prune nonce");
        let ext = create_extension(ms, 42);
        let pruned = Extension {
            nonce: 43, // wrong nonce
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        assert!(verify_extension(ms, &pruned, &easy_target()).is_err());
    }

    #[test]
    fn verify_pruned_extension_tampered_final_hash() {
        let ms = hash(b"prune tamper");
        let ext = create_extension(ms, 0);
        let mut pruned = Extension {
            nonce: ext.nonce,
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        pruned.final_hash[0] ^= 0xFF;
        assert!(verify_extension(ms, &pruned, &easy_target()).is_err());
    }

    #[test]
    fn verify_pruned_extension_still_checks_target() {
        let ms = hash(b"prune target");
        let ext = create_extension(ms, 0);
        let pruned = Extension {
            nonce: ext.nonce,
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        let impossible_target = [0u8; 32];
        assert!(verify_extension(ms, &pruned, &impossible_target).is_err());
    }

    #[test]
    fn verify_pruned_matches_full_verification() {
        // A pruned extension should accept/reject identically to the full one
        let ms = hash(b"equivalence test");
        let ext = create_extension(ms, 99);
        let pruned = Extension {
            nonce: ext.nonce,
            final_hash: ext.final_hash,
            checkpoints: vec![],
        };
        let full_ok = verify_extension(ms, &ext, &easy_target()).is_ok();
        let pruned_ok = verify_extension(ms, &pruned, &easy_target()).is_ok();
        assert_eq!(full_ok, pruned_ok);
    }

    // ── mine_extension (use fast-mining feature for test speed) ─────────

    #[test]
    fn mine_extension_meets_target() {
        let ms = hash(b"mine test");
        // Use easy target so mining finishes quickly
        let target = easy_target();
        let cancel = Arc::new(AtomicBool::new(false));
        let ext = mine_extension(ms, target, cancel).unwrap();
        assert!(ext.final_hash < target);
        assert!(verify_extension(ms, &ext, &target).is_ok());
    }

    // ── spot_check_indices ──────────────────────────────────────────────

    #[test]
    fn spot_check_indices_deterministic() {
        let fh = hash(b"deterministic");
        let a = spot_check_indices(&fh, 100, 10);
        let b = spot_check_indices(&fh, 100, 10);
        assert_eq!(a, b);
    }

    #[test]
    fn spot_check_indices_unique() {
        let fh = hash(b"unique check");
        let indices = spot_check_indices(&fh, 1000, 50);
        let mut deduped = indices.clone();
        deduped.sort();
        deduped.dedup();
        assert_eq!(indices.len(), deduped.len());
    }

    #[test]
    fn spot_check_indices_within_bounds() {
        let fh = hash(b"bounds");
        let num_segments = 100;
        let indices = spot_check_indices(&fh, num_segments, 20);
        for &idx in &indices {
            assert!(idx < num_segments);
        }
    }

    #[test]
    fn spot_check_count_capped_at_segments() {
        let fh = hash(b"cap");
        let indices = spot_check_indices(&fh, 5, 100);
        assert_eq!(indices.len(), 5);
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/finality.rs
══════════════════════════════════════════════════════════════

//! Bayesian Finality Estimation using a Beta-Binomial Model.

/// Dynamically estimates the safe re-organization depth using a Beta-Binomial model.
///
/// The probability of a successful deep reorg of length `z` by an attacker with
/// hashpower proportion `q` (and honest proportion `p`) is `(q/p)^z`. We model our 
/// uncertainty about `p` using a Beta distribution updated by network observations.
#[derive(Clone, Debug)]
pub struct FinalityEstimator {
    pub alpha: u64,
    pub beta: u64,
}

impl FinalityEstimator {
    /// Creates a new estimator with a prior belief. 
    ///
    /// # Examples
    /// ```
    /// use midstate::core::finality::FinalityEstimator;
    /// let estimator = FinalityEstimator::new(10, 2);
    /// assert_eq!(estimator.alpha, 10);
    /// ```
    pub fn new(alpha: u64, beta: u64) -> Self {
        Self { alpha, beta }
    }

    /// Records an honest block observation (e.g., linear chain extension).
    /// Caps at 10,000 to maintain agility and prevent overflow.
    ///
    /// # Examples
    /// ```
    /// use midstate::core::finality::FinalityEstimator;
    /// let mut est = FinalityEstimator::new(10, 2);
    /// est.observe_honest();
    /// assert_eq!(est.alpha, 11);
    /// ```
    pub fn observe_honest(&mut self) {
        self.alpha = self.alpha.saturating_add(1).min(10_000);
    }

    /// Records an adversarial observation (e.g., deep fork or orphaned block).
    ///
    /// # Examples
    /// ```
    /// use midstate::core::finality::FinalityEstimator;
    /// let mut est = FinalityEstimator::new(10, 2);
    /// est.observe_adversarial();
    /// assert_eq!(est.beta, 3);
    /// ```
    pub fn observe_adversarial(&mut self) {
        self.beta = self.beta.saturating_add(1).min(10_000);
    }

    /// Calculates the required block depth `z` to ensure the expected probability
    /// of a successful reorg is below `target_risk`.
    ///
    /// # Examples
    /// ```
    /// use midstate::core::finality::FinalityEstimator;
    /// let mut est = FinalityEstimator::new(10, 2);
    /// 
    /// // Initially requires a high depth for 1 in a million risk
    /// let safe_initial = est.calculate_safe_depth(1e-6);
    /// 
    /// // After observing consistent honest network behavior, required depth drops
    /// for _ in 0..100 { est.observe_honest(); }
    /// let safe_later = est.calculate_safe_depth(1e-6);
    /// 
    /// assert!(safe_later < safe_initial);
    /// ```
    pub fn calculate_safe_depth(&self, target_risk: f64) -> u64 {
        const STEPS: usize = 1000;
        let mut log_weights = vec![0.0; STEPS];
        let mut max_log_w = f64::NEG_INFINITY;
        
        // 1. Calculate unnormalized log weights of the Beta PDF
        for i in 1..STEPS {
            let p = i as f64 / STEPS as f64;
            let log_w = (self.alpha as f64 - 1.0) * p.ln() + (self.beta as f64 - 1.0) * (1.0 - p).ln();
            log_weights[i] = log_w;
            if log_w > max_log_w {
                max_log_w = log_w;
            }
        }

        // 2. Normalize using Log-Sum-Exp to prevent float underflow
        let mut weights = vec![0.0; STEPS];
        let mut total_weight = 0.0;
        for i in 1..STEPS {
            let w = (log_weights[i] - max_log_w).exp();
            weights[i] = w;
            total_weight += w;
        }

        // 3. Find the lowest depth `z` that satisfies the target risk
        let mut z = 1;
        loop {
            let mut expected_risk = 0.0;
            for i in 1..STEPS {
                let p = i as f64 / STEPS as f64;
                let w = weights[i] / total_weight;
                
                let catchup_prob = if p > 0.5 {
                    ((1.0 - p) / p).powi(z as i32)
                } else {
                    1.0 // If attacker has > 50%, catchup is guaranteed
                };
                
                expected_risk += catchup_prob * w;
            }

            if expected_risk <= target_risk || z >= 10_000 {
                return z;
            }
            z += 1;
        }
    }
}
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_initialization() {
        let est = FinalityEstimator::new(15, 5);
        assert_eq!(est.alpha, 15);
        assert_eq!(est.beta, 5);
    }

    #[test]
    fn test_observe_caps() {
        let mut est = FinalityEstimator::new(9999, 9999);
        
        // Should cap at 10,000
        est.observe_honest();
        est.observe_honest();
        assert_eq!(est.alpha, 10_000);

        est.observe_adversarial();
        est.observe_adversarial();
        assert_eq!(est.beta, 10_000);
    }

    #[test]
    fn test_safe_depth_decreases_with_honesty() {
        let mut est = FinalityEstimator::new(10, 2);
        let initial_depth = est.calculate_safe_depth(1e-6);

        // Accumulate a track record of honest network behavior
        for _ in 0..100 {
            est.observe_honest();
        }
        
        let later_depth = est.calculate_safe_depth(1e-6);
        // Greater confidence means we require fewer blocks for the same risk tolerance
        assert!(later_depth < initial_depth);
    }

    #[test]
    fn test_safe_depth_increases_with_adversity() {
        let mut est = FinalityEstimator::new(100, 2);
        let initial_depth = est.calculate_safe_depth(1e-6);

        // Network starts experiencing forks/orphans
        for _ in 0..10 {
            est.observe_adversarial();
        }
        
        let later_depth = est.calculate_safe_depth(1e-6);
        // Lower confidence means we must wait longer to achieve the same risk tolerance
        assert!(later_depth > initial_depth);
    }

    #[test]
    fn test_extreme_adversarial_hits_cap() {
        // If the prior strongly suggests the attacker has the majority of the hash power (beta > alpha),
        // the catchup probability is effectively 1.0. It should hit the 10,000 safety cap.
        let est = FinalityEstimator::new(2, 50); 
        let depth = est.calculate_safe_depth(1e-6);
        assert_eq!(depth, 10_000);
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/mmr.rs
══════════════════════════════════════════════════════════════

//! # Merkle Mountain Range (MMR) + UTXO Accumulator
//!
//! Two structures for replacing the `HashSet<[u8; 32]>` coin set:
//!
//! 1. **MMR** — append-only log of state transitions for light-client proofs.
//!    O(1) amortised append, O(log n) inclusion proofs, O(log n) root via peaks.
//!
//! 2. **UtxoAccumulator** — Merkle-committed mutable UTXO set.
//!    Sorted-vec backed today (O(n) insert/remove, trivially correct).
//!    Drop-in replacement: swap `State.coins: HashSet` → `UtxoAccumulator`,
//!    keep the same `.contains()` / `.insert()` / `.remove()` API.
//!    For millions of coins, swap internals for a Sparse Merkle Tree or Utreexo.

use super::types::hash_concat;
use anyhow::{bail, Result};
use serde::{Deserialize, Serialize};

// ═══════════════════════════════════════════════════════════════════════════
//  MMR
// ═══════════════════════════════════════════════════════════════════════════

/// Height of the node at MMR position `pos` (0-indexed).
/// Leaf = height 0.
fn pos_height(mut pos: u64) -> u32 {
    // Find the height of the smallest perfect tree that contains 'pos'
    let mut h = 0;
    while pos >= (1 << (h + 1)) - 1 {
        h += 1;
    }
    
    // We iterate down from the top of that tree to find the height of 'pos'
    let mut cur_h = h;
    let mut cur_size = (1 << (cur_h + 1)) - 1;
    
    loop {
        // If pos is the root of the current subtree, return its height
        if pos == cur_size - 1 {
            return cur_h;
        }
        
        // Otherwise, descend
        cur_h -= 1;
        let left_size = (1 << (cur_h + 1)) - 1;
        
        // If pos is in the right child, shift it relative to the right child
        if pos >= left_size {
            pos -= left_size;
        }
        // If pos is in the left child, we just process it with the reduced height
        
        cur_size = left_size;
    }
}

/// Total nodes in an MMR with `n` leaves: `2n − popcount(n)`.
pub fn mmr_size(n: u64) -> u64 {
    if n == 0 { 0 } else { 2 * n - (n.count_ones() as u64) }
}

/// Peak positions in an MMR of `size` nodes.
pub fn peaks(size: u64) -> Vec<u64> {
    let mut result = Vec::new();
    let mut remaining = size;
    let mut offset = 0u64;

    while remaining > 0 {
        let mut h = 1u32;
        while (1u64 << (h + 1)) - 1 <= remaining {
            h += 1;
        }
        let tree_size = (1u64 << h) - 1;
        if tree_size > remaining { break; }
        result.push(offset + tree_size - 1);
        offset += tree_size;
        remaining -= tree_size;
    }
    result
}

/// A Merkle Mountain Range backed by a flat vec of hashes.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MerkleMountainRange {
    nodes: Vec<[u8; 32]>,
    leaf_count: u64,
}

impl MerkleMountainRange {
    pub fn new() -> Self {
        Self { nodes: Vec::new(), leaf_count: 0 }
    }

    pub fn leaf_count(&self) -> u64 { self.leaf_count }
    pub fn size(&self) -> u64 { self.nodes.len() as u64 }

    /// Append a leaf, auto-merging complete pairs. Returns its MMR position.
    pub fn append(&mut self, leaf_hash: &[u8; 32]) -> u64 {
        let pos = self.nodes.len() as u64;
        self.nodes.push(*leaf_hash);
        self.leaf_count += 1;

        let mut current_pos = pos;
        let mut current_height = 0u32;

        loop {
            let left_sibling_size = (1u64 << (current_height + 1)) - 1;
            if current_pos < left_sibling_size { break; }
            let left_pos = current_pos - left_sibling_size;
            if pos_height(left_pos) != current_height { break; }

            let parent_hash = hash_concat(
                &self.nodes[left_pos as usize],
                &self.nodes[current_pos as usize],
            );
            let parent_pos = self.nodes.len() as u64;
            self.nodes.push(parent_hash);
            current_pos = parent_pos;
            current_height += 1;
        }
        pos
    }

    /// Bag peaks right-to-left into a single root.
    pub fn root(&self) -> [u8; 32] {
        let peak_positions = peaks(self.nodes.len() as u64);
        if peak_positions.is_empty() { return [0u8; 32]; }

        let mut root = self.nodes[*peak_positions.last().unwrap() as usize];
        for &pos in peak_positions.iter().rev().skip(1) {
            root = hash_concat(&self.nodes[pos as usize], &root);
        }
        root
    }

    /// Inclusion proof for the leaf at `leaf_pos`.
    pub fn prove(&self, leaf_pos: u64) -> Result<MmrProof> {
        let sz = self.nodes.len() as u64;
        if leaf_pos >= sz { bail!("position {} out of range (size {})", leaf_pos, sz); }
        if pos_height(leaf_pos) != 0 { bail!("position {} is not a leaf", leaf_pos); }

        let peak_positions = peaks(sz);
        let mut siblings = Vec::new();
        let mut pos = leaf_pos;
        let mut height = 0u32;

        loop {
            if peak_positions.contains(&pos) { break; }

            let right_sibling = pos + (1u64 << (height + 1)) - 1;
            if right_sibling < sz && pos_height(right_sibling) == height {
                siblings.push(ProofElement {
                    hash: self.nodes[right_sibling as usize],
                    is_right: true,
                });
                pos = right_sibling + 1;
            } else {
                let left_sibling = pos - ((1u64 << (height + 1)) - 1);
                siblings.push(ProofElement {
                    hash: self.nodes[left_sibling as usize],
                    is_right: false,
                });
                pos += 1;
            }
            height += 1;
        }

        let our_peak = pos;
        let peak_index = peak_positions.iter().position(|&p| p == our_peak)
            .ok_or_else(|| anyhow::anyhow!("internal error: peak not found"))?;

        Ok(MmrProof {
            leaf_pos,
            siblings,
            peak_hashes: peak_positions.iter().map(|&p| self.nodes[p as usize]).collect(),
            peak_index,
            mmr_size: sz,
        })
    }

    pub fn get(&self, pos: u64) -> Option<&[u8; 32]> {
        self.nodes.get(pos as usize)
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ProofElement {
    pub hash: [u8; 32],
    pub is_right: bool,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MmrProof {
    pub leaf_pos: u64,
    pub siblings: Vec<ProofElement>,
    pub peak_hashes: Vec<[u8; 32]>,
    pub peak_index: usize,
    pub mmr_size: u64,
}

/// Verify an MMR inclusion proof.
pub fn verify_mmr_proof(leaf_hash: &[u8; 32], proof: &MmrProof, expected_root: &[u8; 32]) -> bool {
    let mut current = *leaf_hash;
    for elem in &proof.siblings {
        current = if elem.is_right {
            hash_concat(&current, &elem.hash)
        } else {
            hash_concat(&elem.hash, &current)
        };
    }

    if proof.peak_index >= proof.peak_hashes.len() { return false; }
    if current != proof.peak_hashes[proof.peak_index] { return false; }

    if proof.peak_hashes.is_empty() { return false; }
    let mut root = *proof.peak_hashes.last().unwrap();
    for peak in proof.peak_hashes.iter().rev().skip(1) {
        root = hash_concat(peak, &root);
    }
    root == *expected_root
}

// ═══════════════════════════════════════════════════════════════════════════
//  UTXO Accumulator  (Sparse Merkle Tree)
// ═══════════════════════════════════════════════════════════════════════════

static EMPTY_HASHES: std::sync::OnceLock<Vec<[u8; 32]>> = std::sync::OnceLock::new();

fn get_empty_hash(height: usize) -> [u8; 32] {
    let hashes = EMPTY_HASHES.get_or_init(|| {
        let mut h = Vec::with_capacity(257);
        h.push([0u8; 32]);
        for i in 0..257 {
            h.push(hash_concat(&h[i], &h[i]));
        }
        h
    });
    hashes[height]
}

/// Sparse Merkle Tree backed UTXO accumulator.
/// BTreeSet for O(log n) insert/remove/contains (was sorted Vec with O(n) shift).
/// SMT nodes stored as (height, path_key) -> hash.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct UtxoAccumulator {
    coins: std::collections::BTreeSet<[u8; 32]>,
    #[serde(skip)]
    nodes: std::collections::HashMap<(u16, [u8; 32]), [u8; 32]>,
}

impl PartialEq for UtxoAccumulator {
    fn eq(&self, other: &Self) -> bool {
        self.coins == other.coins
    }
}

impl Eq for UtxoAccumulator {}

impl UtxoAccumulator {
    pub fn new() -> Self {
        Self { coins: std::collections::BTreeSet::new(), nodes: std::collections::HashMap::new() }
    }

    /// Rebuild the SMT node cache from the coin list (after deserialization).
    pub fn rebuild_tree(&mut self) {
        self.nodes.clear();
        // Collect first → immutable borrow ends before we start mutating
        let coins: Vec<[u8; 32]> = self.coins.iter().copied().collect();
        for coin in coins {
            self.update_path(coin, true);
        }
    }

    pub fn from_set(coins: impl IntoIterator<Item = [u8; 32]>) -> Self {
        let mut acc = Self::new();
        for c in coins { acc.insert(c); }
        acc
    }

    pub fn len(&self) -> usize { self.coins.len() }
    pub fn is_empty(&self) -> bool { self.coins.is_empty() }

    pub fn contains(&self, coin: &[u8; 32]) -> bool {
        self.coins.contains(coin)
    }

    pub fn insert(&mut self, coin: [u8; 32]) -> bool {
        if !self.coins.insert(coin) { return false; }
        self.update_path(coin, true);
        true
    }

    pub fn remove(&mut self, coin: &[u8; 32]) -> bool {
        if !self.coins.remove(coin) { return false; }
        self.update_path(*coin, false);
        true
    }

    pub fn root(&mut self) -> [u8; 32] {
        self.get_node(256, [0u8; 32])
    }

    pub fn prove(&self, coin: &[u8; 32]) -> Result<UtxoProof> {
        if !self.contains(coin) {
            bail!("coin not in accumulator");
        }

        let mut siblings = Vec::with_capacity(256);
        let mut current_path = *coin;

        for h in 0usize..256 {
            let bit = get_bit(coin, h);
            let mut sibling_path = current_path;
            flip_bit(&mut sibling_path, h);
            mask_lower_bits(&mut sibling_path, h);

            let sibling_hash = self.get_node(h as u16, sibling_path);
            siblings.push(ProofElement {
                hash: sibling_hash,
                is_right: bit == 0,
            });

            mask_lower_bits(&mut current_path, h + 1);
        }

        Ok(UtxoProof {
            leaf_index: 0,
            leaf_count: self.coins.len(),
            siblings,
        })
    }

    pub fn iter(&self) -> impl Iterator<Item = &[u8; 32]> { self.coins.iter() }
    pub fn into_vec(self) -> Vec<[u8; 32]> { self.coins.into_iter().collect() }

    fn get_node(&self, height: u16, path: [u8; 32]) -> [u8; 32] {
        self.nodes.get(&(height, path))
            .copied()
            .unwrap_or_else(|| get_empty_hash(height as usize))
    }

    fn update_path(&mut self, coin: [u8; 32], inserting: bool) {
        if inserting {
            self.nodes.insert((0u16, coin), coin);
        } else {
            self.nodes.remove(&(0u16, coin));
        }

        let mut current_path = coin;

        for h in 0usize..256 {
            let bit = get_bit(&coin, h);

            let mut sibling_path = current_path;
            flip_bit(&mut sibling_path, h);
            mask_lower_bits(&mut sibling_path, h);

            let current_hash = self.get_node(h as u16, current_path);
            let sibling_hash = self.get_node(h as u16, sibling_path);

            let parent_hash = if bit == 0 {
                hash_concat(&current_hash, &sibling_hash)
            } else {
                hash_concat(&sibling_hash, &current_hash)
            };

            mask_lower_bits(&mut current_path, h + 1);

            let empty = get_empty_hash(h + 1);
            if parent_hash == empty {
                self.nodes.remove(&((h + 1) as u16, current_path));
            } else {
                self.nodes.insert(((h + 1) as u16, current_path), parent_hash);
            }
        }
    }
}

fn get_bit(bytes: &[u8; 32], bit_index: usize) -> u8 {
    let byte_idx = 31 - (bit_index / 8);
    let bit_offset = bit_index % 8;
    (bytes[byte_idx] >> bit_offset) & 1
}

fn flip_bit(bytes: &mut [u8; 32], bit_index: usize) {
    let byte_idx = 31 - (bit_index / 8);
    let bit_offset = bit_index % 8;
    bytes[byte_idx] ^= 1 << bit_offset;
}

fn mask_lower_bits(path: &mut [u8; 32], height: usize) {
    for i in 0..height {
        let byte_idx = 31 - (i / 8);
        let bit_offset = i % 8;
        path[byte_idx] &= !(1 << bit_offset);
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct UtxoProof {
    pub leaf_index: usize,
    pub leaf_count: usize,
    pub siblings: Vec<ProofElement>,
}

/// Verify a UTXO inclusion proof against the SMT root.
pub fn verify_utxo_proof(coin: &[u8; 32], proof: &UtxoProof, expected_root: &[u8; 32]) -> bool {
    if proof.siblings.len() != 256 { return false; }

    let mut current = *coin;
    for (h, elem) in proof.siblings.iter().enumerate() {
        let bit = get_bit(coin, h);
        let should_be_right = bit == 0;
        if elem.is_right != should_be_right { return false; }

        current = if elem.is_right {
            hash_concat(&current, &elem.hash)
        } else {
            hash_concat(&elem.hash, &current)
        };
    }
    current == *expected_root
}

// ═══════════════════════════════════════════════════════════════════════════
//  Tests
// ═══════════════════════════════════════════════════════════════════════════

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::types::hash;

    // ── MMR tests ───────────────────────────────────────────────────────

    #[test]
    fn mmr_append_and_root() {
        let mut mmr = MerkleMountainRange::new();
        let h1 = hash(b"leaf1");
        let h2 = hash(b"leaf2");

        mmr.append(&h1);
        assert_eq!(mmr.root(), h1);

        mmr.append(&h2);
        assert_ne!(mmr.root(), h1);

        mmr.append(&hash(b"leaf3"));
        assert_eq!(mmr.leaf_count(), 3);
    }

    #[test]
    fn mmr_proof_round_trip() {
        let mut mmr = MerkleMountainRange::new();
        let leaves: Vec<[u8; 32]> = (0..8u8).map(|i| hash(&[i])).collect();
        for leaf in &leaves { mmr.append(leaf); }
        let root = mmr.root();

        let positions = [0u64, 1, 3, 4, 7, 8, 10, 11];
        for (i, leaf) in leaves.iter().enumerate() {
            let proof = mmr.prove(positions[i]).unwrap();
            assert!(verify_mmr_proof(leaf, &proof, &root), "proof failed for leaf {}", i);
        }
    }

    #[test]
    fn mmr_size_formula() {
        assert_eq!(mmr_size(0), 0);
        assert_eq!(mmr_size(1), 1);
        assert_eq!(mmr_size(2), 3);
        assert_eq!(mmr_size(4), 7);
        assert_eq!(mmr_size(8), 15);
    }

    #[test]
    fn peaks_correctness() {
        assert_eq!(peaks(1), vec![0]);
        assert_eq!(peaks(3), vec![2]);
        assert_eq!(peaks(4), vec![2, 3]);
        assert_eq!(peaks(7), vec![6]);
    }

    // ── UTXO Accumulator (SMT) tests ────────────────────────────────────

    #[test]
    fn utxo_accumulator_basics() {
        let mut acc = UtxoAccumulator::new();
        let c1 = hash(b"coin1");
        let c2 = hash(b"coin2");
        let c3 = hash(b"coin3");

        assert!(acc.insert(c1));
        assert!(acc.insert(c2));
        assert!(acc.insert(c3));
        assert!(!acc.insert(c1)); // dup

        assert_eq!(acc.len(), 3);
        assert!(acc.contains(&c1));

        let r1 = acc.root();
        assert!(acc.remove(&c2));
        assert_ne!(r1, acc.root());
    }

    #[test]
    fn utxo_insert_remove_reinsert_same_root() {
        let mut acc = UtxoAccumulator::new();
        let c1 = hash(b"coin1");
        let c2 = hash(b"coin2");

        acc.insert(c1);
        acc.insert(c2);
        let root_before = acc.root();

        acc.remove(&c1);
        assert_ne!(root_before, acc.root());

        acc.insert(c1);
        assert_eq!(root_before, acc.root());
    }

    #[test]
    fn utxo_empty_root_is_deterministic() {
        let mut a = UtxoAccumulator::new();
        let mut b = UtxoAccumulator::new();
        assert_eq!(a.root(), b.root());
    }

    #[test]
    fn utxo_proof_round_trip() {
        let mut acc = UtxoAccumulator::new();
        let coins: Vec<[u8; 32]> = (0..10u8).map(|i| hash(&[i])).collect();
        for c in &coins { acc.insert(*c); }
        let root = acc.root();

        for c in &coins {
            let proof = acc.prove(c).unwrap();
            assert!(verify_utxo_proof(c, &proof, &root));
        }
    }

    #[test]
    fn utxo_wrong_coin_fails() {
        let mut acc = UtxoAccumulator::new();
        acc.insert(hash(b"coin1"));
        acc.insert(hash(b"coin2"));
        let root = acc.root();

        let proof = acc.prove(&hash(b"coin1")).unwrap();
        assert!(!verify_utxo_proof(&hash(b"fake"), &proof, &root));
    }

    #[test]
    fn utxo_proof_against_wrong_root_fails() {
        let mut acc = UtxoAccumulator::new();
        let c = hash(b"coin1");
        acc.insert(c);
        let proof = acc.prove(&c).unwrap();
        let fake_root = hash(b"not the root");
        assert!(!verify_utxo_proof(&c, &proof, &fake_root));
    }

    #[test]
    fn utxo_from_set() {
        let coins: Vec<[u8; 32]> = (0..5u8).map(|i| hash(&[i])).collect();
        let mut acc = UtxoAccumulator::from_set(coins.clone());
        assert_eq!(acc.len(), 5);
        for c in &coins { assert!(acc.contains(c)); }

        // Compare root with manual insert
        let mut acc2 = UtxoAccumulator::new();
        for c in &coins { acc2.insert(*c); }
        assert_eq!(acc.root(), acc2.root());
    }
    #[test]
    fn utxo_accumulator_large_set() {
        let mut acc = UtxoAccumulator::new();
        let coins: Vec<[u8; 32]> = (0..200u32).map(|i| {
            let mut h = blake3::Hasher::new();
            h.update(&i.to_le_bytes());
            *h.finalize().as_bytes()
        }).collect();
        for c in &coins { acc.insert(*c); }
        assert_eq!(acc.len(), 200);
        let root_with_all = acc.root();
        for c in &coins { assert!(acc.contains(c)); }
        for c in &coins[..100] { acc.remove(c); }
        assert_eq!(acc.len(), 100);
        assert_ne!(root_with_all, acc.root());
    }

    #[test]
    fn utxo_remove_all_returns_to_empty_root() {
        let mut acc = UtxoAccumulator::new();
        let empty_root = acc.root();
        let coins: Vec<[u8; 32]> = (0..5u8).map(|i| hash(&[i])).collect();
        for c in &coins { acc.insert(*c); }
        for c in &coins { acc.remove(c); }
        assert_eq!(acc.root(), empty_root);
        assert!(acc.is_empty());
    }

    #[test]
    fn utxo_proof_non_member_fails() {
        let acc = UtxoAccumulator::new();
        let coin = hash(b"not in set");
        assert!(acc.prove(&coin).is_err());
    }

    #[test]
    fn mmr_proof_invalid_position() {
        let mut mmr = MerkleMountainRange::new();
        mmr.append(&hash(b"a"));
        assert!(mmr.prove(999).is_err());
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/mod.rs
══════════════════════════════════════════════════════════════

pub mod finality;
pub mod types;
pub mod wots;
pub mod transaction;
pub mod extension;
pub mod state;
pub mod mmr;  
pub mod mss;   

pub use finality::*;
pub use types::*;
pub use state::adjust_difficulty;




══════════════════════════════════════════════════════════════
 FILE: core/mss.rs
══════════════════════════════════════════════════════════════

//! # Merkle Signature Scheme (MSS)
//!
//! Wraps WOTS one-time keys in a binary Merkle tree so that a single
//! **master public key** (the tree root, 32 bytes) can authorise up to
//! `2^H` signatures.
//!
//! ```text
//!             root  ←  master public key
//!            /    \
//!          h1      h2
//!         /  \    /  \
//!       pk0  pk1 pk2  pk3   ← WOTS public keys
//! ```
//!
//! ## Signing (stateful)
//!
//! Each call to `sign()` consumes the next unused leaf.  The signer
//! **must** persist `next_leaf` — reusing a WOTS leaf is catastrophic.
//!
//! ## Signature contents
//!
//! `MssSignature` = WOTS sig (576 B) + WOTS pk (32 B) + leaf index (8 B)
//!                + auth path (H × 33 B).
//! At height 10: ~950 bytes total — compact for post-quantum.
//!
//! ## Integration with midstate
//!
//! The master public key **is** the coin ID.  On-chain, the verifier:
//!   1. Checks the WOTS sig against `sig.wots_pk`.
//!   2. Checks the Merkle path from `sig.wots_pk` to the coin ID.
//!
//! This means `Transaction::Reveal` signatures can carry either a raw
//! WOTS sig (legacy, one-time) or an `MssSignature` (reusable address).
//! The verifier distinguishes them by length.

use super::types::hash_concat;
use super::wots;
use anyhow::{bail, Result};
use serde::{Deserialize, Serialize};

// ── Config ──────────────────────────────────────────────────────────────────

/// Default tree height. 2^10 = 1024 signatures per master key.
pub const DEFAULT_HEIGHT: u32 = 10;

/// Max supported height. 2^20 ≈ 1M keys.
pub const MAX_HEIGHT: u32 = 20;

// ── Types ───────────────────────────────────────────────────────────────────

/// Full MSS keypair (private — stored in wallet).
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MssKeypair {
    pub height: u32,
    /// All WOTS seeds derive from this: seed_i = BLAKE3(master_seed || i).
    pub master_seed: [u8; 32],
    /// 1-indexed binary tree.  tree[1] = root, leaves at [2^H .. 2^{H+1}).
    pub tree: Vec<[u8; 32]>,
    /// Next unused leaf (0-based among leaves).
    pub next_leaf: u64,
    /// Cached root = tree[1].
    pub master_pk: [u8; 32],
}

pub type MasterPublicKey = [u8; 32];

/// An MSS signature: WOTS sig + Merkle auth path to root.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MssSignature {
    pub leaf_index: u64,
    /// WOTS public key for this leaf (Merkle-verified against master PK).
    pub wots_pk: [u8; 32],
    /// The WOTS signature (18 × 32-byte chunks).
    pub wots_sig: Vec<[u8; 32]>,
    /// Auth path: H sibling hashes from leaf to root.
    pub auth_path: Vec<AuthNode>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct AuthNode {
    pub hash: [u8; 32],
    /// true → sibling is on the right (we are the left child).
    pub is_right: bool,
}

// ── Key generation ──────────────────────────────────────────────────────────

/// Derive WOTS seed for leaf `i`.
fn derive_wots_seed(master_seed: &[u8; 32], index: u64) -> [u8; 32] {
    hash_concat(master_seed, &index.to_le_bytes())
}

/// Generate an MSS keypair.
///
/// Cost: 2^height WOTS key generations (~1-2 ms each at w=16).
/// Height 10 ≈ 1-2 s, height 16 ≈ 1-2 min.
pub fn keygen(master_seed: &[u8; 32], height: u32) -> Result<MssKeypair> {
    if height == 0 || height > MAX_HEIGHT {
        bail!("height must be in 1..={}", MAX_HEIGHT);
    }

    let num_leaves = 1u64 << height;
    let tree_size = (num_leaves * 2) as usize; // 1-indexed, [0] unused

    let mut tree = vec![[0u8; 32]; tree_size];

    // Leaves: tree[num_leaves .. 2*num_leaves)
    let leaf_start = num_leaves as usize;
    for i in 0..num_leaves {
        let seed = derive_wots_seed(master_seed, i);
        tree[leaf_start + i as usize] = wots::keygen(&seed);
    }

    // Internal nodes bottom-up
    for i in (1..leaf_start).rev() {
        tree[i] = hash_concat(&tree[2 * i], &tree[2 * i + 1]);
    }

    Ok(MssKeypair {
        height,
        master_seed: *master_seed,
        master_pk: tree[1],
        tree,
        next_leaf: 0,
    })
}

/// Generate a random MSS keypair with default height.
pub fn keygen_random() -> Result<MssKeypair> {
    let seed: [u8; 32] = rand::random();
    keygen(&seed, DEFAULT_HEIGHT)
}

// ── Signing ─────────────────────────────────────────────────────────────────

impl MssKeypair {
    pub fn remaining(&self) -> u64 { (1u64 << self.height) - self.next_leaf }
    pub fn used(&self) -> u64 { self.next_leaf }
    pub fn public_key(&self) -> MasterPublicKey { self.master_pk }

    /// Sign a 32-byte message, consuming the next leaf.
    pub fn sign(&mut self, message: &[u8; 32]) -> Result<MssSignature> {
        let num_leaves = 1u64 << self.height;
        if self.next_leaf >= num_leaves {
            bail!("MSS tree exhausted: all {} leaves used", num_leaves);
        }

        let leaf_idx = self.next_leaf;
        self.next_leaf += 1;

        let wots_seed = derive_wots_seed(&self.master_seed, leaf_idx);
        let wots_pk = wots::keygen(&wots_seed);
        let wots_sig = wots::sign(&wots_seed, message);
        let auth_path = self.auth_path(leaf_idx);

        Ok(MssSignature { leaf_index: leaf_idx, wots_pk, wots_sig, auth_path })
    }

    /// Merkle auth path for leaf `leaf_idx`.
    fn auth_path(&self, leaf_idx: u64) -> Vec<AuthNode> {
        let num_leaves = 1u64 << self.height;
        let mut path = Vec::with_capacity(self.height as usize);
        let mut node = (num_leaves + leaf_idx) as usize; // 1-indexed

        for _ in 0..self.height {
            let (sibling_hash, is_right) = if node % 2 == 0 {
                (self.tree[node + 1], true) // we're left, sibling right
            } else {
                (self.tree[node - 1], false) // we're right, sibling left
            };
            path.push(AuthNode { hash: sibling_hash, is_right });
            node /= 2;
        }
        path
    }

    pub fn peek_next_leaf(&self) -> u64 { self.next_leaf }

    /// Force leaf index (recovery only — reuse breaks security).
    pub fn set_next_leaf(&mut self, idx: u64) { self.next_leaf = idx; }
}

// ── Verification ────────────────────────────────────────────────────────────

/// Verify MSS signature: check WOTS sig, then Merkle path to master PK.
pub fn verify(sig: &MssSignature, message: &[u8; 32], master_pk: &MasterPublicKey) -> bool {
    // 1. WOTS
    if !wots::verify(&sig.wots_sig, message, &sig.wots_pk) {
        return false;
    }

    // 2. Merkle auth path
    let mut current = sig.wots_pk;
    for node in &sig.auth_path {
        current = if node.is_right {
            hash_concat(&current, &node.hash)
        } else {
            hash_concat(&node.hash, &current)
        };
    }
    current == *master_pk
}

// ── Serialization ───────────────────────────────────────────────────────────

impl MssSignature {
    /// Layout: leaf_index(8) || wots_pk(32) || wots_sig(576) ||
    ///         auth_len(4) || auth_path(len × 33)
    pub fn to_bytes(&self) -> Vec<u8> {
        let wots_bytes = wots::sig_to_bytes(&self.wots_sig);
        let auth_len = self.auth_path.len();
        let mut buf = Vec::with_capacity(8 + 32 + wots_bytes.len() + 4 + auth_len * 33);

        buf.extend_from_slice(&self.leaf_index.to_le_bytes());
        buf.extend_from_slice(&self.wots_pk);
        buf.extend_from_slice(&wots_bytes);
        buf.extend_from_slice(&(auth_len as u32).to_le_bytes());
        for node in &self.auth_path {
            buf.extend_from_slice(&node.hash);
            buf.push(if node.is_right { 1 } else { 0 });
        }
        buf
    }

    pub fn from_bytes(data: &[u8]) -> Result<Self> {
        let min = 8 + 32 + wots::SIG_SIZE + 4;
        if data.len() < min { bail!("MSS signature too short"); }

        let leaf_index = u64::from_le_bytes(data[..8].try_into().unwrap());
        let wots_pk: [u8; 32] = data[8..40].try_into().unwrap();
        let wots_sig = wots::sig_from_bytes(&data[40..40 + wots::SIG_SIZE])
            .ok_or_else(|| anyhow::anyhow!("invalid WOTS sig in MSS"))?;

        let ao = 40 + wots::SIG_SIZE;
        let auth_len = u32::from_le_bytes(data[ao..ao + 4].try_into().unwrap()) as usize;
        if auth_len > MAX_HEIGHT as usize {
            bail!("MSS auth path too long: {} > {}", auth_len, MAX_HEIGHT);
        }
        let ps = ao + 4;
        if data.len() < ps + auth_len * 33 { bail!("MSS signature truncated"); }

        let auth_path = (0..auth_len).map(|i| {
            let o = ps + i * 33;
            AuthNode {
                hash: data[o..o + 32].try_into().unwrap(),
                is_right: data[o + 32] != 0,
            }
        }).collect();

        Ok(Self { leaf_index, wots_pk, wots_sig, auth_path })
    }

    pub fn size(&self) -> usize {
        8 + 32 + wots::SIG_SIZE + 4 + self.auth_path.len() * 33
    }
}

// ── Tests ───────────────────────────────────────────────────────────────────

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::types::hash;
    
    fn test_seed() -> [u8; 32] { hash(b"test mss master seed") }

    #[test]
    fn keygen_valid() {
        let kp = keygen(&test_seed(), 4).unwrap();
        assert_eq!(kp.height, 4);
        assert_eq!(kp.remaining(), 16);
        assert_ne!(kp.master_pk, [0u8; 32]);
    }

    #[test]
    fn sign_verify() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let msg = hash(b"hello");
        let sig = kp.sign(&msg).unwrap();
        assert!(verify(&sig, &msg, &kp.public_key()));
        assert_eq!(kp.remaining(), 15);
    }

    #[test]
    fn sign_all_leaves() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let pk = kp.public_key();
        for i in 0..16u8 {
            let msg = hash(&[i]);
            let sig = kp.sign(&msg).unwrap();
            assert!(verify(&sig, &msg, &pk));
        }
        assert_eq!(kp.remaining(), 0);
    }

    #[test]
    fn exhausted_errors() {
        let mut kp = keygen(&test_seed(), 2).unwrap();
        for _ in 0..4 { kp.sign(&hash(b"m")).unwrap(); }
        assert!(kp.sign(&hash(b"one more")).is_err());
    }

    #[test]
    fn wrong_message_fails() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let sig = kp.sign(&hash(b"correct")).unwrap();
        assert!(!verify(&sig, &hash(b"wrong"), &kp.public_key()));
    }

    #[test]
    fn wrong_master_pk_fails() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let sig = kp.sign(&hash(b"test")).unwrap();
        let other = keygen(&hash(b"other"), 4).unwrap();
        assert!(!verify(&sig, &hash(b"test"), &other.public_key()));
    }

    #[test]
    fn ser_deser_round_trip() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let msg = hash(b"serialize");
        let sig = kp.sign(&msg).unwrap();
        let bytes = sig.to_bytes();
        let sig2 = MssSignature::from_bytes(&bytes).unwrap();
        assert!(verify(&sig2, &msg, &kp.public_key()));
    }

    #[test]
    fn deterministic_keygen() {
        let s = test_seed();
        assert_eq!(keygen(&s, 4).unwrap().master_pk, keygen(&s, 4).unwrap().master_pk);
    }

    #[test]
    fn different_leaves_both_verify() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let msg = hash(b"same");
        let s1 = kp.sign(&msg).unwrap();
        let s2 = kp.sign(&msg).unwrap();
        assert_ne!(s1.leaf_index, s2.leaf_index);
        let pk = kp.public_key();
        assert!(verify(&s1, &msg, &pk));
        assert!(verify(&s2, &msg, &pk));
    }

    #[test]
    fn sig_size_reasonable() {
        let mut kp = keygen(&test_seed(), 2).unwrap();
        let sig = kp.sign(&hash(b"t")).unwrap();
        // 8 + 32 + 576 + 4 + 2*33 = 686
        assert_eq!(sig.size(), 686);
    }
    #[test]
    fn keygen_rejects_zero_height() {
        assert!(keygen(&test_seed(), 0).is_err());
    }

    #[test]
    fn keygen_rejects_excessive_height() {
        assert!(keygen(&test_seed(), MAX_HEIGHT + 1).is_err());
    }

    #[test]
    fn from_bytes_truncated_fails() {
        let mut kp = keygen(&test_seed(), 4).unwrap();
        let sig = kp.sign(&hash(b"test")).unwrap();
        let bytes = sig.to_bytes();
        assert!(MssSignature::from_bytes(&bytes[..bytes.len() - 10]).is_err());
    }

    #[test]
    fn from_bytes_too_short_fails() {
        assert!(MssSignature::from_bytes(&[0u8; 10]).is_err());
    }
    
}




══════════════════════════════════════════════════════════════
 FILE: core/state.rs
══════════════════════════════════════════════════════════════

use super::types::*;
use super::transaction::apply_transaction;
use super::extension::verify_extension;
use anyhow::{bail, Result};
use primitive_types::U256;
use std::time::{SystemTime, UNIX_EPOCH};

/// Adjusts the mining difficulty using the ASERT algorithm.
///
/// ASERT compares absolute elapsed time since genesis against the ideal
/// schedule (height × TARGET_BLOCK_TIME) and applies an exponential
/// correction with a configurable half-life. This eliminates the
/// sliding-window exploits (time warp, hash-and-flee, echo effects)
/// inherent to relative algorithms like LWMA.
///
/// All arithmetic is deterministic integer math (16.16 fixed-point Taylor
/// polynomial for 2^x) — no floating-point is used.
pub fn adjust_difficulty(state: &State) -> [u8; 32] {
    if state.height == 0 {
        return state.target;
    }

    let (genesis, _) = State::genesis();

    // 1. Drift = how far actual time is from ideal time
    let ideal_time = (state.height - genesis.height) as i64 * (TARGET_BLOCK_TIME as i64);
    let actual_time = (state.timestamp as i64).saturating_sub(genesis.timestamp as i64);
    let drift = actual_time - ideal_time;

    // 2. Fixed-point exponent: drift / half_life in 16.16
    let exponent = drift.saturating_mul(65536) / ASERT_HALF_LIFE;
    let shifts = exponent >> 16;       // integer part (whole powers of 2)
    let frac = exponent & 0xFFFF;      // fractional part

    // 3. Taylor polynomial approximation of 2^frac (16.16 fixed-point)
    //    Coefficients match the BCH aserti3-2d reference implementation.
    let mut factor = 65536i64;
    factor += (frac * 45426) >> 16;
    factor += (frac * frac * 15746) >> 32;
    factor += (frac * frac * frac * 3643) >> 48;

    // 4. Apply factor to genesis target (divide-first to avoid U256 overflow,
    //    since genesis target can be ~2^253 and factor ~2^17)
    let mut target = U256::from_big_endian(&genesis.target);
    let f = U256::from(factor as u64);
    let base = U256::from(65536u64);
    target = target / base * f + (target % base) * f / base;

    let ceiling = U256::from_big_endian(&[0xff; 32]);

    if shifts > 0 {
        let s = (shifts as usize).min(255);
        let headroom = ceiling >> s;
        target = if target > headroom { ceiling } else { target << s };
    } else if shifts < 0 {
        let s = ((-shifts) as usize).min(255);
        target = target >> s;
    }

    // 5. Clamp: never zero, never above the absolute ceiling
    if target > ceiling || target.is_zero() {
        target = ceiling;
    }

    target.to_big_endian()
}

pub fn current_timestamp() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_secs()
}

/// Validate a block's timestamp against the chain.
pub fn validate_timestamp(
    new_timestamp: u64,
    previous_timestamps: &[u64],
    current_time: u64,
) -> Result<()> {
    const MAX_FUTURE_BLOCK_TIME: u64 = 2 * 60 * 60;

    if new_timestamp > current_time + MAX_FUTURE_BLOCK_TIME {
        bail!(
            "Block timestamp too far in future: {} > {} (max future: {}s)",
            new_timestamp,
            current_time,
            MAX_FUTURE_BLOCK_TIME
        );
    }

    if previous_timestamps.len() >= MEDIAN_TIME_PAST_WINDOW {
        let mut recent_timestamps: Vec<u64> = previous_timestamps
            .iter()
            .rev()
            .take(MEDIAN_TIME_PAST_WINDOW)
            .copied()
            .collect();

        recent_timestamps.sort_unstable();
        let median = recent_timestamps[MEDIAN_TIME_PAST_WINDOW / 2];

        if new_timestamp <= median {
            bail!(
                "Block timestamp {} must be greater than median of last {} blocks ({})",
                new_timestamp,
                MEDIAN_TIME_PAST_WINDOW,
                median
            );
        }
    } else if let Some(&last_ts) = previous_timestamps.last() {
        if new_timestamp <= last_ts {
            bail!(
                "Block timestamp {} must be greater than previous block timestamp {}",
                new_timestamp,
                last_ts
            );
        }
    }

    Ok(())
}

/// Apply a batch to the state
pub fn apply_batch(state: &mut State, batch: &Batch, previous_timestamps: &[u64]) -> Result<()> {
    // 1. Check parent linkage
    if batch.prev_midstate != state.midstate {
        bail!("Block parent mismatch: expected {}, got {}",
              hex::encode(state.midstate),
              hex::encode(batch.prev_midstate));
    }

    if batch.target != state.target {
        bail!("Batch target mismatch: expected {}, got {}",
              hex::encode(state.target),
              hex::encode(batch.target));
    }

    //timewarp prevention: Validate timestamp
    if state.height > 0 {
        validate_timestamp(batch.timestamp, previous_timestamps, current_timestamp())?;
    }

    // 2. Reject batches that would require excessive signature verification
    let total_inputs: usize = batch.transactions.iter().map(|tx| match tx {
        Transaction::Reveal { inputs, .. } => inputs.len(),
        _ => 0,
    }).sum();
    if total_inputs > MAX_BATCH_INPUTS {
        bail!("Batch exceeds max total inputs: {} > {}", total_inputs, MAX_BATCH_INPUTS);
    }

    // 3. Apply transactions and tally fees
    let mut total_fees: u64 = 0;
    for tx in &batch.transactions {
        total_fees += tx.fee();
        apply_transaction(state, tx)?;
    }

    // 3. Validate coinbase outputs
    let reward = block_reward(state.height);
    let allowed_value = reward + total_fees;

    let mut coinbase_total: u64 = 0;
    for (i, cb) in batch.coinbase.iter().enumerate() {
        if cb.value == 0 {
            bail!("Zero-value coinbase output {}", i);
        }
        if !cb.value.is_power_of_two() {
            bail!("Coinbase output {} value {} is not a power of 2", i, cb.value);
        }
        coinbase_total = coinbase_total.checked_add(cb.value)
            .ok_or_else(|| anyhow::anyhow!("Coinbase value overflow"))?;
    }
    if coinbase_total != allowed_value {
        bail!("Coinbase total {} != expected {} (reward {} + fees {})",
              coinbase_total, allowed_value, reward, total_fees);
    }

// 4. Compute future midstate with coinbase coin IDs
    let mut future_midstate = state.midstate;
    let coinbase_ids: Vec<[u8; 32]> = batch.coinbase.iter().map(|cb| cb.coin_id()).collect();
    for coin_id in &coinbase_ids {
        future_midstate = hash_concat(&future_midstate, coin_id);
    }

    // --- LOGGING START ---
    if state.height == 0 { // Only log for genesis to reduce noise
        tracing::error!("APPLY_BATCH DEBUG:");
        tracing::error!("  State Midstate: {}", hex::encode(state.midstate));
        tracing::error!("  Future Midstate (calculated): {}", hex::encode(future_midstate));
    }
    // --- LOGGING END ---

    {
        let expired: Vec<[u8; 32]> = state.commitment_heights.iter()
            .filter(|(_, &h)| state.height.saturating_sub(h) > COMMITMENT_TTL)
            .map(|(c, _)| *c)
            .collect();
        for c in &expired {
            state.commitments.remove(c);
            state.commitment_heights.remove(c);
        }
    }

    // 5. Verify extension against future midstate
    verify_extension(future_midstate, &batch.extension, &batch.target)?;

    // 6. Add coinbase coins to state
    for coin_id in &coinbase_ids {
        if !state.coins.insert(*coin_id) {
            bail!("Duplicate coinbase coin");
        }
        state.midstate = hash_concat(&state.midstate, coin_id);
    }

    // 7. Finalize
    state.midstate = batch.extension.final_hash;
    state.depth += EXTENSION_ITERATIONS;
    state.height += 1;
    state.timestamp = batch.timestamp;

    Ok(())
}

/// Choose the better of two states (fork resolution)
pub fn choose_best_state<'a>(a: &'a State, b: &'a State) -> &'a State {
    match a.depth.cmp(&b.depth) {
        std::cmp::Ordering::Greater => a,
        std::cmp::Ordering::Less => b,
        std::cmp::Ordering::Equal => {
            if a.midstate < b.midstate { a } else { b }
        }
    }
}
// ============================================================
// ADD THIS ENTIRE BLOCK at the bottom of src/core/state.rs
// ============================================================

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::extension::create_extension;
    use crate::core::mmr::UtxoAccumulator;
    use crate::core::wots;

    fn easy_target() -> [u8; 32] {
        [0xff; 32]
    }

    fn genesis_state() -> State {
        State::genesis().0
    }

    /// Build a valid batch on top of the given state (no transactions).
    fn make_empty_batch(state: &State, reward: u64, timestamp: u64) -> Batch {
        let coinbase = make_coinbase(state, reward);
        let mut mining_midstate = state.midstate;
        for cb in &coinbase {
            mining_midstate = hash_concat(&mining_midstate, &cb.coin_id());
        }
        // Search for a nonce that meets the target
        let mut nonce = 0u64;
        let extension = loop {
            let ext = create_extension(mining_midstate, nonce);
            if ext.final_hash < state.target {
                break ext;
            }
            nonce += 1;
        };
        Batch {
            prev_midstate: state.midstate,
            transactions: vec![],
            extension,
            coinbase,
            timestamp,
            target: state.target,
        }
    }

    fn make_coinbase(state: &State, total_value: u64) -> Vec<CoinbaseOutput> {
        let denoms = decompose_value(total_value);
        denoms.iter().enumerate().map(|(i, &value)| {
            let seed = hash_concat(&state.midstate, &(i as u64).to_le_bytes());
            let pk = wots::keygen(&seed);
            let address = compute_address(&pk);
            let salt = hash_concat(&seed, &[0xCBu8; 32]);
            CoinbaseOutput { address, value, salt }
        }).collect()
    }

    // ── apply_batch ─────────────────────────────────────────────────────

    #[test]
    fn apply_batch_advances_height() {
        let mut state = genesis_state();
        let reward = block_reward(state.height);
        let batch = make_empty_batch(&state, reward, state.timestamp + 1);
        let timestamps = vec![state.timestamp];
        apply_batch(&mut state, &batch, &timestamps).unwrap();
        assert_eq!(state.height, 1);
    }

    #[test]
    fn apply_batch_advances_depth() {
        let mut state = genesis_state();
        let initial_depth = state.depth;
        let reward = block_reward(state.height);
        let batch = make_empty_batch(&state, reward, state.timestamp + 1);
    let timestamps = vec![state.timestamp];
        apply_batch(&mut state, &batch, &timestamps).unwrap();
        assert_eq!(state.depth, initial_depth + EXTENSION_ITERATIONS);
    }

    #[test]
    fn apply_batch_updates_midstate() {
        let mut state = genesis_state();
        let old_midstate = state.midstate;
        let reward = block_reward(state.height);
        let batch = make_empty_batch(&state, reward, state.timestamp + 1);
        let timestamps = vec![state.timestamp];
        apply_batch(&mut state, &batch, &timestamps).unwrap();
        assert_ne!(state.midstate, old_midstate);
        assert_eq!(state.midstate, batch.extension.final_hash);
    }

    #[test]
    fn apply_batch_adds_coinbase_coins() {
        let mut state = genesis_state();
        let reward = block_reward(state.height);
        let batch = make_empty_batch(&state, reward, state.timestamp + 1);
        let coinbase_ids: Vec<[u8; 32]> = batch.coinbase.iter().map(|c| c.coin_id()).collect();
        let timestamps = vec![state.timestamp];
        apply_batch(&mut state, &batch, &timestamps).unwrap();
        for id in &coinbase_ids {
            assert!(state.coins.contains(id), "coinbase coin should be in state");
        }
    }

    #[test]
    fn apply_batch_rejects_wrong_prev_midstate() {
        let state = genesis_state();
        let reward = block_reward(state.height);
        let mut batch = make_empty_batch(&state, reward, state.timestamp + 1);
        batch.prev_midstate = [0xFFu8; 32]; // wrong parent
        let mut state2 = state.clone();
        let timestamps = vec![state2.timestamp];
        assert!(apply_batch(&mut state2, &batch, &timestamps).is_err());
    }

    #[test]
    fn apply_batch_rejects_wrong_target() {
        let mut state = genesis_state();
        let reward = block_reward(state.height);
        let mut batch = make_empty_batch(&state, reward, state.timestamp + 1);
        batch.target = [0x00; 32]; // wrong target
        let timestamps = vec![state.timestamp];
        assert!(apply_batch(&mut state, &batch, &timestamps).is_err());
    }

    #[test]
    fn apply_batch_rejects_wrong_coinbase_total() {
        let mut state = genesis_state();
        // Coinbase with too much value
        let batch = make_empty_batch(&state, block_reward(state.height) + 100, state.timestamp + 1);
        let timestamps = vec![state.timestamp];
        assert!(apply_batch(&mut state, &batch, &timestamps).is_err());
    }

    #[test]
    fn apply_batch_rejects_non_power_of_two_coinbase() {
        let mut state = genesis_state();
        let mut batch = make_empty_batch(&state, block_reward(state.height), state.timestamp + 1);
        // Corrupt a coinbase value to be non-power-of-2
        if let Some(cb) = batch.coinbase.first_mut() {
            cb.value = 3; // not a power of 2
        }
        let timestamps = vec![state.timestamp];
        assert!(apply_batch(&mut state, &batch, &timestamps).is_err());
    }

    #[test]
    fn apply_batch_rejects_past_timestamp() {
        let mut state = genesis_state();
        // Apply genesis batch first to get height > 0
        let reward = block_reward(state.height);
        let batch0 = make_empty_batch(&state, reward, state.timestamp + 1);
        let timestamps_0 = vec![state.timestamp];
        apply_batch(&mut state, &batch0, &timestamps_0).unwrap();

        // Now try a batch with timestamp <= previous
        let reward = block_reward(state.height);
        let batch1 = make_empty_batch(&state, reward, state.timestamp); // same timestamp
        let timestamps_1 = vec![state.timestamp];
        assert!(apply_batch(&mut state, &batch1, &timestamps_1).is_err());
    }

    #[test]
    fn apply_batch_validates_timestamp() {
        let mut state = genesis_state();
        let reward = block_reward(state.height);
        
        // 1. Apply a valid block so height > 0
        let valid_batch = make_empty_batch(&state, reward, state.timestamp + 1);
        let timestamps_0 = vec![state.timestamp];
        apply_batch(&mut state, &valid_batch, &timestamps_0).unwrap();

        // 2. Try to apply an invalid block (timestamp not strictly greater)
        let reward2 = block_reward(state.height);
        let invalid_batch = make_empty_batch(&state, reward2, state.timestamp);
        let timestamps_1 = vec![state.timestamp];

        
        let result = apply_batch(&mut state, &invalid_batch, &timestamps_1);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("greater than previous"));
    }

    // ── choose_best_state ───────────────────────────────────────────────

    #[test]
    fn choose_best_state_higher_depth_wins() {
        let mut a = genesis_state();
        let mut b = genesis_state();
        a.depth = 100;
        b.depth = 200;
        assert_eq!(choose_best_state(&a, &b).depth, 200);
        assert_eq!(choose_best_state(&b, &a).depth, 200);
    }

    #[test]
    fn choose_best_state_equal_depth_uses_midstate() {
        let mut a = genesis_state();
        let mut b = genesis_state();
        a.depth = 100;
        b.depth = 100;
        a.midstate = [0x01; 32];
        b.midstate = [0x02; 32];
        // Lower midstate wins
        assert_eq!(choose_best_state(&a, &b).midstate, [0x01; 32]);
    }

    // ── adjust_difficulty (ASERT) ─────────────────────────────────────

    #[test]
    fn adjust_difficulty_no_change_at_height_zero() {
        let state = genesis_state();
        let result = adjust_difficulty(&state);
        assert_eq!(result, state.target);
    }

    #[test]
    fn adjust_difficulty_stable_when_on_target() {
        let genesis = genesis_state();
        let mut state = genesis.clone();
        state.height = 100;
        state.timestamp = genesis.timestamp + (100 * TARGET_BLOCK_TIME);

        let result = adjust_difficulty(&state);
        assert_eq!(result, genesis.target, "Target should not change when blocks are exactly on schedule");
    }

    #[test]
    fn adjust_difficulty_drops_when_blocks_slow() {
        let genesis = genesis_state();
        let mut state = genesis.clone();
        state.height = 10;
        // 10 blocks should take 600s. They took 2000s (too slow).
        state.timestamp = genesis.timestamp + 2000;

        let result = adjust_difficulty(&state);
        let old_u256 = U256::from_big_endian(&genesis.target);
        let new_u256 = U256::from_big_endian(&result);
        assert!(new_u256 > old_u256, "Target should increase (get easier) when blocks are slow");
    }

    #[test]
    fn adjust_difficulty_rises_when_blocks_fast() {
        let genesis = genesis_state();
        let mut state = genesis.clone();
        state.height = 10;
        // 10 blocks should take 600s. They took 100s (too fast).
        state.timestamp = genesis.timestamp + 100;

        let result = adjust_difficulty(&state);
        let old_u256 = U256::from_big_endian(&genesis.target);
        let new_u256 = U256::from_big_endian(&result);
        assert!(new_u256 < old_u256, "Target should decrease (get harder) when blocks are fast");
    }

    #[test]
    fn adjust_difficulty_exact_halving() {
        let genesis = genesis_state();
        let mut state = genesis.clone();
        // Mine 240 blocks instantly → drift = -14400s = exactly -1 half-life.
        state.height = 240;
        state.timestamp = genesis.timestamp; // no time passed

        let result = adjust_difficulty(&state);
        let old_u256 = U256::from_big_endian(&genesis.target);
        let new_u256 = U256::from_big_endian(&result);
        let expected = old_u256 >> 1;
        assert_eq!(new_u256, expected, "Target must exactly halve after 1 half-life of negative drift");
    }

    #[test]
    fn adjust_difficulty_ceiling_clamp() {
        let genesis = genesis_state();
        let mut state = genesis.clone();
        // Extreme stall: huge positive drift.
        state.height = 1;
        state.timestamp = genesis.timestamp + 999_999_999;

        let result = adjust_difficulty(&state);
        assert_eq!(result, [0xff; 32], "Target must clamp to the 0xff ceiling");
    }

    // ── validate_timestamp ──────────────────────────────────────────────

    #[test]
    fn validate_timestamp_accepts_recent() {
        let current_time = 1_000_000;
        let result = validate_timestamp(current_time - 10, &[], current_time);
        assert!(result.is_ok());
    }

    #[test]
    fn validate_timestamp_rejects_far_future() {
        let current_time = 1_000_000;
        let far_future = current_time + 3 * 60 * 60; // 3 hours ahead
        let result = validate_timestamp(far_future, &[], current_time);
        assert!(result.is_err());
    }

    #[test]
    fn validate_timestamp_rejects_before_previous() {
        let prev = State {
            midstate: [0; 32],
            coins: UtxoAccumulator::new(),
            commitments: UtxoAccumulator::new(),
            depth: 0,
            target: easy_target(),
            height: 1,
            timestamp: 1000,
            commitment_heights: std::collections::HashMap::new(),
        };
        
        let timestamps = vec![prev.timestamp];
        
        let result = validate_timestamp(999, &timestamps, 2000);
        assert!(result.is_err());
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/transaction.rs
══════════════════════════════════════════════════════════════

use super::types::*;
use super::wots;
use super::mss;
use anyhow::{bail, Result};

const COMMIT_POW_TARGET: u16 = 0x0000;

fn validate_commit_pow(commitment: &[u8; 32], nonce: u64) -> Result<()> {
    let h = super::types::hash_concat(commitment, &nonce.to_le_bytes());
    if u16::from_be_bytes([h[0], h[1]]) != COMMIT_POW_TARGET {
        bail!("Insufficient Commit PoW");
    }
    Ok(())
}

/// Verify a signature that may be either raw WOTS (576 bytes) or MSS (longer).
fn verify_signature(sig_bytes: &[u8], message: &[u8; 32], owner_pk: &[u8; 32]) -> bool {
    if sig_bytes.len() == wots::SIG_SIZE {
        match wots::sig_from_bytes(sig_bytes) {
            Some(sig) => wots::verify(&sig, message, owner_pk),
            None => false,
        }
    } else {
        match mss::MssSignature::from_bytes(sig_bytes) {
            Ok(mss_sig) => mss::verify(&mss_sig, message, owner_pk),
            Err(_) => false,
        }
    }
}

/// Apply a transaction to the state
pub fn apply_transaction(state: &mut State, tx: &Transaction) -> Result<()> {
    match tx {
        Transaction::Commit { commitment, spam_nonce } => {
            validate_commit_pow(commitment, *spam_nonce)?;
            if !state.commitments.insert(*commitment) {
                bail!("Duplicate commitment");
            }
            state.commitment_heights.insert(*commitment, state.height);
            state.midstate = hash_concat(&state.midstate, commitment);
            Ok(())
        }

        Transaction::Reveal { inputs, signatures, outputs, salt, .. } => {
            if inputs.is_empty() {
                bail!("Transaction must spend at least one coin");
            }
            if outputs.is_empty() {
                bail!("Transaction must create at least one new coin");
            }
            if inputs.len() > MAX_TX_INPUTS { 
                bail!("Too many inputs (max {})", MAX_TX_INPUTS); 
                }
            if outputs.len() > MAX_TX_OUTPUTS { 
                bail!("Too many outputs (max {})", MAX_TX_OUTPUTS); 
            }
            
            if signatures.len() != inputs.len() {
                bail!("Signature count must match input count");
            }
            for (i, sig) in signatures.iter().enumerate() {
                if sig.len() > MAX_SIGNATURE_SIZE {
                    bail!("Signature {} too large: {} > {}", i, sig.len(), MAX_SIGNATURE_SIZE);
                }
            }
            {
                let mut seen = std::collections::HashSet::new();
                for input in inputs {
                    if !seen.insert(input.coin_id()) {
                        bail!("Duplicate input coin");
                    }
                }
            }
            // 1. Validate all output values are power of 2 and nonzero
            for (i, out) in outputs.iter().enumerate() {
                if out.value == 0 {
                    bail!("Zero-value output {}", i);
                }
                if !out.value.is_power_of_two() {
                    bail!("Invalid denomination: output {} value {} is not a power of 2", i, out.value);
                }
            }

            // 2. Value conservation: sum(inputs) > sum(outputs)
            let in_sum: u64 = inputs.iter().map(|i| i.value).sum();
            let out_sum: u64 = outputs.iter().map(|o| o.value).sum();
            if in_sum <= out_sum {
                bail!(
                    "Input value ({}) must exceed output value ({}) to pay fee",
                    in_sum, out_sum
                );
            }

            // 3. Compute coin IDs from preimages
            let input_coin_ids: Vec<[u8; 32]> = inputs.iter().map(|i| i.coin_id()).collect();
            let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();

            // 4. Verify commitment exists and matches
            let expected = compute_commitment(&input_coin_ids, &output_coin_ids, salt);
            if !state.commitments.remove(&expected) {
                bail!(
                    "No matching commitment found (expected {})",
                    hex::encode(expected)
                );
            }

            // 5. Verify each input coin exists and signature is valid against owner_pk
            for (i, (input, sig_bytes)) in inputs.iter().zip(signatures.iter()).enumerate() {
                let coin_id = input.coin_id();
                if !state.coins.contains(&coin_id) {
                    bail!("Coin {} not found or already spent", hex::encode(coin_id));
                }
                if !verify_signature(sig_bytes, &expected, &input.owner_pk) {
                    bail!("Invalid signature for input {}", i);
                }
            }

            // 6. Remove spent coins
            for coin_id in &input_coin_ids {
                state.coins.remove(coin_id);
            }

            // 7. Add new coins (store only the coin_id hash)
            for coin_id in &output_coin_ids {
                if !state.coins.insert(*coin_id) {
                    bail!("Duplicate coin created");
                }
            }

            // 8. Update midstate
            {
                let mut hasher = blake3::Hasher::new();
                for coin_id in &input_coin_ids {
                    hasher.update(coin_id);
                }
                for coin_id in &output_coin_ids {
                    hasher.update(coin_id);
                }
                hasher.update(salt);
                let tx_hash = *hasher.finalize().as_bytes();
                state.midstate = hash_concat(&state.midstate, &tx_hash);
            }

            Ok(())
        }
    }
}

/// Validate a transaction without applying it
pub fn validate_transaction(state: &State, tx: &Transaction) -> Result<()> {
    match tx {
        Transaction::Commit { commitment, spam_nonce } => {
            validate_commit_pow(commitment, *spam_nonce)?;
            if state.commitments.contains(commitment) {
                bail!("Duplicate commitment");
            }
            Ok(())
        }

        Transaction::Reveal { inputs, signatures, outputs, salt, .. } => {
            if inputs.is_empty() {
                bail!("Must spend at least one coin");
            }
            if outputs.is_empty() {
                bail!("Must create at least one coin");
            }
            if inputs.len() > MAX_TX_INPUTS { 
                bail!("Too many inputs (max {})", MAX_TX_INPUTS); 
                }
            if outputs.len() > MAX_TX_OUTPUTS { 
                bail!("Too many outputs (max {})", MAX_TX_OUTPUTS); 
            }
            
            if signatures.len() != inputs.len() {
                bail!("Signature count must match input count");
            }
            for (i, sig) in signatures.iter().enumerate() {
                if sig.len() > MAX_SIGNATURE_SIZE {
                    bail!("Signature {} too large: {} > {}", i, sig.len(), MAX_SIGNATURE_SIZE);
                }
            }
            {
                let mut seen = std::collections::HashSet::new();
                for input in inputs {
                    if !seen.insert(input.coin_id()) {
                        bail!("Duplicate input coin");
                    }
                }
            }
            for (i, out) in outputs.iter().enumerate() {
                if out.value == 0 {
                    bail!("Zero-value output {}", i);
                }
                if !out.value.is_power_of_two() {
                    bail!("Invalid denomination: output {} value {} is not a power of 2", i, out.value);
                }
            }

            let in_sum: u64 = inputs.iter().map(|i| i.value).sum();
            let out_sum: u64 = outputs.iter().map(|o| o.value).sum();
            if in_sum <= out_sum {
                bail!("Input value must exceed output value");
            }

            let input_coin_ids: Vec<[u8; 32]> = inputs.iter().map(|i| i.coin_id()).collect();
            let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();

            let expected = compute_commitment(&input_coin_ids, &output_coin_ids, salt);
            if !state.commitments.contains(&expected) {
                bail!("No matching commitment found");
            }

            // Check commitment hasn't expired (or is about to expire this block)
            if let Some(&commit_height) = state.commitment_heights.get(&expected) {
                if state.height.saturating_sub(commit_height) >= COMMITMENT_TTL {
                    bail!("Commitment expired (committed at height {}, current {})", commit_height, state.height);
                }
            }

            for (i, (input, sig_bytes)) in inputs.iter().zip(signatures.iter()).enumerate() {
                let coin_id = input.coin_id();
                if !state.coins.contains(&coin_id) {
                    bail!("Coin {} not found", hex::encode(coin_id));
                }
                if !verify_signature(sig_bytes, &expected, &input.owner_pk) {
                    bail!("Invalid signature for input {}", i);
                }
            }

            Ok(())
        }
    }
}

#[cfg(test)] 
mod tests {
    use super::*;
    use crate::core::mmr::UtxoAccumulator;

    fn empty_state() -> State {
        State {
            midstate: [0u8; 32],
            coins: UtxoAccumulator::new(),
            commitments: UtxoAccumulator::new(),
            depth: 0,
            target: [0xff; 32],
            height: 1,
            timestamp: 1000,
            commitment_heights: std::collections::HashMap::new(),
        }
    }

    fn mine_commit_nonce(commitment: &[u8; 32]) -> u64 {
        let mut n = 0u64;
        loop {
            let h = hash_concat(commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 {
                return n;
            }
            n += 1;
        }
    }

    #[test]
    fn commit_pow_valid_nonce_passes() {
        let commitment = hash(b"test commitment");
        let nonce = mine_commit_nonce(&commitment);
        assert!(validate_commit_pow(&commitment, nonce).is_ok());
    }

    #[test]
    fn commit_pow_invalid_nonce_fails() {
        let commitment = hash(b"test commitment");
        // Nonce 0 is almost certainly invalid (1 in 65536 chance)
        // Try a few to find one that fails
        let mut bad_nonce = 0u64;
        loop {
            let h = hash_concat(&commitment, &bad_nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) != 0x0000 {
                break;
            }
            bad_nonce += 1;
        }
        assert!(validate_commit_pow(&commitment, bad_nonce).is_err());
    }

    #[test]
    fn commit_pow_benchmark() {
        // Measure time to mine a valid nonce — should be ~10-50ms
        let commitment = hash(b"benchmark commitment");
        let start = std::time::Instant::now();
        let nonce = mine_commit_nonce(&commitment);
        let elapsed = start.elapsed();
        // Verify it's actually valid
        assert!(validate_commit_pow(&commitment, nonce).is_ok());
        // Log timing (visible with `cargo test -- --nocapture`)
        eprintln!("Commit PoW mining took {:?} (nonce: {})", elapsed, nonce);
        // Soft assert: should complete within 5 seconds even on slow hardware
        assert!(elapsed.as_secs() < 5, "PoW took too long: {:?}", elapsed);
    }

    #[test]
    fn validate_transaction_rejects_bad_commit_pow() {
        let state = empty_state();
        let commitment = hash(b"reject test");
        // Find an invalid nonce
        let mut bad_nonce = 0u64;
        loop {
            let h = hash_concat(&commitment, &bad_nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) != 0x0000 {
                break;
            }
            bad_nonce += 1;
        }
        let tx = Transaction::Commit { commitment, spam_nonce: bad_nonce };
        assert!(validate_transaction(&state, &tx).is_err());
    }

    #[test]
    fn validate_transaction_accepts_good_commit_pow() {
        let state = empty_state();
        let commitment = hash(b"accept test");
        let nonce = mine_commit_nonce(&commitment);
        let tx = Transaction::Commit { commitment, spam_nonce: nonce };
        assert!(validate_transaction(&state, &tx).is_ok());
    }

    // ── Full Commit + Reveal flow ───────────────────────────────────────

    /// Helper: create a state with a spendable coin, returning (state, seed, coin_id, salt).
    fn state_with_coin(value: u64) -> (State, [u8; 32], [u8; 32], [u8; 32]) {
        let mut state = empty_state();
        let seed: [u8; 32] = [0x42; 32];
        let owner_pk = wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        let salt = hash(b"test salt");
        let coin_id = compute_coin_id(&address, value, &salt);
        state.coins.insert(coin_id);
        (state, seed, coin_id, salt)
    }

    fn do_commit(state: &mut State, input_ids: &[[u8; 32]], output_ids: &[[u8; 32]], salt: &[u8; 32]) -> [u8; 32] {
        let commitment = compute_commitment(input_ids, output_ids, salt);
        let nonce = mine_commit_nonce(&commitment);
        let tx = Transaction::Commit { commitment, spam_nonce: nonce };
        apply_transaction(state, &tx).unwrap();
        commitment
    }

    #[test]
    fn full_commit_reveal_flow() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(16);

        // Build output
        let out_addr = hash(b"recipient");
        let out_salt: [u8; 32] = [0x11; 32];
        let output = OutputData { address: out_addr, value: 8, salt: out_salt };
        let output_coin_id = output.coin_id();

        // Commit
        let commit_salt: [u8; 32] = [0x22; 32];
        let commitment = do_commit(
            &mut state,
            &[coin_id],
            &[output_coin_id],
            &commit_salt,
        );

        // Reveal
        let owner_pk = wots::keygen(&seed);
        let sig = wots::sign(&seed, &commitment);
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        apply_transaction(&mut state, &tx).unwrap();

        // Input coin spent
        assert!(!state.coins.contains(&coin_id));
        // Output coin created
        assert!(state.coins.contains(&output_coin_id));
    }

    #[test]
    fn reveal_rejects_without_commit() {
        let (mut state, seed, _coin_id, input_salt) = state_with_coin(16);
        let owner_pk = wots::keygen(&seed);
        let output = OutputData { address: hash(b"r"), value: 8, salt: [0; 32] };
        let fake_commitment = hash(b"not committed");
        let sig = wots::sign(&seed, &fake_commitment);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: [0; 32],
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_wrong_signature() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(16);
        let owner_pk = wots::keygen(&seed);
        let output = OutputData { address: hash(b"r"), value: 8, salt: [0; 32] };
        let commit_salt: [u8; 32] = [0x33; 32];
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);

        // Sign with wrong key
        let wrong_seed: [u8; 32] = [0xFF; 32];
        let bad_sig = wots::sign(&wrong_seed, &commitment);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&bad_sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_nonexistent_coin() {
        let mut state = empty_state();
        let seed: [u8; 32] = [0x42; 32];
        let owner_pk = wots::keygen(&seed);
        let input_salt = [0u8; 32];
        let address = compute_address(&owner_pk);
        let coin_id = compute_coin_id(&address, 16, &input_salt);
        // Do NOT insert coin into state

        let output = OutputData { address: hash(b"r"), value: 8, salt: [0; 32] };
        let commit_salt = [1u8; 32];

        // Commit is allowed (doesn't check coins)
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);
        let sig = wots::sign(&seed, &commitment);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    // ── Value conservation ──────────────────────────────────────────────

    #[test]
    fn reveal_rejects_output_exceeding_input() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(8);
        let owner_pk = wots::keygen(&seed);
        let output = OutputData { address: hash(b"r"), value: 8, salt: [0; 32] };
        let commit_salt = [0u8; 32];
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);
        let sig = wots::sign(&seed, &commitment);

        // output value == input value, no fee → rejected
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 8, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    // ── Edge cases ──────────────────────────────────────────────────────

    #[test]
    fn reveal_rejects_empty_inputs() {
        let mut state = empty_state();
        let tx = Transaction::Reveal {
            inputs: vec![],
            signatures: vec![],
            outputs: vec![OutputData { address: [0; 32], value: 1, salt: [0; 32] }],
            salt: [0; 32],
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_empty_outputs() {
        let (mut state, seed, _coin_id, input_salt) = state_with_coin(8);
        let owner_pk = wots::keygen(&seed);
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 8, salt: input_salt }],
            signatures: vec![vec![0; wots::SIG_SIZE]],
            outputs: vec![],
            salt: [0; 32],
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_mismatched_sig_count() {
        let (mut state, seed, _coin_id, input_salt) = state_with_coin(8);
        let owner_pk = wots::keygen(&seed);
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 8, salt: input_salt }],
            signatures: vec![], // 0 sigs for 1 input
            outputs: vec![OutputData { address: [0; 32], value: 4, salt: [0; 32] }],
            salt: [0; 32],
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_zero_value_output() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(8);
        let owner_pk = wots::keygen(&seed);
        let output = OutputData { address: hash(b"r"), value: 0, salt: [0; 32] };
        let commit_salt = [0u8; 32];
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);
        let sig = wots::sign(&seed, &commitment);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 8, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_non_power_of_two_output() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(16);
        let owner_pk = wots::keygen(&seed);
        let output = OutputData { address: hash(b"r"), value: 3, salt: [0; 32] };
        let commit_salt = [0u8; 32];
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);
        let sig = wots::sign(&seed, &commitment);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn reveal_rejects_duplicate_inputs() {
        let (mut state, seed, _coin_id, input_salt) = state_with_coin(16);
        let owner_pk = wots::keygen(&seed);
        let same_input = InputReveal { owner_pk, value: 16, salt: input_salt };
        let tx = Transaction::Reveal {
            inputs: vec![same_input.clone(), same_input],
            signatures: vec![vec![0; wots::SIG_SIZE], vec![0; wots::SIG_SIZE]],
            outputs: vec![OutputData { address: [0; 32], value: 16, salt: [0; 32] }],
            salt: [0; 32],
        };
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    #[test]
    fn double_spend_prevented() {
        let (mut state, seed, coin_id, input_salt) = state_with_coin(16);
        let owner_pk = wots::keygen(&seed);

        // First spend
        let out1 = OutputData { address: hash(b"r1"), value: 8, salt: [1; 32] };
        let salt1 = [0xA0; 32];
        let commitment1 = do_commit(&mut state, &[coin_id], &[out1.coin_id()], &salt1);
        let sig1 = wots::sign(&seed, &commitment1);
        let tx1 = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig1)],
            outputs: vec![out1],
            salt: salt1,
        };
        apply_transaction(&mut state, &tx1).unwrap();

        // Second spend of same coin should fail
        let out2 = OutputData { address: hash(b"r2"), value: 4, salt: [2; 32] };
        let salt2 = [0xB0; 32];
        let commitment2 = do_commit(&mut state, &[coin_id], &[out2.coin_id()], &salt2);
        let sig2 = wots::sign(&seed, &commitment2);
        let tx2 = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 16, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig2)],
            outputs: vec![out2],
            salt: salt2,
        };
        assert!(apply_transaction(&mut state, &tx2).is_err());
    }

    // ── MSS signature path ──────────────────────────────────────────────

    #[test]
    fn reveal_with_mss_signature() {
        use crate::core::mss;

        let mut state = empty_state();
        let mss_seed = hash(b"mss test seed");
        let mut keypair = mss::keygen(&mss_seed, 4).unwrap();
        let master_pk = keypair.public_key(); // this is the owner_pk

        // The address = hash(master_pk)
        let address = compute_address(&master_pk);
        let input_salt = hash(b"mss coin salt");
        let value = 16u64;
        let coin_id = compute_coin_id(&address, value, &input_salt);
        state.coins.insert(coin_id);

        let output = OutputData { address: hash(b"dest"), value: 8, salt: [0; 32] };
        let commit_salt = [0xCC; 32];
        let commitment = do_commit(&mut state, &[coin_id], &[output.coin_id()], &commit_salt);

        // Sign with MSS
        let mss_sig = keypair.sign(&commitment).unwrap();
        let sig_bytes = mss_sig.to_bytes();

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk: master_pk, value, salt: input_salt }],
            signatures: vec![sig_bytes],
            outputs: vec![output],
            salt: commit_salt,
        };
        apply_transaction(&mut state, &tx).unwrap();
        assert!(!state.coins.contains(&coin_id));
    }

    // ── Duplicate commitment ────────────────────────────────────────────

    #[test]
    fn commit_duplicate_rejected() {
        let mut state = empty_state();
        let commitment = hash(b"dup commit");
        let nonce = mine_commit_nonce(&commitment);
        let tx = Transaction::Commit { commitment, spam_nonce: nonce };

        apply_transaction(&mut state, &tx.clone()).unwrap();
        assert!(apply_transaction(&mut state, &tx).is_err());
    }

    // ── validate_transaction (read-only) ────────────────────────────────

    #[test]
    fn validate_commit_does_not_mutate() {
        let state = empty_state();
        let commitment = hash(b"validate only");
        let nonce = mine_commit_nonce(&commitment);
        let tx = Transaction::Commit { commitment, spam_nonce: nonce };
        validate_transaction(&state, &tx).unwrap();
        // State should not have the commitment
        assert!(!state.commitments.contains(&commitment));
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/types.rs
══════════════════════════════════════════════════════════════

use serde::{Deserialize, Serialize};
use super::mmr::UtxoAccumulator;


/// Hash a byte slice with BLAKE3 (truncated to 32 bytes — BLAKE3 native output).
pub fn hash(data: &[u8]) -> [u8; 32] {
    *blake3::hash(data).as_bytes()
}

/// Concatenate two byte slices and hash them with BLAKE3.
pub fn hash_concat(a: &[u8], b: &[u8]) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(a);
    hasher.update(b);
    *hasher.finalize().as_bytes()
}

/// Compute a P2PKH address: address = BLAKE3(owner_pk).
pub fn compute_address(owner_pk: &[u8; 32]) -> [u8; 32] {
    hash(owner_pk)
}

/// Compute a coin ID that commits to address, value, and salt.
/// CoinID = BLAKE3(address || value_le_bytes || salt)
 /// The UTXO set stores ONLY this 32-byte hash.
pub fn compute_coin_id(address: &[u8; 32], value: u64, salt: &[u8; 32]) -> [u8; 32] {
     let mut hasher = blake3::Hasher::new();
     hasher.update(address);
     hasher.update(&value.to_le_bytes());
     hasher.update(salt);
     *hasher.finalize().as_bytes()
 }

/// Compute a commitment hash that binds inputs to outputs.
///
/// commitment = BLAKE3(coin_id_1 || ... || new_coin_id_1 || ... || salt)
pub fn compute_commitment(
    input_coins: &[[u8; 32]],
    new_coins: &[[u8; 32]],
    salt: &[u8; 32],
) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    for coin in input_coins {
        hasher.update(coin);
    }
    for coin in new_coins {
        hasher.update(coin);
    }
    hasher.update(salt);
    *hasher.finalize().as_bytes()
}

/// Decompose a value into power-of-2 denominations (its binary representation).
pub fn decompose_value(mut value: u64) -> Vec<u64> {
    let mut parts = Vec::new();
    let mut bit = 1u64;
    while value > 0 {
        if value & 1 == 1 {
            parts.push(bit);
        }
        value >>= 1;
        bit <<= 1;
    }
    parts
}

/// The global consensus state
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct State {
    pub midstate: [u8; 32],
    pub coins: UtxoAccumulator,
    pub commitments: UtxoAccumulator,
    pub depth: u64,
    pub target: [u8; 32],
    pub height: u64,
    pub timestamp: u64,
    #[serde(default)]
    pub commitment_heights: std::collections::HashMap<[u8; 32], u64>,  
}

impl State {
    pub fn genesis() -> (Self, Vec<CoinbaseOutput>) {
        use super::wots;

        // Bitcoin block anchor
        // Height: 935897
        // Hash: 00000000000000000000329a84d79877397ec0fa7c5aaa706a88e545daf599a5
        // Time: 2026-02-10 10:37:27 UTC
        const BITCOIN_BLOCK_HASH: &str = "00000000000000000000329a84d79877397ec0fa7c5aaa706a88e545daf599a5";
        const BITCOIN_BLOCK_HEIGHT: u64 = 935897;
        const BITCOIN_BLOCK_TIME: u64 = 1770719847;

        let anchor = hash(BITCOIN_BLOCK_HASH.as_bytes());

        const MERKLE_ROOT: &str = "6def077d292edb863bd64d2a8d8803ab12caf1eef9c76823ee01e9e47fce7d0d";
        let merkle_hash = hash(MERKLE_ROOT.as_bytes());

        // Genesis coinbase: INITIAL_REWARD decomposed into power-of-2 outputs.
        // Each output gets a deterministic seed and salt.
        let base_seed = hash_concat(&anchor, &merkle_hash);
        let denominations = decompose_value(INITIAL_REWARD);

        let genesis_coinbase: Vec<CoinbaseOutput> = denominations
            .iter()
            .enumerate()
            .map(|(i, &value)| {
                let seed = hash_concat(&base_seed, &(i as u64).to_le_bytes());
                let owner_pk = wots::keygen(&seed);
                let address = compute_address(&owner_pk);
                let salt = hash_concat(&seed, &[0xCBu8; 32]);
                CoinbaseOutput { address, value, salt }
            })
            .collect();

        // Initial difficulty target (very easy for testing)
        let target = [
            0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
        ];

        let initial_midstate = hash_concat(&anchor, &BITCOIN_BLOCK_HEIGHT.to_le_bytes());

        let state = Self {
            midstate: initial_midstate,
            coins: UtxoAccumulator::new(),
            commitments: UtxoAccumulator::new(),
            depth: 0,
            target,
            height: 0,
            timestamp: BITCOIN_BLOCK_TIME,
            commitment_heights: std::collections::HashMap::new(),
        };

        (state, genesis_coinbase)
    }
    pub fn header(&self) -> BatchHeader {
        BatchHeader {
            height: self.height,
            prev_midstate: [0u8; 32],
            post_tx_midstate: self.midstate,
            extension: Extension {
                nonce: 0,
                final_hash: self.midstate,
                checkpoints: vec![],
            },
            timestamp: self.timestamp,
            target: self.target,
        }
    }
}
impl Batch {
    /// Compute header from full batch by replaying transaction commitments
    pub fn header(&self) -> BatchHeader {
        let mut midstate = self.prev_midstate;

        // 1. Replay transaction commitments
        for tx in &self.transactions {
            match tx {
                Transaction::Commit { commitment, .. } => {
                    midstate = hash_concat(&midstate, commitment);
                }
                Transaction::Reveal { inputs, outputs, salt, .. } => {
                    let mut hasher = blake3::Hasher::new();
                    for i in inputs {
                        hasher.update(&i.coin_id());
                    }
                    for o in outputs {
                        hasher.update(&o.coin_id());
                    }
                    hasher.update(salt);
                    let tx_hash = *hasher.finalize().as_bytes();
                    midstate = hash_concat(&midstate, &tx_hash);
                }
            }
        }

        // 2. Replay coinbase
        for cb in &self.coinbase {
            midstate = hash_concat(&midstate, &cb.coin_id());
        }
 
        BatchHeader {
            height: 0, // Caller assigns this
            prev_midstate: self.prev_midstate,
            post_tx_midstate: midstate,
            extension: self.extension.clone(),
            timestamp: self.timestamp,
            target: self.target,
        }
    }
}

// ── Value-bearing data structures ───────────────────────────────────────────


/// Cleartext output data carried in a transaction.
/// Transmitted in the block, validated (value is power of 2), then discarded from state.
/// Only the resulting coin_id is stored in the UTXO set.
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct OutputData {
    pub address: [u8; 32],
    pub value: u64,
    pub salt: [u8; 32],
}

impl OutputData {
    pub fn coin_id(&self) -> [u8; 32] {
        compute_coin_id(&self.address, self.value, &self.salt)
    }
}

/// Cleartext input preimage carried in a reveal transaction.
/// Proves what value a coin holds by revealing the preimage of its coin_id.
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct InputReveal {
    pub owner_pk: [u8; 32],
    pub value: u64,
    pub salt: [u8; 32],
}

impl InputReveal {
    pub fn coin_id(&self) -> [u8; 32] {
        let address = compute_address(&self.owner_pk);
        compute_coin_id(&address, self.value, &self.salt)
    }
}

/// Coinbase output carried in a Batch. Same validation rules as OutputData.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct CoinbaseOutput {
    pub address: [u8; 32],
    pub value: u64,
    pub salt: [u8; 32],
}

impl CoinbaseOutput {
    pub fn coin_id(&self) -> [u8; 32] {
        compute_coin_id(&self.address, self.value, &self.salt)
    }
}

// ── Transaction ─────────────────────────────────────────────────────────────

/// A transaction is either a Commit or a Reveal
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum Transaction {
    /// Phase 1: Register a commitment binding inputs to outputs.
    Commit {
        commitment: [u8; 32],
        spam_nonce: u64,
    },

    /// Phase 2: Reveal and execute the spend with signatures.
    Reveal {
        /// Preimages proving what each input coin contains.
        inputs: Vec<InputReveal>,
        /// Signatures proving ownership (one per input, verified against input.owner_pk).
        signatures: Vec<Vec<u8>>,
        /// New coins to create. Value + salt revealed for validation, then discarded.
        outputs: Vec<OutputData>,
        /// Salt used when computing the commitment.
        salt: [u8; 32],
    },
}

impl Transaction {
    /// Coin IDs this transaction spends.
    pub fn input_coin_ids(&self) -> Vec<[u8; 32]> {
        match self {
            Transaction::Commit { .. } => vec![],
            Transaction::Reveal { inputs, .. } => inputs.iter().map(|i| i.coin_id()).collect(),
        }
    }

    /// Output coin IDs this transaction creates.
    pub fn output_coin_ids(&self) -> Vec<[u8; 32]> {
        match self {
            Transaction::Commit { .. } => vec![],
            Transaction::Reveal { outputs, .. } => outputs.iter().map(|o| o.coin_id()).collect(),
        }
    }

    /// Fee = sum(input values) - sum(output values). Zero for Commit.
    pub fn fee(&self) -> u64 {
        match self {
            Transaction::Commit { .. } => 0,
            Transaction::Reveal { inputs, outputs, .. } => {
                let in_sum: u64 = inputs.iter().map(|i| i.value).sum();
                let out_sum: u64 = outputs.iter().map(|o| o.value).sum();
                in_sum.saturating_sub(out_sum)
            }
        }
    }
}

/// Proof of sequential work with checkpoint witnesses
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Extension {
    pub nonce: u64,
    pub final_hash: [u8; 32],
    pub checkpoints: Vec<[u8; 32]>,
}

/// A batch of transactions plus proof of work
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Batch {
    /// The midstate of the previous batch this one extends
    pub prev_midstate: [u8; 32],
    pub transactions: Vec<Transaction>,
    pub extension: Extension,
    /// Coinbase outputs with revealed values. Each must be a power of 2.
    #[serde(default)]
    pub coinbase: Vec<CoinbaseOutput>,
    /// Block timestamp (seconds since Unix epoch)
    pub timestamp: u64,
    /// Target this batch was mined against
    pub target: [u8; 32],
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct BatchHeader {
    pub height: u64,
    pub prev_midstate: [u8; 32],
    pub post_tx_midstate: [u8; 32],
    pub extension: Extension,
    pub timestamp: u64,
    pub target: [u8; 32],
}



// ── Protocol constants ──────────────────────────────────────────────────────

#[cfg(not(feature = "fast-mining"))]
pub const EXTENSION_ITERATIONS: u64 = 1_000_000;
#[cfg(feature = "fast-mining")]
pub const EXTENSION_ITERATIONS: u64 = 100;

#[cfg(not(feature = "fast-mining"))]
pub const CHECKPOINT_INTERVAL: u64 = 1_000;
#[cfg(feature = "fast-mining")]
pub const CHECKPOINT_INTERVAL: u64 = 10;

#[cfg(not(feature = "fast-mining"))]
pub const SPOT_CHECK_COUNT: usize = 16;
#[cfg(feature = "fast-mining")]
pub const SPOT_CHECK_COUNT: usize = 3;

pub const MAX_BATCH_SIZE: usize = 100;

// ── Difficulty adjustment ───────────────────────────────────────────────────

pub const TARGET_BLOCK_TIME: u64 = 60;
/// ASERT half-life in seconds. Difficulty halves (or doubles) for every
/// half-life of drift between actual and ideal elapsed time since genesis.
pub const ASERT_HALF_LIFE: i64 = 4 * 60 * 60; // 4 hours (240 blocks)
/// Maximum number of recent timestamps kept for median-time-past validation
/// and timestamp window management in the node layer.
pub const DIFFICULTY_LOOKBACK: u64 = 60;
pub const MEDIAN_TIME_PAST_WINDOW: usize = 11;
pub const COMMITMENT_TTL: u64 = 100; 

/// Blocks behind tip before checkpoints are pruned from stored batches.
/// Pruning reclaims ~32 KB per block (~98% of batch storage). Pruned batches
/// remain fully verifiable via full-chain recomputation in verify_extension.
pub const PRUNE_DEPTH: u64 = 1000;
// ── Economics ───────────────────────────────────────────────────────────────

/// Blocks per year at TARGET_BLOCK_TIME seconds per block.
pub const BLOCKS_PER_YEAR: u64 = 365 * 24 * 3600 / TARGET_BLOCK_TIME; // 3_153_600

/// Initial block reward in value units.
pub const INITIAL_REWARD: u64 = 16;

pub const MAX_TX_INPUTS: usize = 256;
pub const MAX_TX_OUTPUTS: usize = 256;
/// Maximum size of a single signature in bytes.
/// WOTS = 576, MSS(height=20) = 1280. Pad for safety.
pub const MAX_SIGNATURE_SIZE: usize = 1_536;
/// Cap on total signature verifications per batch to prevent CPU exhaustion.
/// 100 txs × 256 inputs = 25,600 WOTS verifications ≈ 15B hashes — too much.
/// 1,024 total inputs keeps verification under ~600M hashes (~0.5s).
pub const MAX_BATCH_INPUTS: usize = 1_024;

/// Block reward value at a given height. Halves every BLOCKS_PER_YEAR, minimum 1.
pub fn block_reward(height: u64) -> u64 {
    let halvings = height / BLOCKS_PER_YEAR;
    if halvings >= 8 {
        1
    } else {
        (INITIAL_REWARD >> halvings).max(1)
    }
}

const _: () = assert!(
    EXTENSION_ITERATIONS % CHECKPOINT_INTERVAL == 0,
    "EXTENSION_ITERATIONS must be divisible by CHECKPOINT_INTERVAL"
);
// ============================================================
// ADD THIS ENTIRE BLOCK at the bottom of src/core/types.rs
// ============================================================

#[cfg(test)]
mod tests {
    use super::*;

    // ── hash / hash_concat ──────────────────────────────────────────────

    #[test]
    fn hash_deterministic() {
        assert_eq!(hash(b"hello"), hash(b"hello"));
    }

    #[test]
    fn hash_different_inputs_differ() {
        assert_ne!(hash(b"hello"), hash(b"world"));
    }

    #[test]
    fn hash_empty_input() {
        let h = hash(b"");
        assert_ne!(h, [0u8; 32]); // BLAKE3 of empty is defined, not zero
    }

    #[test]
    fn hash_concat_not_commutative() {
        let a = b"alpha";
        let b = b"beta";
        assert_ne!(hash_concat(a, b), hash_concat(b, a));
    }

    #[test]
    fn hash_concat_vs_manual() {
        // hash_concat(a,b) should equal BLAKE3(a || b)
        let a = b"foo";
        let b = b"bar";
        let expected = {
            let mut h = blake3::Hasher::new();
            h.update(a);
            h.update(b);
            *h.finalize().as_bytes()
        };
        assert_eq!(hash_concat(a, b), expected);
    }

    // ── compute_address ─────────────────────────────────────────────────

    #[test]
    fn compute_address_deterministic() {
        let pk = [0xAA; 32];
        assert_eq!(compute_address(&pk), compute_address(&pk));
    }

    #[test]
    fn compute_address_is_hash_of_pk() {
        let pk = [0xBB; 32];
        assert_eq!(compute_address(&pk), hash(&pk));
    }

    // ── compute_coin_id ─────────────────────────────────────────────────

    #[test]
    fn coin_id_deterministic() {
        let addr = [1u8; 32];
        let salt = [2u8; 32];
        assert_eq!(
            compute_coin_id(&addr, 16, &salt),
            compute_coin_id(&addr, 16, &salt),
        );
    }

    #[test]
    fn coin_id_differs_by_value() {
        let addr = [1u8; 32];
        let salt = [2u8; 32];
        assert_ne!(
            compute_coin_id(&addr, 8, &salt),
            compute_coin_id(&addr, 16, &salt),
        );
    }

    #[test]
    fn coin_id_differs_by_salt() {
        let addr = [1u8; 32];
        assert_ne!(
            compute_coin_id(&addr, 8, &[0u8; 32]),
            compute_coin_id(&addr, 8, &[1u8; 32]),
        );
    }

    #[test]
    fn coin_id_differs_by_address() {
        let salt = [2u8; 32];
        assert_ne!(
            compute_coin_id(&[0u8; 32], 8, &salt),
            compute_coin_id(&[1u8; 32], 8, &salt),
        );
    }

    // ── compute_commitment ──────────────────────────────────────────────

    #[test]
    fn commitment_deterministic() {
        let inputs = vec![[1u8; 32]];
        let outputs = vec![[2u8; 32]];
        let salt = [3u8; 32];
        assert_eq!(
            compute_commitment(&inputs, &outputs, &salt),
            compute_commitment(&inputs, &outputs, &salt),
        );
    }

    #[test]
    fn commitment_differs_with_different_salt() {
        let inputs = vec![[1u8; 32]];
        let outputs = vec![[2u8; 32]];
        assert_ne!(
            compute_commitment(&inputs, &outputs, &[0u8; 32]),
            compute_commitment(&inputs, &outputs, &[1u8; 32]),
        );
    }

    // ── decompose_value ─────────────────────────────────────────────────

    #[test]
    fn decompose_zero() {
        assert!(decompose_value(0).is_empty());
    }

    #[test]
    fn decompose_power_of_two() {
        assert_eq!(decompose_value(16), vec![16]);
        assert_eq!(decompose_value(1), vec![1]);
    }

    #[test]
    fn decompose_sums_correctly() {
        for v in [1, 7, 15, 100, 255, 1023, 65535] {
            let parts = decompose_value(v);
            assert_eq!(parts.iter().sum::<u64>(), v);
            for &p in &parts {
                assert!(p.is_power_of_two());
            }
        }
    }

    #[test]
    fn decompose_all_unique() {
        let parts = decompose_value(255);
        let mut sorted = parts.clone();
        sorted.sort();
        sorted.dedup();
        assert_eq!(parts.len(), sorted.len(), "decomposition should have unique denominations");
    }

    // ── block_reward ────────────────────────────────────────────────────

    #[test]
    fn block_reward_initial() {
        assert_eq!(block_reward(0), INITIAL_REWARD);
    }

    #[test]
    fn block_reward_first_halving() {
        assert_eq!(block_reward(BLOCKS_PER_YEAR), INITIAL_REWARD / 2);
    }

    #[test]
    fn block_reward_floor_at_one() {
        assert_eq!(block_reward(u64::MAX), 1);
        assert_eq!(block_reward(BLOCKS_PER_YEAR * 100), 1);
    }

    #[test]
    fn block_reward_monotonically_decreasing() {
        let mut prev = block_reward(0);
        for era in 1..=10 {
            let r = block_reward(BLOCKS_PER_YEAR * era);
            assert!(r <= prev);
            prev = r;
        }
    }

    // ── OutputData / InputReveal / CoinbaseOutput coin_id ───────────────

    #[test]
    fn output_data_coin_id() {
        let o = OutputData { address: [1u8; 32], value: 8, salt: [2u8; 32] };
        assert_eq!(o.coin_id(), compute_coin_id(&[1u8; 32], 8, &[2u8; 32]));
    }

    #[test]
    fn input_reveal_coin_id_uses_address() {
        let pk = [0xAA; 32];
        let ir = InputReveal { owner_pk: pk, value: 4, salt: [0u8; 32] };
        let expected_addr = compute_address(&pk);
        assert_eq!(ir.coin_id(), compute_coin_id(&expected_addr, 4, &[0u8; 32]));
    }

    #[test]
    fn coinbase_output_coin_id() {
        let cb = CoinbaseOutput { address: [5u8; 32], value: 16, salt: [6u8; 32] };
        assert_eq!(cb.coin_id(), compute_coin_id(&[5u8; 32], 16, &[6u8; 32]));
    }

    // ── Transaction methods ─────────────────────────────────────────────

    #[test]
    fn commit_fee_is_zero() {
        let tx = Transaction::Commit { commitment: [0u8; 32], spam_nonce: 0 };
        assert_eq!(tx.fee(), 0);
        assert!(tx.input_coin_ids().is_empty());
        assert!(tx.output_coin_ids().is_empty());
    }

    #[test]
    fn reveal_fee_computed() {
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk: [0u8; 32], value: 10, salt: [0u8; 32] }],
            signatures: vec![vec![]],
            outputs: vec![OutputData { address: [0u8; 32], value: 8, salt: [0u8; 32] }],
            salt: [0u8; 32],
        };
        assert_eq!(tx.fee(), 2);
    }

    #[test]
    fn reveal_input_output_coin_ids() {
        let input = InputReveal { owner_pk: [1u8; 32], value: 8, salt: [2u8; 32] };
        let output = OutputData { address: [3u8; 32], value: 4, salt: [4u8; 32] };
        let tx = Transaction::Reveal {
            inputs: vec![input.clone()],
            signatures: vec![vec![]],
            outputs: vec![output.clone()],
            salt: [0u8; 32],
        };
        assert_eq!(tx.input_coin_ids(), vec![input.coin_id()]);
        assert_eq!(tx.output_coin_ids(), vec![output.coin_id()]);
    }

    // ── State::genesis ──────────────────────────────────────────────────

    #[test]
    fn genesis_deterministic() {
        let (s1, cb1) = State::genesis();
        let (s2, cb2) = State::genesis();
        assert_eq!(s1.midstate, s2.midstate);
        assert_eq!(s1.height, 0);
        assert_eq!(cb1.len(), cb2.len());
        for (a, b) in cb1.iter().zip(cb2.iter()) {
            assert_eq!(a.coin_id(), b.coin_id());
        }
    }

    #[test]
    fn genesis_coinbase_values_sum_to_initial_reward() {
        let (_, cb) = State::genesis();
        let total: u64 = cb.iter().map(|c| c.value).sum();
        assert_eq!(total, INITIAL_REWARD);
    }

    #[test]
    fn genesis_coinbase_all_power_of_two() {
        let (_, cb) = State::genesis();
        for c in &cb {
            assert!(c.value.is_power_of_two(), "coinbase value {} not power of 2", c.value);
        }
    }
}




══════════════════════════════════════════════════════════════
 FILE: core/wots.rs
══════════════════════════════════════════════════════════════

use super::types::hash;
use rayon::prelude::*;

// ── Winternitz parameter w=16 ───────────────────────────────────────────────
//
// The message (32 bytes = 256 bits) is parsed as 16-bit digits:
//   256 / 16 = 16 message chains
//
// Checksum:
//   max_sum = 16 * 65535 = 1,048,560  (fits in 20 bits)
//   Encoded as 2 × 16-bit digits → 2 checksum chains
//
// Total: 16 + 2 = 18 chains
// Chain depth: 0..65535
// Signature size: 18 × 32 = 576 bytes  (was 1,088 at w=8)

pub const W: usize = 16;               // bits per digit
pub const MSG_CHAINS: usize = 16;      // 256 / W
pub const CHECKSUM_CHAINS: usize = 2;  // ceil(20 / 16)
pub const CHAINS: usize = MSG_CHAINS + CHECKSUM_CHAINS; // 18
pub const MAX_DIGIT: u32 = (1 << W) - 1; // 65_535
pub const SIG_SIZE: usize = CHAINS * 32; // 576 bytes

/// Iteratively hash `data` exactly `n` times using BLAKE3.
fn hash_n(data: &[u8; 32], n: u32) -> [u8; 32] {
    let mut x = *data;
    for _ in 0..n {
        x = hash(&x);
    }
    x
}

/// Derive chain secret key element: sk[i] = BLAKE3(seed || i)
fn chain_sk(seed: &[u8; 32], i: usize) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(seed);
    hasher.update(&(i as u32).to_le_bytes());
    *hasher.finalize().as_bytes()
}

/// Compress all chain endpoints into a single 32-byte coin ID.
fn compress(endpoints: &[[u8; 32]; CHAINS]) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    for ep in endpoints {
        hasher.update(ep);
    }
    *hasher.finalize().as_bytes()
}

/// Parse a 32-byte message into 16 × 16-bit digits (big-endian).
fn message_digits(msg: &[u8; 32]) -> [u32; MSG_CHAINS] {
    let mut digits = [0u32; MSG_CHAINS];
    for i in 0..MSG_CHAINS {
        digits[i] = u16::from_be_bytes([msg[i * 2], msg[i * 2 + 1]]) as u32;
    }
    digits
}

/// Compute the 2-digit checksum over the message digits.
///
/// checksum = Σ (MAX_DIGIT - d_i)  for all message digits
///
/// Max value: 16 × 65535 = 1,048,560 (0x000F_FFF0), fits in 20 bits.
/// Encoded big-endian into 2 × 16-bit digits.
fn checksum_digits(msg_digits: &[u32; MSG_CHAINS]) -> [u32; CHECKSUM_CHAINS] {
    let sum: u32 = msg_digits.iter().map(|&d| MAX_DIGIT - d).sum();
    [
        (sum >> 16) & 0xFFFF, // high 16 bits (at most 0x000F = 15)
        sum & 0xFFFF,         // low 16 bits
    ]
}

/// Combine message + checksum digits into the full digit vector.
fn all_digits(msg: &[u8; 32]) -> [u32; CHAINS] {
    let md = message_digits(msg);
    let cd = checksum_digits(&md);
    let mut digits = [0u32; CHAINS];
    digits[..MSG_CHAINS].copy_from_slice(&md);
    digits[MSG_CHAINS..].copy_from_slice(&cd);
    digits
}

/// Generate a coin ID (public key) from a seed (private key).
///
/// Each chain element is hashed MAX_DIGIT (65535) times to reach the endpoint.
/// Cost: CHAINS × MAX_DIGIT = 18 × 65535 ≈ 1.18M hashes.
/// With BLAKE3: ~1–2 ms on modern hardware.
pub fn keygen(seed: &[u8; 32]) -> [u8; 32] {
    let endpoints: Vec<[u8; 32]> = (0..CHAINS)
        .into_par_iter()
        .map(|i| {
            let sk_i = chain_sk(seed, i);
            hash_n(&sk_i, MAX_DIGIT)
        })
        .collect();
    let mut arr = [[0u8; 32]; CHAINS];
    arr.copy_from_slice(&endpoints);
    compress(&arr)
}

/// Sign a 32-byte message with the given seed.
///
/// For each digit d_i, reveals hash^{d_i}(sk_i).
/// The verifier can hash the remaining (MAX_DIGIT - d_i) times to reach the endpoint.
pub fn sign(seed: &[u8; 32], message: &[u8; 32]) -> Vec<[u8; 32]> {
    let digits = all_digits(message);
    (0..CHAINS)
        .into_par_iter()
        .map(|i| {
            let sk_i = chain_sk(seed, i);
            hash_n(&sk_i, digits[i])
        })
        .collect()
}

/// Verify a WOTS signature against a message and coin ID.
///
/// For each digit d_i, hashes sig[i] exactly (MAX_DIGIT - d_i) times
/// and checks that all endpoints compress to the coin ID.
///
/// Average verification cost: CHAINS × (MAX_DIGIT / 2) ≈ 590K hashes.
/// With BLAKE3: ~0.5–1 ms on modern hardware.
pub fn verify(sig: &[[u8; 32]], message: &[u8; 32], coin_id: &[u8; 32]) -> bool {
    if sig.len() != CHAINS {
        return false;
    }

    let digits = all_digits(message);
    let endpoints: Vec<[u8; 32]> = (0..CHAINS)
        .into_par_iter()
        .map(|i| {
            let remaining = MAX_DIGIT - digits[i];
            hash_n(&sig[i], remaining)
        })
        .collect();
    let mut arr = [[0u8; 32]; CHAINS];
    arr.copy_from_slice(&endpoints);
    compress(&arr) == *coin_id
}

/// Serialize signature to bytes (18 × 32 = 576 bytes).
pub fn sig_to_bytes(sig: &[[u8; 32]]) -> Vec<u8> {
    let mut out = Vec::with_capacity(sig.len() * 32);
    for chunk in sig {
        out.extend_from_slice(chunk);
    }
    out
}

/// Deserialize signature from bytes.
pub fn sig_from_bytes(bytes: &[u8]) -> Option<Vec<[u8; 32]>> {
    if bytes.len() != SIG_SIZE {
        return None;
    }
    let mut sig = Vec::with_capacity(CHAINS);
    for chunk in bytes.chunks_exact(32) {
        sig.push(<[u8; 32]>::try_from(chunk).unwrap());
    }
    Some(sig)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn sign_verify_round_trip() {
        let seed: [u8; 32] = [0x42; 32];
        let coin = keygen(&seed);
        let msg = hash(b"test message");
        let sig = sign(&seed, &msg);
        assert!(verify(&sig, &msg, &coin));
    }

    #[test]
    fn wrong_message_fails() {
        let seed: [u8; 32] = [0x42; 32];
        let coin = keygen(&seed);
        let msg = hash(b"test message");
        let sig = sign(&seed, &msg);
        let bad_msg = hash(b"wrong message");
        assert!(!verify(&sig, &bad_msg, &coin));
    }

    #[test]
    fn wrong_key_fails() {
        let seed: [u8; 32] = [0x42; 32];
        let msg = hash(b"test message");
        let sig = sign(&seed, &msg);
        let other_seed: [u8; 32] = [0x43; 32];
        let other_coin = keygen(&other_seed);
        assert!(!verify(&sig, &msg, &other_coin));
    }

    #[test]
    fn ser_deser_round_trip() {
        let seed: [u8; 32] = [0x42; 32];
        let msg = hash(b"test");
        let sig = sign(&seed, &msg);
        let bytes = sig_to_bytes(&sig);
        assert_eq!(bytes.len(), SIG_SIZE);
        assert_eq!(bytes.len(), 576);
        let sig2 = sig_from_bytes(&bytes).unwrap();
        assert_eq!(sig, sig2);
    }

    #[test]
    fn signature_size_is_576() {
        assert_eq!(CHAINS, 18);
        assert_eq!(SIG_SIZE, 576);
    }

    #[test]
    fn checksum_prevents_forgery() {
        // Verify that increasing a message digit forces a checksum digit to decrease,
        // which requires hashing *forward* on the checksum chain (infeasible).
        let msg1 = [0u8; 32];
        let msg2 = {
            let mut m = [0u8; 32];
            m[0] = 1; // increase first digit
            m
        };
        let d1 = all_digits(&msg1);
        let d2 = all_digits(&msg2);

        // Message digit increased
        assert!(d2[0] > d1[0]);

        // At least one checksum digit decreased
        let cs_decreased = (MSG_CHAINS..CHAINS).any(|i| d2[i] < d1[i]);
        assert!(cs_decreased, "checksum must decrease when a message digit increases");
    }

    #[test]
    fn digit_extraction() {
        let mut msg = [0u8; 32];
        // Set first two bytes to 0x0100 = 256
        msg[0] = 0x01;
        msg[1] = 0x00;
        let digits = message_digits(&msg);
        assert_eq!(digits[0], 256);
        assert_eq!(digits[1], 0);
    }

    #[test]
    fn max_checksum_fits() {
        // All-zero message → max checksum
        let msg = [0u8; 32];
        let md = message_digits(&msg);
        let cd = checksum_digits(&md);
        let sum: u32 = md.iter().map(|&d| MAX_DIGIT - d).sum();
        assert_eq!(sum, 16 * 65535); // 1,048,560
        // Must fit in 2 × 16-bit digits
        assert!(cd[0] <= MAX_DIGIT);
        assert!(cd[1] <= MAX_DIGIT);
    }

    #[test]
    fn all_ff_message() {
        // All-0xFF message → all digits = 65535 → checksum = 0
        let msg = [0xff; 32];
        let md = message_digits(&msg);
        for &d in &md {
            assert_eq!(d, 65535);
        }
        let cd = checksum_digits(&md);
        assert_eq!(cd[0], 0);
        assert_eq!(cd[1], 0);
    }

    #[test]
    fn sig_from_bytes_wrong_length() {
        assert!(sig_from_bytes(&[0u8; 100]).is_none());
        assert!(sig_from_bytes(&[0u8; SIG_SIZE + 1]).is_none());
        assert!(sig_from_bytes(&[]).is_none());
    }

    #[test]
    fn keygen_deterministic() {
        let seed = [0x42u8; 32];
        assert_eq!(keygen(&seed), keygen(&seed));
    }

    #[test]
    fn different_seeds_different_keys() {
        assert_ne!(keygen(&[1u8; 32]), keygen(&[2u8; 32]));
    }

    #[test]
    fn verify_wrong_length_sig_fails() {
        let seed = [0x42u8; 32];
        let coin = keygen(&seed);
        let msg = hash(b"test");
        // Too few chunks
        let short_sig: Vec<[u8; 32]> = vec![[0u8; 32]; CHAINS - 1];
        assert!(!verify(&short_sig, &msg, &coin));
    }



}




══════════════════════════════════════════════════════════════
 FILE: lib.rs
══════════════════════════════════════════════════════════════

pub mod core;
pub mod storage;
pub mod mempool;
pub mod network;
pub mod node;
pub mod rpc;
pub mod metrics;
pub mod sync;
pub mod wallet;
pub mod mix;

pub use core::types::*;




══════════════════════════════════════════════════════════════
 FILE: main.rs
══════════════════════════════════════════════════════════════

use anyhow::{Result, Context};
use clap::{Parser, Subcommand};
use midstate::*;
use midstate::compute_address;
use midstate::wallet::{self, Wallet, short_hex};
use midstate::core::wots;
use midstate::core::state::apply_batch;
use midstate::network::{MidstateNetwork, NetworkEvent, Message};
use std::path::PathBuf;
use std::time::Duration;
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};
use rayon::prelude::*;

// ── Config file ─────────────────────────────────────────────────────────────

#[derive(Debug, Default, serde::Deserialize, serde::Serialize)]
struct Config {
    /// Bootstrap peer multiaddrs
    #[serde(default)]
    bootstrap_peers: Vec<String>,
}

impl Config {
    fn load(path: &std::path::Path) -> Result<Self> {
        if !path.exists() {
            return Ok(Self::default());
        }
        let contents = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config: {}", path.display()))?;
        let config: Config = toml::from_str(&contents)
            .with_context(|| format!("Failed to parse config: {}", path.display()))?;
        Ok(config)
    }

    /// Create a default config file if it doesn't exist.
    fn create_default(path: &std::path::Path) -> Result<()> {
        if path.exists() {
            return Ok(());
        }
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        let default_contents = r#"# Midstate node configuration
            #
            # Bootstrap peers — full multiaddr with peer ID.
            # Get peer ID from the bootstrap node's startup logs.
            #
            # Example:
            # bootstrap_peers = [
            #     "/ip4/203.0.113.10/tcp/9333/p2p/12D3KooWAbCdEf...",
            #     "/ip4/198.51.100.5/tcp/9333/p2p/12D3KooWGhIjKl...",
            # ]

            bootstrap_peers = []
            "#;
        std::fs::write(path, default_contents)
            .with_context(|| format!("Failed to create default config: {}", path.display()))?;
        tracing::info!("Created default config at {}", path.display());
        Ok(())
    }
}

fn default_wallet_path() -> PathBuf {
    wallet::default_path()
}

#[derive(Parser)]
#[command(name = "midstate")]
#[command(about = "A minimal sequential-time cryptocurrency", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Command,
}

#[derive(Subcommand)]
enum Command {
    /// Run a node
    Node {
        #[arg(long, default_value = "./data")]
        data_dir: PathBuf,
        #[arg(long, default_value = "9333")]
        port: u16,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        #[arg(long)]
        peer: Vec<String>,
        #[arg(long)]
        mine: bool,
        #[arg(long)]
        listen: Option<String>,
        /// Path to config file (default: <data_dir>/config.toml)
        #[arg(long)]
        config: Option<PathBuf>,
    },

    /// Wallet operations
    Wallet {
        #[command(subcommand)]
        action: WalletAction,
    },

    /// Phase 1: Commit to a spend
    Commit {
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        #[arg(long)]
        coin: Vec<String>,
        #[arg(long)]
        dest: Vec<String>,
    },

    /// Check if a coin exists
    Balance {
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        #[arg(long)]
        coin: String,
    },

    /// Get current state
    State {
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
    },

    /// Get mempool info
    Mempool {
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
    },

    /// Get peer list
    Peers {
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
    },

    /// Generate a WOTS keypair
    Keygen {
        #[arg(long)]
        rpc_port: Option<u16>,
    },

    /// Sync from genesis
    Sync {
        #[arg(long, default_value = "./data")]
        data_dir: PathBuf,
        #[arg(long)]
        peer: String,
        #[arg(long, default_value = "9333")]
        port: u16,
    },
}

#[derive(Subcommand)]
enum WalletAction {
    Create {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
    },
    /// Generate a receiving address (WOTS key)
    Receive {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long)]
        label: Option<String>,
    },
    /// Generate multiple receiving keys
    Generate {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, short, default_value = "1")]
        count: usize,
        #[arg(long)]
        label: Option<String>,
    },
    /// Generate a reusable MSS address (Merkle Tree)
    GenerateMss {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "10")]
        height: u32,
        #[arg(long)]
        label: Option<String>,
    },
    List {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        #[arg(long)]
        full: bool,
    },
    Balance {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
    },
    /// Send value. --to format: <address_hex>:<value>
    Send {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        /// Explicit input coin IDs (optional, auto-selects if omitted)
        #[arg(long)]
        coin: Vec<String>,
        /// Recipient outputs: <address_hex>:<value>
        #[arg(long)]
        to: Vec<String>,
        #[arg(long, default_value = "120")]
        timeout: u64,
        #[arg(long)]
        private: bool,
    },
    /// Import a coin with known seed, value, and salt
    Import {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        /// WOTS seed (hex)
        #[arg(long)]
        seed: String,
        #[arg(long)]
        value: u64,
        #[arg(long)]
        salt: String,
        #[arg(long)]
        label: Option<String>,
    },
    Export {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long)]
        coin: String,
    },
    Pending {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
    },
    Reveal {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        #[arg(long)]
        commitment: Option<String>,
    },
    History {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, short, default_value = "20")]
        count: usize,
    },
    /// Import coinbase rewards from mining log
    ImportRewards {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long)]
        coinbase_file: PathBuf,
    },
    Scan {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
    },
    /// CoinJoin mix: create or join a mixing session
    Mix {
        #[arg(long, default_value_os_t = default_wallet_path())]
        path: PathBuf,
        #[arg(long, default_value = "8545")]
        rpc_port: u16,
        /// Denomination to mix (power of 2)
        #[arg(long)]
        denomination: u64,
        /// Explicit coin ID to mix (auto-selects if omitted)
        #[arg(long)]
        coin: Option<String>,
        /// Join an existing mix session (hex mix_id) instead of creating one
        #[arg(long)]
        join: Option<String>,
        /// Also pay the fee (requires a denomination-1 coin)
        #[arg(long)]
        pay_fee: bool,
        /// Timeout in seconds to wait for the mix to complete
        #[arg(long, default_value = "300")]
        timeout: u64,
    },
}

fn read_password(prompt: &str) -> Result<Vec<u8>> {
    if let Ok(val) = std::env::var("MIDSTATE_PASSWORD") {
        if val.is_empty() { anyhow::bail!("MIDSTATE_PASSWORD is set but empty"); }
        return Ok(val.into_bytes());
    }
    let input = rpassword::prompt_password(prompt)?;
    if input.is_empty() { anyhow::bail!("password cannot be empty"); }
    Ok(input.into_bytes())
}

fn read_password_confirm() -> Result<Vec<u8>> {
    if std::env::var("MIDSTATE_PASSWORD").is_ok() {
        return read_password(""); // env var skips confirmation
    }
    let p1 = read_password("Password: ")?;
    let p2 = read_password("Confirm:  ")?;
    if p1 != p2 { anyhow::bail!("passwords do not match"); }
    Ok(p1)
}

fn parse_hex32(s: &str) -> Result<[u8; 32]> {
    let bytes = hex::decode(s)?;
    if bytes.len() != 32 { anyhow::bail!("expected 32 bytes, got {}", bytes.len()); }
    Ok(<[u8; 32]>::try_from(bytes).unwrap())
}

fn parse_output_spec(s: &str) -> Result<([u8; 32], u64)> {
    let parts: Vec<&str> = s.splitn(2, ':').collect();
    if parts.len() != 2 {
        anyhow::bail!("expected format <owner_pk_hex>:<value>, got: {}", s);
    }
    let pk = parse_hex32(parts[0])?;
    let value: u64 = parts[1].parse()
        .map_err(|_| anyhow::anyhow!("invalid value: {}", parts[1]))?;
    if value == 0 {
        anyhow::bail!("value must be > 0");
    }
    Ok((pk, value))
}

fn format_age(secs: u64) -> String {
    if secs < 60 { format!("{}s ago", secs) }
    else if secs < 3600 { format!("{}m ago", secs / 60) }
    else if secs < 86400 { format!("{}h ago", secs / 3600) }
    else { format!("{}d ago", secs / 86400) }
}

fn now_secs() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs()
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::registry()
        .with(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| "midstate=info,tower_http=debug".into()),
        )
        .with(tracing_subscriber::fmt::layer())
        .init();

    let cli = Cli::parse();

    match cli.command {
        Command::Node { data_dir, port, rpc_port, peer, mine, listen, config } => {
            run_node(data_dir, port, rpc_port, peer, mine, listen, config).await
        }
        Command::Wallet { action } => handle_wallet(action).await,
        Command::Commit { rpc_port, coin, dest } => {
            commit_transaction(rpc_port, coin, dest).await
        }
        Command::Balance { rpc_port, coin } => check_balance(rpc_port, coin).await,
        Command::State { rpc_port } => get_state(rpc_port).await,
        Command::Mempool { rpc_port } => get_mempool(rpc_port).await,
        Command::Peers { rpc_port } => get_peers(rpc_port).await,
        Command::Keygen { rpc_port } => keygen(rpc_port).await,
        Command::Sync { data_dir, peer, port } => sync_from_genesis(data_dir, peer, port).await,
    }
}

// ── Wallet commands ─────────────────────────────────────────────────────────
async fn wallet_scan(path: &PathBuf, rpc_port: u16) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    let client = reqwest::Client::new();

    let addresses = wallet.watched_addresses();
    if addresses.is_empty() {
        println!("No addresses to scan for. Generate keys first.");
        return Ok(());
    }

    let state_url = format!("http://127.0.0.1:{}/state", rpc_port);
    let state: rpc::GetStateResponse = client.get(&state_url).send().await?.json().await?;
    let chain_height = state.height;
    let start = wallet.data.last_scan_height;

    if start >= chain_height {
        println!("Already scanned to height {}. Chain is at {}.", start, chain_height);
        return Ok(());
    }

    println!("Scanning blocks {}..{} for {} address(es)...", start, chain_height, addresses.len());

    let scan_url = format!("http://127.0.0.1:{}/scan", rpc_port);
    let req = rpc::ScanRequest {
        addresses: addresses.iter().map(|a| hex::encode(a)).collect(),
        start_height: start,
        end_height: chain_height,
    };
    let resp: rpc::ScanResponse = client.post(&scan_url).json(&req).send().await?.json().await?;

    let mut imported = 0usize;
    for sc in &resp.coins {
        let address = parse_hex32(&sc.address)?;
        let salt = parse_hex32(&sc.salt)?;
        if let Some(coin_id) = wallet.import_scanned(address, sc.value, salt)? {
            println!("  found: {} (value {}, height {})", short_hex(&coin_id), sc.value, sc.height);
            imported += 1;
        }
    }

    wallet.data.last_scan_height = chain_height;
    
    // Sync MSS key indices (stateful recovery)
    if !wallet.data.mss_keys.is_empty() {
        println!("Syncing MSS key indices...");
        let mss_url = format!("http://127.0.0.1:{}/mss_state", rpc_port);
        for mss_key in &mut wallet.data.mss_keys {
            let req = rpc::GetMssStateRequest {
                master_pk: hex::encode(mss_key.master_pk),
            };
            match client.post(&mss_url).json(&req).send().await {
                Ok(resp) => {
                    if let Ok(mss_resp) = resp.json::<rpc::GetMssStateResponse>().await {
                        if mss_resp.next_index >= mss_key.next_leaf {
                            const SAFETY_MARGIN: u64 = 20;
                            let new_leaf = mss_resp.next_index + SAFETY_MARGIN;
                            println!("  MSS {}: advancing leaf {} -> {}",
                                short_hex(&mss_key.master_pk), mss_key.next_leaf, new_leaf);
                            mss_key.next_leaf = new_leaf;
                        }
                    }
                }
                Err(e) => {
                    println!("  MSS {} sync failed: {}", short_hex(&mss_key.master_pk), e);
                }
            }
        }
    }
    
    
    wallet.save()?;

    println!("Scan complete. {} new coin(s) found. Scanned to height {}.", imported, chain_height);
    Ok(())
}


async fn handle_wallet(action: WalletAction) -> Result<()> {
    match action {
        WalletAction::Create { path } => wallet_create(&path),
        WalletAction::Receive { path, label } => wallet_receive(&path, label),
        WalletAction::Generate { path, count, label } => wallet_generate(&path, count, label),
        WalletAction::List { path, rpc_port, full } => wallet_list(&path, rpc_port, full).await,
        WalletAction::Balance { path, rpc_port } => wallet_balance(&path, rpc_port).await,
        WalletAction::Scan { path, rpc_port } => wallet_scan(&path, rpc_port).await,
        WalletAction::Send { path, rpc_port, coin, to, timeout, private } => {
            wallet_send(&path, rpc_port, coin, to, timeout, private).await
        }
        WalletAction::Import { path, seed, value, salt, label } => {
            wallet_import(&path, &seed, value, &salt, label)
        }
        WalletAction::Export { path, coin } => wallet_export(&path, &coin),
        WalletAction::Pending { path } => wallet_pending(&path),
        WalletAction::Reveal { path, rpc_port, commitment } => {
            wallet_reveal(&path, rpc_port, commitment).await
        }
        WalletAction::History { path, count } => wallet_history(&path, count),
        WalletAction::ImportRewards { path, coinbase_file } => {
            wallet_import_rewards(&path, &coinbase_file)
        }
        WalletAction::GenerateMss { path, height, label } => {
            wallet_generate_mss(&path, height, label)
        }
        WalletAction::Mix { path, rpc_port, denomination, coin, join, pay_fee, timeout } => {
            wallet_mix(&path, rpc_port, denomination, coin, join, pay_fee, timeout).await
        }
        
    }
}

fn wallet_create(path: &PathBuf) -> Result<()> {
    let password = read_password_confirm()?;
    Wallet::create(path, &password)?;
    println!("Wallet created: {}", path.display());
    Ok(())
}

fn wallet_receive(path: &PathBuf, label: Option<String>) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    let label = label.unwrap_or_else(|| format!("receive #{}", wallet.keys().len() + 1));
    let address = wallet.generate_key(Some(label.clone()))?;
    println!("\n  Your receiving address ({}):\n", label);
    println!("  {}\n", hex::encode(address));
    println!("  Share this with the sender.");
    Ok(())
}

fn wallet_generate(path: &PathBuf, count: usize, label: Option<String>) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    for i in 0..count {
        let lbl = if count == 1 {
            label.clone()
        } else {
            label.as_ref().map(|l| format!("{} #{}", l, i + 1))
        };
        let pk = wallet.generate_key(lbl)?;
        println!("  [{}] {}", wallet.keys().len() - 1, hex::encode(pk));
    }
    println!("\nGenerated {} key(s). Total keys: {}, Total coins: {}",
        count, wallet.keys().len(), wallet.coin_count());
    Ok(())
}

async fn wallet_list(path: &PathBuf, rpc_port: u16, full: bool) -> Result<()> {
    let password = read_password("Password: ")?;
    let wallet = Wallet::open(path, &password)?;
    let client = reqwest::Client::new();

    if wallet.coin_count() > 0 {
        println!("COINS:");
        if full {
            println!("{:<5} {:<66} {:<8} {:<10} {}", "#", "COIN_ID", "VALUE", "STATUS", "LABEL");
            println!("{}", "-".repeat(100));
        } else {
            println!("{:<5} {:<15} {:<8} {:<10} {}", "#", "COIN_ID", "VALUE", "STATUS", "LABEL");
            println!("{}", "-".repeat(55));
        }

        for (i, wc) in wallet.coins().iter().enumerate() {
            let coin_hex = hex::encode(wc.coin_id);
            let status = check_coin_rpc(&client, rpc_port, &coin_hex).await;
            let label = wc.label.as_deref().unwrap_or("");
            let status_str = match status {
                Ok(true) => "✓ live",
                Ok(false) => "✗ unset",
                Err(_) => "? error",
            };
            let display = if full { coin_hex } else { short_hex(&wc.coin_id) };
            println!("{:<5} {:<15} {:<8} {:<10} {}", i, display, wc.value, status_str, label);
        }
    }

    if !wallet.keys().is_empty() {
        println!("\nUNUSED RECEIVING KEYS:");
        for (i, k) in wallet.keys().iter().enumerate() {
            let display = if full { hex::encode(k.address) } else { short_hex(&k.address) };
            let label = k.label.as_deref().unwrap_or("");
            println!("  [K{}] {} {}", i, display, label);
        }
    }

    if !full { println!("\nUse --full to show complete IDs."); }
    Ok(())
}

async fn wallet_balance(path: &PathBuf, rpc_port: u16) -> Result<()> {
    let password = read_password("Password: ")?;
    let wallet = Wallet::open(path, &password)?;
    let client = reqwest::Client::new();

    let mut live_count = 0usize;
    let mut live_value = 0u64;
    for wc in wallet.coins() {
        if let Ok(true) = check_coin_rpc(&client, rpc_port, &hex::encode(wc.coin_id)).await {
            live_count += 1;
            live_value += wc.value;
        }
    }

    println!("Coins in wallet:  {}", wallet.coin_count());
    println!("Live on-chain:    {} (value: {})", live_count, live_value);
    println!("Unused keys:      {}", wallet.keys().len());
    println!("Pending commits:  {}", wallet.pending().len());
    Ok(())
}

async fn wallet_send(
    path: &PathBuf,
    rpc_port: u16,
    coin_args: Vec<String>,
    to_args: Vec<String>,
    timeout_secs: u64,
    private: bool,
) -> Result<()> {
    if to_args.is_empty() {
        anyhow::bail!("must specify at least one --to <owner_pk>:<value>");
    }

    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    let client = reqwest::Client::new();

    // MSS Index Verification
    if !wallet.data.mss_keys.is_empty() {
        println!("Connecting to node to verify MSS safety indices...");
        let mss_url = format!("http://127.0.0.1:{}/mss_state", rpc_port);

        for i in 0..wallet.data.mss_keys.len() {
            let master_pk = wallet.data.mss_keys[i].master_pk;
            let current_leaf = wallet.data.mss_keys[i].next_leaf;

            let req = rpc::GetMssStateRequest {
                master_pk: hex::encode(master_pk),
            };

            // STRICT SAFETY: We use context() to ensure that if the node is offline,
            // the program crashes here rather than risking a reuse of the private key.
            let response = client.post(&mss_url).json(&req).send().await
                .context("CRITICAL: Could not connect to node. Aborting to prevent MSS key reuse.")?;

            if !response.status().is_success() {
                anyhow::bail!("Safety Check Failed: Node returned error checking MSS state.");
            }

            let mss_resp: rpc::GetMssStateResponse = response.json().await
                .context("Safety Check Failed: Invalid response from node.")?;

            // If the node has seen more signatures than we have locally, FAST FORWARD.
            if mss_resp.next_index > current_leaf {
                const SAFETY_MARGIN: u64 = 20;
                let new_leaf = mss_resp.next_index + SAFETY_MARGIN;
                
                println!("  ⚠️  MSS Key {}: Old state detected (Node: {}, Local: {})", 
                    short_hex(&master_pk), mss_resp.next_index, current_leaf);
                println!("      Fast-forwarding index to {} to ensure safety.", new_leaf);
                
                wallet.data.mss_keys[i].set_next_leaf(new_leaf);
                
                // Save immediately. If save fails, we crash before signing.
                wallet.save().context("Failed to save updated wallet state")?;
            }
        }
        println!("  ✓ MSS indices verified safe.");
    }


    let recipient_specs: Vec<([u8; 32], u64)> = to_args.iter()
        .map(|s| parse_output_spec(s))
        .collect::<Result<Vec<_>>>()?;

    let total_send: u64 = recipient_specs.iter().map(|(_, v)| v).sum();
    let needed = total_send + 1;

    let mut live_coins = Vec::new();
    for wc in wallet.coins() {
        if let Ok(true) = check_coin_rpc(&client, rpc_port, &hex::encode(wc.coin_id)).await {
            live_coins.push(wc.coin_id);
        }
    }

    if private {
        let denoms: Vec<u64> = recipient_specs.iter().map(|(_, v)| *v).collect();
        let recipient_address = recipient_specs[0].0;
        let pairs = wallet.plan_private_send(&live_coins, &recipient_address, &denoms)?;
        println!("Private send: {} independent transaction(s)\n", pairs.len());

        for (pair_idx, (inputs, outputs, change_seeds)) in pairs.iter().enumerate() {
            let in_val: u64 = inputs.iter()
                .filter_map(|id| wallet.find_coin(id))
                .map(|c| c.value)
                .sum();
            let out_val: u64 = outputs.iter().map(|o| o.value).sum();
            println!("  Pair {}: {} in (value {}) → {} out (value {}, fee {})",
                pair_idx, inputs.len(), in_val, outputs.len(), out_val, in_val - out_val);

            let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();

            let (commitment, _salt) = wallet.prepare_commit(
                inputs, outputs, change_seeds.clone(), true
            )?;

            let commit_req = rpc::CommitRequest {
                coins: inputs.iter().map(|c| hex::encode(c)).collect(),
                destinations: output_coin_ids.iter().map(|d| hex::encode(d)).collect(),
            };

            let url = format!("http://127.0.0.1:{}/commit", rpc_port);
            let response = client.post(&url).json(&commit_req).send().await?;
            if !response.status().is_success() {
                let error: rpc::ErrorResponse = response.json().await?;
                println!("  Pair {} commit failed: {}", pair_idx, error.error);
                continue;
            }
            let commit_resp: rpc::CommitResponse = response.json().await?;
            let server_commitment = parse_hex32(&commit_resp.commitment)?;
            let server_salt = parse_hex32(&commit_resp.salt)?;

            wallet.data.pending.retain(|p| p.commitment != commitment);
            wallet.data.pending.push(wallet::PendingCommit {
                commitment: server_commitment,
                salt: server_salt,
                input_coin_ids: inputs.clone(),
                outputs: outputs.clone(),
                change_seeds: change_seeds.clone(),
                created_at: now_secs(),
                reveal_not_before: now_secs() + 10 + (rand::random::<u64>() % 41),
            });
            wallet.save()?;

            println!("  ✓ Commit submitted ({})", short_hex(&server_commitment));

            if !wait_for_commit_mined(&client, rpc_port, &commit_resp.commitment, timeout_secs).await {
                println!("  ⏳ Not mined yet. Run `wallet reveal` later.");
                continue;
            }

            let pending = wallet.find_pending(&server_commitment).unwrap().clone();
            let delay = pending.reveal_not_before.saturating_sub(now_secs());
            if delay > 0 {
                println!("  Waiting {}s (privacy delay)...", delay);
                tokio::time::sleep(Duration::from_secs(delay)).await;
            }

            do_reveal(&client, &mut wallet, rpc_port, &server_commitment, timeout_secs).await?;
        }
    } else {
        let input_coin_ids: Vec<[u8; 32]> = if !coin_args.is_empty() {
            coin_args.iter()
                .map(|s| wallet.resolve_coin(s))
                .collect::<Result<Vec<_>>>()?
        } else {
            wallet.select_coins(needed, &live_coins)?
        };

        let in_sum: u64 = input_coin_ids.iter()
            .filter_map(|id| wallet.find_coin(id))
            .map(|c| c.value)
            .sum();

        if in_sum <= total_send {
            anyhow::bail!("input value ({}) must exceed output value ({}) to pay fee", in_sum, total_send);
        }

        let fee = 1u64;
        let change = in_sum - total_send - fee;

        let mut all_outputs = Vec::new();
        let mut change_seeds = Vec::new();

        for (address, value) in &recipient_specs {
            for denom in decompose_value(*value) {
                let salt: [u8; 32] = rand::random();
                all_outputs.push(OutputData { address: *address, value: denom, salt });
            }
        }

        if change > 0 {
            let change_denoms = decompose_value(change);
            for denom in change_denoms {
                let seed: [u8; 32] = rand::random();
                let pk = wots::keygen(&seed);
                let addr = compute_address(&pk);
                let salt: [u8; 32] = rand::random();
                let idx = all_outputs.len();
                all_outputs.push(OutputData { address: addr, value: denom, salt });
                change_seeds.push((idx, seed));
            }
        }

        let output_coin_ids: Vec<[u8; 32]> = all_outputs.iter().map(|o| o.coin_id()).collect();

        println!(
            "Spending {} coin(s) (value {}) → {} output(s) (value {}, fee: {})",
            input_coin_ids.len(), in_sum,
            all_outputs.len(), total_send + change,
            fee
        );

        let (commitment, _salt) = wallet.prepare_commit(
            &input_coin_ids, &all_outputs, change_seeds.clone(), false
        )?;

        let commit_req = rpc::CommitRequest {
            coins: input_coin_ids.iter().map(|c| hex::encode(c)).collect(),
            destinations: output_coin_ids.iter().map(|d| hex::encode(d)).collect(),
        };

        let url = format!("http://127.0.0.1:{}/commit", rpc_port);
        let response = client.post(&url).json(&commit_req).send().await?;
        if !response.status().is_success() {
            let error: rpc::ErrorResponse = response.json().await?;
            anyhow::bail!("commit failed: {}", error.error);
        }
        let commit_resp: rpc::CommitResponse = response.json().await?;
        let server_commitment = parse_hex32(&commit_resp.commitment)?;
        let server_salt = parse_hex32(&commit_resp.salt)?;

        wallet.data.pending.retain(|p| p.commitment != commitment);
        wallet.data.pending.push(wallet::PendingCommit {
            commitment: server_commitment,
            salt: server_salt,
            input_coin_ids: input_coin_ids.clone(),
            outputs: all_outputs,
            change_seeds,
            created_at: now_secs(),
            reveal_not_before: 0,
        });
        wallet.save()?;

        println!("\n✓ Commit submitted ({})", short_hex(&server_commitment));
        println!("  Waiting for commit to be mined...");

        if !wait_for_commit_mined(&client, rpc_port, &commit_resp.commitment, timeout_secs).await {
            println!("⏳ Not mined after {}s. Run `wallet reveal` later.", timeout_secs);
            return Ok(());
        }
        println!("✓ Commit mined!");

        do_reveal(&client, &mut wallet, rpc_port, &server_commitment, timeout_secs).await?;
    }

    Ok(())
}

async fn wait_for_commit_mined(
    client: &reqwest::Client,
    rpc_port: u16,
    commitment_hex: &str,
    timeout_secs: u64,
) -> bool {
    let deadline = tokio::time::Instant::now() + Duration::from_secs(timeout_secs);
    while tokio::time::Instant::now() < deadline {
        tokio::time::sleep(Duration::from_secs(2)).await;
        let mp_url = format!("http://127.0.0.1:{}/mempool", rpc_port);
        if let Ok(resp) = client.get(&mp_url).send().await {
            if let Ok(mp) = resp.json::<rpc::GetMempoolResponse>().await {
                let still_pending = mp.transactions.iter().any(|tx| {
                    tx.commitment.as_deref() == Some(commitment_hex)
                });
                if !still_pending {
                    return true;
                }
            }
        }
        eprint!(".");
    }
    eprintln!();
    false
}

async fn do_reveal(
    client: &reqwest::Client,
    wallet: &mut Wallet,
    rpc_port: u16,
    commitment: &[u8; 32],
    timeout_secs: u64,
) -> Result<()> {
    let pending = wallet.find_pending(commitment)
        .ok_or_else(|| anyhow::anyhow!("pending commit not found"))?
        .clone();

    let (input_reveals, signatures) = wallet.sign_reveal(&pending)?;

    let reveal_url = format!("http://127.0.0.1:{}/send", rpc_port);
    let reveal_req = rpc::SendTransactionRequest {
        inputs: input_reveals.iter().map(|ir| rpc::InputRevealJson {
            owner_pk: hex::encode(ir.owner_pk),
            value: ir.value,
            salt: hex::encode(ir.salt),
        }).collect(),
        signatures: signatures.iter().map(|s| hex::encode(s)).collect(),
        outputs: pending.outputs.iter().map(|o| rpc::OutputDataJson {
            address: hex::encode(o.address),
            value: o.value,
            salt: hex::encode(o.salt),
        }).collect(),
        salt: hex::encode(pending.salt),
    };

    let response = client.post(&reveal_url).json(&reveal_req).send().await?;
    if !response.status().is_success() {
        let error: rpc::ErrorResponse = response.json().await?;
        anyhow::bail!("reveal failed: {}", error.error);
    }
    let _result: rpc::SendTransactionResponse = response.json().await?;

    let check_coin_hex = hex::encode(pending.input_coin_ids[0]);
    let deadline = tokio::time::Instant::now() + Duration::from_secs(timeout_secs);
    let mut revealed = false;
    while tokio::time::Instant::now() < deadline {
        tokio::time::sleep(Duration::from_secs(2)).await;
        if let Ok(resp) = client
            .post(&format!("http://127.0.0.1:{}/check", rpc_port))
            .json(&rpc::CheckCoinRequest { coin: check_coin_hex.clone() })
            .send().await
        {
            if let Ok(check) = resp.json::<rpc::CheckCoinResponse>().await {
                if !check.exists { revealed = true; break; }
            }
        }
        eprint!(".");
    }
    eprintln!();

    if !revealed {
        println!("⏳ Reveal submitted but not yet mined.");
        return Ok(());
    }

    wallet.complete_reveal(commitment)?;
    println!("✓ Transfer complete!");
    for id in &pending.input_coin_ids {
        let val = input_reveals.iter().find(|ir| ir.coin_id() == *id).map(|ir| ir.value).unwrap_or(0);
        println!("  spent:   {} (value {})", short_hex(id), val);
    }
    for out in &pending.outputs {
        println!("  created: {} (value {})", short_hex(&out.coin_id()), out.value);
    }
    Ok(())
}

// ── CoinJoin Mix ────────────────────────────────────────────────────────────

async fn wallet_mix(
    path: &PathBuf,
    rpc_port: u16,
    denomination: u64,
    coin_arg: Option<String>,
    join_mix_id: Option<String>,
    pay_fee: bool,
    timeout_secs: u64,
) -> Result<()> {
    if !denomination.is_power_of_two() || denomination == 0 {
        anyhow::bail!("denomination must be a non-zero power of 2");
    }

    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    let client = reqwest::Client::new();
    let base_url = format!("http://127.0.0.1:{}", rpc_port);

    // Find live coins
    let mut live_coins = Vec::new();
    for wc in wallet.coins() {
        if let Ok(true) = check_coin_rpc(&client, rpc_port, &hex::encode(wc.coin_id)).await {
            live_coins.push(wc.coin_id);
        }
    }

    // Select the coin to mix
    let mix_coin_id: [u8; 32] = if let Some(ref coin_ref) = coin_arg {
        let resolved = wallet.resolve_coin(coin_ref)?;
        if !live_coins.contains(&resolved) {
            anyhow::bail!("coin {} is not live on-chain", short_hex(&resolved));
        }
        let coin = wallet.find_coin(&resolved)
            .ok_or_else(|| anyhow::anyhow!("coin not in wallet"))?;
        if coin.value != denomination {
            anyhow::bail!(
                "coin {} has value {} but denomination is {}",
                short_hex(&resolved), coin.value, denomination
            );
        }
        resolved
    } else {
        // Auto-select a coin matching the denomination
        let found = wallet.coins().iter()
            .find(|c| c.value == denomination && live_coins.contains(&c.coin_id))
            .ok_or_else(|| anyhow::anyhow!(
                "no live coin with denomination {} found in wallet", denomination
            ))?;
        found.coin_id
    };

    println!("CoinJoin Mix");
    println!("  Denomination: {}", denomination);
    println!("  Input coin:   {}", short_hex(&mix_coin_id));

    // Step 1: Create or join a session
    let mix_id_hex: String = if let Some(ref join_hex) = join_mix_id {
        println!("  Joining session: {}", join_hex);
        join_hex.clone()
    } else {
        // Create new session
        let create_req = rpc::MixCreateRequest {
            denomination,
            min_participants: 2,
        };
        let resp = client.post(format!("{}/mix/create", base_url))
            .json(&create_req).send().await?;
        if !resp.status().is_success() {
            let error: rpc::ErrorResponse = resp.json().await?;
            anyhow::bail!("create failed: {}", error.error);
        }
        let create_resp: rpc::MixCreateResponse = resp.json().await?;
        println!("  Created session: {}", &create_resp.mix_id[..16]);
        println!("\n  Share this mix_id with other participants:");
        println!("  {}\n", create_resp.mix_id);
        create_resp.mix_id
    };

    // Step 2: Register our input/output
    let (input, output, output_seed) = wallet.prepare_mix_registration(&mix_coin_id)?;

    let register_req = rpc::MixRegisterRequest {
        mix_id: mix_id_hex.clone(),
        coin_id: hex::encode(mix_coin_id),
        input: rpc::InputRevealJson {
            owner_pk: hex::encode(input.owner_pk),
            value: input.value,
            salt: hex::encode(input.salt),
        },
        output: rpc::OutputDataJson {
            address: hex::encode(output.address),
            value: output.value,
            salt: hex::encode(output.salt),
        },
    };

    let resp = client.post(format!("{}/mix/register", base_url))
        .json(&register_req).send().await?;
    if !resp.status().is_success() {
        let error: rpc::ErrorResponse = resp.json().await?;
        anyhow::bail!("register failed: {}", error.error);
    }
    println!("  ✓ Registered in mix session");

    // Step 3: Optionally pay the fee
    let mut fee_coin_id: Option<[u8; 32]> = None;
    if pay_fee {
        match wallet.prepare_mix_fee(&live_coins) {
            Ok((fee_input, fee_cid)) => {
                let fee_req = rpc::MixFeeRequest {
                    mix_id: mix_id_hex.clone(),
                    input: rpc::InputRevealJson {
                        owner_pk: hex::encode(fee_input.owner_pk),
                        value: fee_input.value,
                        salt: hex::encode(fee_input.salt),
                    },
                };
                let resp = client.post(format!("{}/mix/fee", base_url))
                    .json(&fee_req).send().await?;
                if !resp.status().is_success() {
                    let error: rpc::ErrorResponse = resp.json().await?;
                    anyhow::bail!("fee failed: {}", error.error);
                }
                println!("  ✓ Fee coin registered ({})", short_hex(&fee_cid));
                fee_coin_id = Some(fee_cid);
            }
            Err(e) => {
                println!("  ⚠ Cannot pay fee: {}", e);
                println!("    Another participant must contribute a denomination-1 coin.");
            }
        }
    }

    // Step 4: Wait for signing phase
    println!("\n  Waiting for all participants to register...");
    let deadline = tokio::time::Instant::now() + Duration::from_secs(timeout_secs);
    let mut commitment_hex = String::new();

    loop {
        if tokio::time::Instant::now() >= deadline {
            anyhow::bail!("timed out waiting for signing phase");
        }
        tokio::time::sleep(Duration::from_secs(2)).await;

        let status_url = format!("{}/mix/status/{}", base_url, mix_id_hex);
        let resp = client.get(&status_url).send().await?;
        if !resp.status().is_success() {
            anyhow::bail!("mix session not found");
        }
        let status: rpc::MixStatusResponse = resp.json().await?;

        match status.phase.as_str() {
            "collecting" => { eprint!("."); }
            "signing" => {
                eprintln!();
                commitment_hex = status.commitment.unwrap_or_default();
                println!("  ✓ All participants registered! Signing phase.");
                break;
            }
            "failed" => {
                anyhow::bail!("mix session failed");
            }
            other => {
                println!("  Unexpected phase: {}", other);
                break;
            }
        }
    }

    // Step 5: Sign our input(s)
    let commitment = parse_hex32(&commitment_hex)?;

    // Sign the mix input
    let sig = wallet.sign_mix_input(&mix_coin_id, &commitment)?;
    let sign_req = rpc::MixSignRequest {
        mix_id: mix_id_hex.clone(),
        coin_id: hex::encode(mix_coin_id),
        signature: hex::encode(&sig),
    };
    let resp = client.post(format!("{}/mix/sign", base_url))
        .json(&sign_req).send().await?;
    if !resp.status().is_success() {
        let error: rpc::ErrorResponse = resp.json().await?;
        anyhow::bail!("sign failed: {}", error.error);
    }
    println!("  ✓ Signed mix input");

    // Sign fee input if we're paying
    if let Some(fee_cid) = fee_coin_id {
        let fee_sig = wallet.sign_mix_input(&fee_cid, &commitment)?;
        let fee_sign_req = rpc::MixSignRequest {
            mix_id: mix_id_hex.clone(),
            coin_id: hex::encode(fee_cid),
            signature: hex::encode(&fee_sig),
        };
        let resp = client.post(format!("{}/mix/sign", base_url))
            .json(&fee_sign_req).send().await?;
        if !resp.status().is_success() {
            let error: rpc::ErrorResponse = resp.json().await?;
            anyhow::bail!("fee sign failed: {}", error.error);
        }
        println!("  ✓ Signed fee input");
    }

    // Step 6: Wait for completion
    println!("\n  Waiting for all signatures and on-chain confirmation...");
    loop {
        if tokio::time::Instant::now() >= deadline {
            println!("  ⏳ Timed out. The mix may still complete — check status later.");
            break;
        }
        tokio::time::sleep(Duration::from_secs(3)).await;

        let status_url = format!("{}/mix/status/{}", base_url, mix_id_hex);
        let resp = client.get(&status_url).send().await?;
        if !resp.status().is_success() { break; }
        let status: rpc::MixStatusResponse = resp.json().await?;

        match status.phase.as_str() {
            "signing" => { eprint!("."); }
            "commit_submitted" => { eprint!("c"); }
            "complete" => {
                eprintln!();
                // Update wallet: remove spent coin(s), import mixed output
                let mut spent = vec![mix_coin_id];
                if let Some(fee_cid) = fee_coin_id {
                    spent.push(fee_cid);
                }
                wallet.complete_mix(&spent, &output, output_seed)?;

                println!("\n  ✓ CoinJoin mix complete!");
                println!("  Spent:    {}", short_hex(&mix_coin_id));
                println!("  Received: {} (value {})", short_hex(&output.coin_id()), output.value);
                if let Some(fee_cid) = fee_coin_id {
                    println!("  Fee paid: {} (value 1)", short_hex(&fee_cid));
                }
                return Ok(());
            }
            "failed" => {
                eprintln!();
                anyhow::bail!("mix failed");
            }
            _ => { eprint!("?"); }
        }
    }

    Ok(())
}

fn wallet_import(path: &PathBuf, seed_hex: &str, value: u64, salt_hex: &str, label: Option<String>) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;
    let seed = parse_hex32(seed_hex)?;
    let salt = parse_hex32(salt_hex)?;
    let coin_id = wallet.import_coin(seed, value, salt, label)?;
    println!("Imported: {} (value {})", short_hex(&coin_id), value);
    Ok(())
}

fn wallet_export(path: &PathBuf, coin_ref: &str) -> Result<()> {
    let password = read_password("Password: ")?;
    let wallet = Wallet::open(path, &password)?;
    let coin_id = wallet.resolve_coin(coin_ref)?;
    let wc = wallet.find_coin(&coin_id)
        .ok_or_else(|| anyhow::anyhow!("coin not found in wallet"))?;
    println!("Seed:    {}", hex::encode(wc.seed));
    println!("Value:   {}", wc.value);
    println!("Salt:    {}", hex::encode(wc.salt));
    println!("CoinID:  {}", hex::encode(wc.coin_id));
    println!("Address: {}", hex::encode(wc.address));
    println!("\n⚠️  Anyone with the seed + value + salt can spend this coin.");
    Ok(())
}

fn wallet_pending(path: &PathBuf) -> Result<()> {
    let password = read_password("Password: ")?;
    let wallet = Wallet::open(path, &password)?;
    let pending = wallet.pending();
    if pending.is_empty() {
        println!("No pending commits.");
        return Ok(());
    }
    println!("{} pending commit(s):\n", pending.len());
    for (i, p) in pending.iter().enumerate() {
        let age = now_secs().saturating_sub(p.created_at);
        let out_val: u64 = p.outputs.iter().map(|o| o.value).sum();
        println!(
            "  [{}] {} — {} in, {} out (value {}), {}",
            i, short_hex(&p.commitment),
            p.input_coin_ids.len(),
            p.outputs.len(),
            out_val,
            format_age(age),
        );
    }
    Ok(())
}

fn wallet_history(path: &PathBuf, count: usize) -> Result<()> {
    let password = read_password("Password: ")?;
    let wallet = Wallet::open(path, &password)?;
    let history = wallet.history();
    if history.is_empty() {
        println!("No transaction history.");
        return Ok(());
    }
    let start = history.len().saturating_sub(count);
    let entries = &history[start..];
    println!("Transaction history ({} of {}):\n", entries.len(), history.len());
    for (i, entry) in entries.iter().enumerate() {
        let age = now_secs().saturating_sub(entry.timestamp);
        println!("  [{}] {} (fee: {})", start + i, format_age(age), entry.fee);
        for c in &entry.inputs { println!("    spent:   {}", short_hex(c)); }
        for c in &entry.outputs { println!("    created: {}", short_hex(c)); }
        println!();
    }
    Ok(())
}

async fn wallet_reveal(
    path: &PathBuf,
    rpc_port: u16,
    commitment_hex: Option<String>,
) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;

    let targets: Vec<[u8; 32]> = if let Some(hex) = commitment_hex {
        vec![parse_hex32(&hex)?]
    } else {
        wallet.pending().iter().map(|p| p.commitment).collect()
    };

    if targets.is_empty() {
        println!("No pending commits to reveal.");
        return Ok(());
    }

    let client = reqwest::Client::new();

    for commitment in targets {
        let pending = match wallet.find_pending(&commitment) {
            Some(p) => p.clone(),
            None => {
                println!("  {} — not found, skipping", short_hex(&commitment));
                continue;
            }
        };

        if pending.reveal_not_before > now_secs() {
            let wait = pending.reveal_not_before - now_secs();
            println!("  {} — waiting {}s (privacy delay)", short_hex(&commitment), wait);
            tokio::time::sleep(Duration::from_secs(wait)).await;
        }

        let (input_reveals, signatures) = wallet.sign_reveal(&pending)?;

        let url = format!("http://127.0.0.1:{}/send", rpc_port);
        let req = rpc::SendTransactionRequest {
            inputs: input_reveals.iter().map(|ir| rpc::InputRevealJson {
                owner_pk: hex::encode(ir.owner_pk),
                value: ir.value,
                salt: hex::encode(ir.salt),
            }).collect(),
            signatures: signatures.iter().map(|s| hex::encode(s)).collect(),
            outputs: pending.outputs.iter().map(|o| rpc::OutputDataJson {
                address: hex::encode(o.address),
                value: o.value,
                salt: hex::encode(o.salt),
            }).collect(),
            salt: hex::encode(pending.salt),
        };

        let response = client.post(&url).json(&req).send().await?;
        if response.status().is_success() {
            let _result: rpc::SendTransactionResponse = response.json().await?;
            wallet.complete_reveal(&commitment)?;
            println!("  {} — revealed ✓", short_hex(&commitment));
        } else {
            let error: rpc::ErrorResponse = response.json().await?;
            println!("  {} — failed: {}", short_hex(&commitment), error.error);
        }
    }
    Ok(())
}

fn wallet_import_rewards(path: &PathBuf, coinbase_file: &PathBuf) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;

    println!("Reading coinbase log...");
    let contents = std::fs::read_to_string(coinbase_file)?;

    #[derive(serde::Deserialize)]
    struct CoinbaseEntry {
        #[allow(dead_code)]
        height: u64,
        #[allow(dead_code)]
        index: u64,
        seed: String,
        #[serde(rename = "coin")]
        _coin: String,
        value: u64,
        salt: String,
    }

    let entries: Vec<CoinbaseEntry> = contents
        .lines()
        .filter(|l| !l.trim().is_empty())
        .filter_map(|l| serde_json::from_str(l).ok())
        .collect();

    println!("Found {} rewards. Importing...", entries.len());

    let new_coins: Vec<wallet::WalletCoin> = entries
        .into_par_iter()
        .map(|entry| {
            let seed = parse_hex32(&entry.seed).unwrap();
            let salt = parse_hex32(&entry.salt).unwrap();
            let owner_pk = wots::keygen(&seed);
            let address = compute_address(&owner_pk);
            let coin_id = compute_coin_id(&address, entry.value, &salt);

            wallet::WalletCoin {
                seed,
                owner_pk,
                address,
                value: entry.value,
                salt,
                coin_id,
                label: Some(format!("coinbase (value {})", entry.value)),
            }
        })
        .collect();

    let existing_coins: std::collections::HashSet<_> = wallet.data.coins
        .iter()
        .map(|c| c.coin_id)
        .collect();

    let mut imported = 0usize;
    for wc in new_coins {
        if !existing_coins.contains(&wc.coin_id) {
            wallet.data.coins.push(wc);
            imported += 1;
        }
    }

    println!("Saving wallet...");
    wallet.save()?;

    println!("Imported {} coinbase reward(s). Total coins: {}, total value: {}",
        imported, wallet.coin_count(), wallet.total_value());
    Ok(())
}

fn wallet_generate_mss(path: &PathBuf, height: u32, label: Option<String>) -> Result<()> {
    let password = read_password("Password: ")?;
    let mut wallet = Wallet::open(path, &password)?;

    let capacity = 1u64 << height;
    println!("Generating MSS tree (Height {} = {} signatures)...", height, capacity);
    if height > 12 {
        println!("(This might take a minute...)");
    }

    let root = wallet.generate_mss(height, label.clone())?;

    println!("\n✓ MSS Address Generated!");
    if let Some(l) = label {
        println!("  Label:    {}", l);
    }
    println!("  Capacity: {} signatures", capacity);
    println!("  Address:  {}", hex::encode(root));
    println!("\nThis address is reusable until the capacity is exhausted.");

    Ok(())
}

// ── Helpers ─────────────────────────────────────────────────────────────────

async fn check_coin_rpc(client: &reqwest::Client, rpc_port: u16, coin_hex: &str) -> Result<bool> {
    let url = format!("http://127.0.0.1:{}/check", rpc_port);
    let req = rpc::CheckCoinRequest { coin: coin_hex.to_string() };
    let resp: rpc::CheckCoinResponse = client.post(&url).json(&req).send().await?.json().await?;
    Ok(resp.exists)
}

// ── Original commands ───────────────────────────────────────────────────────

async fn run_node(
    data_dir: PathBuf, port: u16, rpc_port: u16, cli_peers: Vec<String>,
    mine: bool, listen: Option<String>, config_path: Option<PathBuf>,
) -> Result<()> {
    // Load config: explicit --config path, or <data_dir>/config.toml
    let config_file = config_path.unwrap_or_else(|| data_dir.join("config.toml"));
    Config::create_default(&config_file)?;
    let config = Config::load(&config_file)?;

    // Merge: config file peers first, then CLI --peer flags on top, dedup
    let mut all_peers = config.bootstrap_peers;
    all_peers.extend(cli_peers);
    all_peers.sort();
    all_peers.dedup();

    if all_peers.is_empty() {
        tracing::warn!("No bootstrap peers configured. Add peers to {} or use --peer", config_file.display());
    } else {
        tracing::info!("Bootstrap peers: {} (config: {})", all_peers.len(), config_file.display());
    }

    let listen_addr: libp2p::Multiaddr = match listen {
        Some(addr) => addr.parse()?,
        None => format!("/ip4/0.0.0.0/tcp/{}", port).parse()?,
    };

    let bootstrap: Vec<libp2p::Multiaddr> = all_peers.iter()
        .map(|a| a.parse())
        .collect::<Result<Vec<_>, _>>()
        .context("Invalid peer multiaddr (expected e.g. /ip4/1.2.3.4/tcp/9333/p2p/12D3KooW...)")?;

    let node = node::Node::new(data_dir, mine, listen_addr, bootstrap).await?;
    let (handle, cmd_rx) = node.create_handle();

    let rpc_server = rpc::RpcServer::new(rpc_port);
    let handle_clone = handle.clone();
    tokio::spawn(async move {
        if let Err(e) = rpc_server.run(handle_clone).await {
            tracing::error!("RPC server error: {}", e);
        }
    });
    tracing::info!("Node started (mining: {}, rpc: {})", mine, rpc_port);
    node.run(handle, cmd_rx).await
}

async fn commit_transaction(rpc_port: u16, coins: Vec<String>, destinations: Vec<String>) -> Result<()> {
    if coins.is_empty() { anyhow::bail!("Must provide at least one coin"); }
    if destinations.is_empty() { anyhow::bail!("Must provide at least one destination"); }

    let client = reqwest::Client::new();
    let url = format!("http://127.0.0.1:{}/commit", rpc_port);
    let req = rpc::CommitRequest { coins, destinations };
    let response = client.post(&url).json(&req).send().await?;

    if response.status().is_success() {
        let result: rpc::CommitResponse = response.json().await?;
        println!("Commitment submitted!");
        println!("  Commitment: {}", result.commitment);
        println!("  Salt:       {}", result.salt);
    } else {
        let error: rpc::ErrorResponse = response.json().await?;
        println!("Error: {}", error.error);
    }
    Ok(())
}

async fn check_balance(rpc_port: u16, coin: String) -> Result<()> {
    let client = reqwest::Client::new();
    let url = format!("http://127.0.0.1:{}/check", rpc_port);
    let req = rpc::CheckCoinRequest { coin };
    let response = client.post(&url).json(&req).send().await?;
    if response.status().is_success() {
        let result: rpc::CheckCoinResponse = response.json().await?;
        println!("Coin: {}", result.coin);
        println!("Exists: {}", if result.exists { "YES ✓" } else { "NO ✗" });
    } else {
        let error: rpc::ErrorResponse = response.json().await?;
        println!("Error: {}", error.error);
    }
    Ok(())
}

async fn get_state(rpc_port: u16) -> Result<()> {
    let client = reqwest::Client::new();
    let url = format!("http://127.0.0.1:{}/state", rpc_port);
    let response: rpc::GetStateResponse = client.get(&url).send().await?.json().await?;
    println!("State:");
    println!("  Height:       {}", response.height);
    println!("  Depth:        {}", response.depth);
    println!("  Safe Depth:   {} blocks (1e-6 risk)", response.safe_depth);
    println!("  Coins:        {}", response.num_coins);
    println!("  Commitments:  {}", response.num_commitments);
    println!("  Midstate:     {}", response.midstate);
    println!("  Target:       {}", response.target);
    println!("  Block reward: {}", response.block_reward);
    Ok(())
}

async fn get_mempool(rpc_port: u16) -> Result<()> {
    let client = reqwest::Client::new();
    let url = format!("http://127.0.0.1:{}/mempool", rpc_port);
    let response: rpc::GetMempoolResponse = client.get(&url).send().await?.json().await?;
    println!("Mempool: {} transaction(s)", response.size);
    for (i, tx) in response.transactions.iter().enumerate() {
        if let Some(ref c) = tx.commitment { println!("  {} [COMMIT]: {}", i + 1, c); }
        if let Some(ref inputs) = tx.input_coins {
            println!("  {} [REVEAL]:", i + 1);
            for (j, input) in inputs.iter().enumerate() { println!("    Input {}: {}", j, input); }
        }
        if let Some(ref outputs) = tx.output_coins {
            for (j, output) in outputs.iter().enumerate() { println!("    Output {}: {}", j, output); }
        }
        if let Some(fee) = tx.fee {
            println!("    Fee: {}", fee);
        }
    }
    Ok(())
}

async fn get_peers(rpc_port: u16) -> Result<()> {
    let client = reqwest::Client::new();
    let url = format!("http://127.0.0.1:{}/peers", rpc_port);
    let response: rpc::GetPeersResponse = client.get(&url).send().await?.json().await?;
    println!("Peers: {}", response.peers.len());
    for peer in response.peers { println!("  {}", peer); }
    Ok(())
}

async fn keygen(rpc_port: Option<u16>) -> Result<()> {
    if let Some(port) = rpc_port {
        let client = reqwest::Client::new();
        let url = format!("http://127.0.0.1:{}/keygen", port);
        let response: rpc::GenerateKeyResponse = client.get(&url).send().await?.json().await?;
        println!("Generated WOTS keypair:");
        println!("  Seed:     {}", response.seed);
        println!("  Address:  {}", response.address);
    } else {
        let seed: [u8; 32] = rand::random();
        let owner_pk = wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        println!("Generated WOTS keypair:");
        println!("  Seed:     {}", hex::encode(seed));
        println!("  Address:  {}", hex::encode(address));
    }
    println!("\n⚠️  Keep the seed safe! Anyone with it can spend coins sent to this address.");
    Ok(())
}

async fn sync_from_genesis(data_dir: PathBuf, peer_addr: String, port: u16) -> Result<()> {
    let storage = storage::Storage::open(data_dir.join("db"))?;
    let syncer = sync::Syncer::new(storage.clone());

    let listen_addr: libp2p::Multiaddr = format!("/ip4/127.0.0.1/tcp/{}", port).parse()?;
    let peer_multiaddr: libp2p::Multiaddr = peer_addr.parse()
       .context("Invalid peer multiaddr (expected e.g. /ip4/1.2.3.4/tcp/9333/p2p/12D3KooW...)")?;

    let keypair = libp2p::identity::Keypair::generate_ed25519();
    let mut network = MidstateNetwork::new(keypair, listen_addr, vec![peer_multiaddr]).await?;

    // Wait for connection
    let peer_id = loop {
        match network.next_event().await {
            NetworkEvent::PeerConnected(id) => break id,
            _ => continue,
        }
    };

    // 1. Ask peer for state
    network.send(peer_id, Message::GetState);
    let (peer_height, _peer_depth) = loop {
        match network.next_event().await {
            NetworkEvent::MessageReceived {
                message: Message::StateInfo { height, depth, .. }, ..
            } => break (height, depth),
            _ => continue,
        }
    };
    println!("Peer at height {}", peer_height);

    // 2. Download headers
    let mut headers = Vec::new();
    let mut cursor = 0u64;
    while cursor < peer_height {
        let count = 100.min(peer_height - cursor);
        network.send(peer_id, Message::GetHeaders { start_height: cursor, count });
        let received = loop {
            match network.next_event().await {
                NetworkEvent::MessageReceived {
                    message: Message::Headers { headers, .. }, ..
                } => break headers,
                _ => continue,
            }
        };
        if received.is_empty() { anyhow::bail!("Peer sent empty headers at {}", cursor); }
        cursor += received.len() as u64;
        headers.extend(received);
    }

    // 3. Verify
    sync::Syncer::verify_header_chain(&headers)?;
    let our_state = storage.load_state()?.unwrap_or_else(|| State::genesis().0);
    let fork_height = syncer.find_fork_point(&headers, our_state.height)?;

    // 4. Download and apply batches
    let mut state = syncer.rebuild_state_to(fork_height)?;
    let mut recent_headers: Vec<u64> = Vec::new();
    let mut dl_cursor = fork_height;
    while dl_cursor < peer_height {
        let chunk = 10.min(peer_height - dl_cursor);
        network.send(peer_id, Message::GetBatches { start_height: dl_cursor, count: chunk });
        let batches = loop {
            match network.next_event().await {
                NetworkEvent::MessageReceived {
                    message: Message::Batches { batches, .. }, ..
                } => break batches,
                _ => continue,
            }
        };
        if batches.is_empty() { anyhow::bail!("Peer sent empty batches at {}", dl_cursor); }
        for batch in &batches {
            recent_headers.push(state.timestamp);
            if recent_headers.len() > MEDIAN_TIME_PAST_WINDOW { recent_headers.remove(0); }
            apply_batch(&mut state, batch, &recent_headers)?;
            storage.save_batch(dl_cursor, batch)?;
            dl_cursor += 1;
        }
    }
    storage.save_state(&state)?;

    println!("Sync complete!");
    println!("  Height:      {}", state.height);
    println!("  Depth:       {}", state.depth);
    println!("  Coins:       {}", state.coins.len());
    println!("  Commitments: {}", state.commitments.len());
    println!("  Midstate:    {}", hex::encode(state.midstate));
    Ok(())
}




══════════════════════════════════════════════════════════════
 FILE: mempool.rs
══════════════════════════════════════════════════════════════

use crate::core::{State, Transaction};
use crate::core::transaction::validate_transaction;
use anyhow::Result;
use std::collections::HashSet;

const MAX_MEMPOOL_SIZE: usize = 10_000;
const MAX_PENDING_COMMITS: usize = 1_000;
const MIN_REVEAL_FEE: u64 = 1;

pub struct Mempool {
    transactions: Vec<Transaction>,
    seen_inputs: HashSet<[u8; 32]>,
    seen_commitments: HashSet<[u8; 32]>,
}

impl Mempool {
    pub fn new() -> Self {
        Self {
            transactions: Vec::new(),
            seen_inputs: HashSet::new(),
            seen_commitments: HashSet::new(),
        }
    }

    pub fn add(&mut self, tx: Transaction, state: &State) -> Result<()> {
        // 1. Initial Validation
        match &tx {
            Transaction::Commit { .. } => {
                if self.seen_commitments.len() >= MAX_PENDING_COMMITS {
                    anyhow::bail!("Too many pending commits");
                }
            }
            Transaction::Reveal { .. } => {
                if tx.fee() < MIN_REVEAL_FEE {
                    anyhow::bail!("Fee too low (minimum: {})", MIN_REVEAL_FEE);
                }
            }
        }
        
        validate_transaction(state, &tx)?;

        // 2. Duplicate Checks
        match &tx {
            Transaction::Commit { commitment, .. } => {
                if self.seen_commitments.contains(commitment) {
                    anyhow::bail!("Commitment already in mempool");
                }
            }
            Transaction::Reveal { .. } => {
                for input in tx.input_coin_ids() {
                    if self.seen_inputs.contains(&input) {
                        anyhow::bail!("Transaction input already in mempool");
                    }
                }
            }
        }

        // 3. Fee-Based Eviction if Full
        if self.transactions.len() >= MAX_MEMPOOL_SIZE {
            if let Transaction::Reveal { .. } = &tx {
                let tx_fee = tx.fee();
                
                // Find the Reveal transaction with the lowest fee
                let mut lowest_fee = u64::MAX;
                let mut lowest_fee_idx = None;

                for (i, existing_tx) in self.transactions.iter().enumerate() {
                    if let Transaction::Reveal { .. } = existing_tx {
                        let f = existing_tx.fee();
                        if f < lowest_fee {
                            lowest_fee = f;
                            lowest_fee_idx = Some(i);
                        }
                    }
                }

                if let Some(idx) = lowest_fee_idx {
                    if tx_fee > lowest_fee {
                        let evicted = self.transactions.remove(idx);
                        tracing::info!("Evicted tx with fee {} to make room for tx with fee {}", lowest_fee, tx_fee);
                        
                        // Clean up the indexes for the evicted transaction
                        for input in evicted.input_coin_ids() {
                            self.seen_inputs.remove(&input);
                        }
                    } else {
                        anyhow::bail!("Mempool full: fee {} too low to replace existing tx (lowest: {})", tx_fee, lowest_fee);
                    }
                } else {
                    anyhow::bail!("Mempool full of commits, cannot accept new Reveal");
                }
            } else {
                anyhow::bail!("Mempool full: cannot accept more commits");
            }
        }

        // 4. Insert the new transaction
        match &tx {
            Transaction::Commit { commitment, .. } => {
                self.seen_commitments.insert(*commitment);
            }
            Transaction::Reveal { .. } => {
                for input in tx.input_coin_ids() {
                    self.seen_inputs.insert(input);
                }
            }
        }
        self.transactions.push(tx);

        tracing::debug!("Added transaction to mempool (size: {})", self.transactions.len());

        Ok(())
    }

    pub fn re_add(&mut self, txs: Vec<Transaction>, state: &State) {
        let mut restored = 0usize;
        for tx in txs {
            if validate_transaction(state, &tx).is_err() {
                continue;
            }

            let dominated = match &tx {
                Transaction::Commit { commitment, .. } => self.seen_commitments.contains(commitment),
                Transaction::Reveal { .. } => {
                    tx.input_coin_ids().iter().any(|i| self.seen_inputs.contains(i))
                }
            };
            if dominated {
                continue;
            }

            match &tx {
                Transaction::Commit { commitment, .. } => {
                    self.seen_commitments.insert(*commitment);
                }
                Transaction::Reveal { .. } => {
                    for input in tx.input_coin_ids() {
                        self.seen_inputs.insert(input);
                    }
                }
            }
            self.transactions.push(tx);
            restored += 1;
        }

        if restored > 0 {
            tracing::info!("Restored {} transactions to mempool", restored);
        }
    }

    pub fn drain(&mut self, max: usize) -> Vec<Transaction> {
        let count = max.min(self.transactions.len());
        let drained: Vec<_> = self.transactions.drain(..count).collect();

        for tx in &drained {
            match tx {
                Transaction::Commit { commitment, .. } => {
                    self.seen_commitments.remove(commitment);
                }
                Transaction::Reveal { .. } => {
                    for input in tx.input_coin_ids() {
                        self.seen_inputs.remove(&input);
                    }
                }
            }
        }

        drained
    }

    pub fn len(&self) -> usize {
        self.transactions.len()
    }

    pub fn transactions(&self) -> &[Transaction] {
        &self.transactions
    }

    pub fn prune_invalid(&mut self, state: &State) {
        let mut inputs_to_remove: HashSet<[u8; 32]> = HashSet::new();
        let mut commitments_to_remove: HashSet<[u8; 32]> = HashSet::new();

        for tx in &self.transactions {
            if validate_transaction(state, tx).is_err() {
                match tx {
                    Transaction::Commit { commitment, .. } =>{
                        commitments_to_remove.insert(*commitment);
                    }
                    Transaction::Reveal { .. } => {
                        inputs_to_remove.extend(tx.input_coin_ids());
                    }
                }
            }
        }

        if !inputs_to_remove.is_empty() || !commitments_to_remove.is_empty() {
            tracing::info!(
                "Pruning invalid transactions from mempool (inputs: {}, commitments: {})",
                inputs_to_remove.len(),
                commitments_to_remove.len()
            );

            self.transactions.retain(|tx| {
                let should_remove = match tx {
                    Transaction::Commit { commitment, .. } => {
                        commitments_to_remove.contains(commitment)
                    }
                    Transaction::Reveal { .. } => {
                        let inputs = tx.input_coin_ids();
                        inputs.iter().any(|input| inputs_to_remove.contains(input))
                    }
                };

                if should_remove {
                    match tx {
                        Transaction::Commit { commitment, .. } => {
                            self.seen_commitments.remove(commitment);
                        }
                        Transaction::Reveal { .. } => {
                            for input in tx.input_coin_ids() {
                                self.seen_inputs.remove(&input);
                            }
                        }
                    }
                    false
                } else {
                    true
                }
            });
        }
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::types::*;
    use crate::core::mmr::UtxoAccumulator;

    fn empty_state() -> State {
        State {
            midstate: [0u8; 32],
            coins: UtxoAccumulator::new(),
            commitments: UtxoAccumulator::new(),
            depth: 0,
            target: [0xff; 32],
            height: 1,
            timestamp: 1000,
            commitment_heights: std::collections::HashMap::new(),
        }
    }

    #[test]
    fn mempool_rejects_bad_commit_pow() {
        let mut mp = Mempool::new();
        let state = empty_state();
        let commitment = hash(b"mempool test");
        // Find a bad nonce
        let mut bad = 0u64;
        loop {
            let h = hash_concat(&commitment, &bad.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) != 0x0000 { break; }
            bad += 1;
        }
        let tx = Transaction::Commit { commitment, spam_nonce: bad };
        assert!(mp.add(tx, &state).is_err());
        assert_eq!(mp.len(), 0);
    }

    #[test]
    fn mempool_accepts_good_commit_pow() {
        let mut mp = Mempool::new();
        let state = empty_state();
        let commitment = hash(b"mempool test good");
        let mut n = 0u64;
        loop {
            let h = hash_concat(&commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        let tx = Transaction::Commit { commitment, spam_nonce: n };
        assert!(mp.add(tx, &state).is_ok());
        assert_eq!(mp.len(), 1);
    }
    
    #[test]
    fn mempool_full_rejects() {
        let state = empty_state();
        let mut mp = Mempool::new();
        
        // Fill to capacity with dummy Reveals to avoid hitting MAX_PENDING_COMMITS
        for i in 0..MAX_MEMPOOL_SIZE {
            let dummy_input = InputReveal { 
                owner_pk: [0; 32], 
                value: 10, 
                salt: hash(&(i as u64).to_le_bytes()) 
            };
            let dummy_reveal = Transaction::Reveal {
                inputs: vec![dummy_input.clone()],
                signatures: vec![],
                outputs: vec![OutputData { address: [0; 32], value: 8, salt: [0; 32] }],
                salt: [0; 32],
            };
            mp.transactions.push(dummy_reveal);
            mp.seen_inputs.insert(dummy_input.coin_id());
        }
        assert_eq!(mp.len(), MAX_MEMPOOL_SIZE);

        let extra = hash(b"one more");
        let mut n = 0u64;
        loop {
            let h = hash_concat(&extra, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        
        // Attempting to add a Commit to a full mempool should be rejected
        let tx = Transaction::Commit { commitment: extra, spam_nonce: n };
        let err = mp.add(tx, &state).unwrap_err();
        assert!(err.to_string().contains("Mempool full"));
    }

    #[test]
    fn max_pending_commits_enforced() {
        let state = empty_state();
        let mut mp = Mempool::new();
        for i in 0..MAX_PENDING_COMMITS {
            let commitment = hash(&(i as u64).to_le_bytes());
            mp.transactions.push(Transaction::Commit { commitment, spam_nonce: 0 });
            mp.seen_commitments.insert(commitment);
        }

        let extra = hash(b"commit overflow");
        let mut n = 0u64;
        loop {
            let h = hash_concat(&extra, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        let tx = Transaction::Commit { commitment: extra, spam_nonce: n };
        let err = mp.add(tx, &state).unwrap_err();
        assert!(err.to_string().contains("Too many pending commits"));
    }

    #[test]
    fn duplicate_commitment_rejected() {
        let mut mp = Mempool::new();
        let state = empty_state();
        let commitment = hash(b"dup test");
        let mut n = 0u64;
        loop {
            let h = hash_concat(&commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        let tx = Transaction::Commit { commitment, spam_nonce: n };
        assert!(mp.add(tx.clone(), &state).is_ok());
        assert!(mp.add(tx, &state).is_err());
    }

    // ── Reveal path ─────────────────────────────────────────────────────

    fn state_with_committed_coin() -> (State, [u8; 32], [u8; 32], [u8; 32], [u8; 32], OutputData) {
        // Returns (state, seed, coin_id, input_salt, commit_salt, output)
        use crate::core::{wots, types::*};

        let mut state = empty_state();
        let seed: [u8; 32] = [0x42; 32];
        let owner_pk = wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        let input_salt = hash(b"test salt");
        let coin_id = compute_coin_id(&address, 16, &input_salt);
        state.coins.insert(coin_id);

        let output = OutputData { address: hash(b"recipient"), value: 8, salt: [0x11; 32] };
        let commit_salt: [u8; 32] = [0x22; 32];
        let commitment = compute_commitment(&[coin_id], &[output.coin_id()], &commit_salt);

        // Mine PoW and add commitment
        let mut n = 0u64;
        loop {
            let h = hash_concat(&commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        let commit_tx = Transaction::Commit { commitment, spam_nonce: n };
        crate::core::transaction::apply_transaction(&mut state, &commit_tx).unwrap();

        (state, seed, coin_id, input_salt, commit_salt, output)
    }

    fn make_reveal_tx(seed: &[u8; 32], value: u64, input_salt: [u8; 32], commit_salt: [u8; 32], output: OutputData) -> Transaction {
        use crate::core::wots;

        let owner_pk = wots::keygen(seed);
        let address = compute_address(&owner_pk);
        let coin_id = compute_coin_id(&address, value, &input_salt);

        let commitment = compute_commitment(&[coin_id], &[output.coin_id()], &commit_salt);
        let sig = wots::sign(seed, &commitment);

        Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value, salt: input_salt }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        }
    }

    #[test]
    fn mempool_accepts_valid_reveal() {
        let (state, seed, _coin_id, input_salt, commit_salt, output) = state_with_committed_coin();
        let mut mp = Mempool::new();
        let tx = make_reveal_tx(&seed, 16, input_salt, commit_salt, output);
        assert!(mp.add(tx, &state).is_ok());
        assert_eq!(mp.len(), 1);
    }

    #[test]
    fn mempool_rejects_duplicate_input() {
        let (state, seed, _coin_id, input_salt, commit_salt, output) = state_with_committed_coin();
        let mut mp = Mempool::new();
        let tx = make_reveal_tx(&seed, 16, input_salt, commit_salt, output);
        mp.add(tx.clone(), &state).unwrap();
        assert!(mp.add(tx, &state).is_err());
    }

    #[test]
    fn mempool_rejects_low_fee_reveal() {
        let mut state = empty_state();
        let seed: [u8; 32] = [0x42; 32];
        let owner_pk = crate::core::wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        let input_salt = [0u8; 32];
        let coin_id = compute_coin_id(&address, 1, &input_salt);
        state.coins.insert(coin_id);

        let output = OutputData { address: hash(b"r"), value: 1, salt: [0; 32] };
        let commit_salt = [1u8; 32];
        let commitment = compute_commitment(&[coin_id], &[output.coin_id()], &commit_salt);

        let mut n = 0u64;
        loop {
            let h = hash_concat(&commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            n += 1;
        }
        crate::core::transaction::apply_transaction(&mut state, &Transaction::Commit { commitment, spam_nonce: n }).unwrap();

        let sig = crate::core::wots::sign(&seed, &commitment);
        // in=1, out=1 → fee=0 < MIN_REVEAL_FEE
        let tx = Transaction::Reveal {
            inputs: vec![InputReveal { owner_pk, value: 1, salt: input_salt }],
            signatures: vec![crate::core::wots::sig_to_bytes(&sig)],
            outputs: vec![output],
            salt: commit_salt,
        };

        let mut mp = Mempool::new();
        let err = mp.add(tx, &state).unwrap_err();
        assert!(err.to_string().contains("Fee too low"));
    }

    // ── drain ───────────────────────────────────────────────────────────

    #[test]
    fn drain_returns_and_removes() {
        let _state = empty_state();
        let mut mp = Mempool::new();
        for i in 0..5 {
            let commitment = hash(&(i as u64).to_le_bytes());
            mp.transactions.push(Transaction::Commit { commitment, spam_nonce: 0 });
            mp.seen_commitments.insert(commitment);
        }
        assert_eq!(mp.len(), 5);

        let drained = mp.drain(3);
        assert_eq!(drained.len(), 3);
        assert_eq!(mp.len(), 2);
        assert_eq!(mp.seen_commitments.len(), 2);
    }

    #[test]
    fn drain_more_than_available() {
        let mut mp = Mempool::new();
        mp.transactions.push(Transaction::Commit { commitment: [1; 32], spam_nonce: 0 });
        mp.seen_commitments.insert([1; 32]);
        let drained = mp.drain(100);
        assert_eq!(drained.len(), 1);
        assert_eq!(mp.len(), 0);
    }

    // ── prune_invalid ───────────────────────────────────────────────────

    #[test]
    fn prune_removes_invalid_commits() {
        let state = empty_state();
        let mut mp = Mempool::new();
        let commitment = hash(b"prune test");
        // Directly push a commit with a bad PoW nonce
        mp.transactions.push(Transaction::Commit { commitment, spam_nonce: u64::MAX });
        mp.seen_commitments.insert(commitment);
        assert_eq!(mp.len(), 1);
        mp.prune_invalid(&state);
        assert_eq!(mp.len(), 0);
    }

    // ── re_add ──────────────────────────────────────────────────────────

    #[test]
    fn re_add_skips_invalid() {
        let state = empty_state();
        let mut mp = Mempool::new();
        // Bad PoW commit won't re-add
        let bad_tx = Transaction::Commit { commitment: hash(b"bad"), spam_nonce: u64::MAX };
        mp.re_add(vec![bad_tx], &state);
        assert_eq!(mp.len(), 0);
    }

    #[test]
    fn re_add_skips_duplicates() {
        let state = empty_state();
        let mut mp = Mempool::new();
        let commitment = hash(b"dup re-add");
        mp.transactions.push(Transaction::Commit { commitment, spam_nonce: 0 });
        mp.seen_commitments.insert(commitment);

        // Try to re_add the same commitment
        mp.re_add(vec![Transaction::Commit { commitment, spam_nonce: 0 }], &state);
        assert_eq!(mp.len(), 1); // still just 1
    }
    
    #[test]
    fn mempool_rbf_evicts_lowest_fee_reveal() {
        let (state, seed, _coin_id, input_salt, commit_salt, output) = state_with_committed_coin();
        let mut mp = Mempool::new();

        // 1. Fill mempool to capacity with dummy Commits and exactly one Reveal with a low fee (fee = 2)
        for i in 0..(MAX_MEMPOOL_SIZE - 1) {
            let commitment = hash(&(i as u64).to_le_bytes());
            mp.transactions.push(Transaction::Commit { commitment, spam_nonce: 0 });
            mp.seen_commitments.insert(commitment);
        }

        let dummy_input = InputReveal { owner_pk: [0; 32], value: 10, salt: [0; 32] };
        let dummy_output = OutputData { address: [0; 32], value: 8, salt: [0; 32] }; // fee = 2
        let dummy_reveal = Transaction::Reveal {
            inputs: vec![dummy_input.clone()],
            signatures: vec![],
            outputs: vec![dummy_output],
            salt: [0; 32],
        };
        mp.transactions.push(dummy_reveal);
        mp.seen_inputs.insert(dummy_input.coin_id());

        assert_eq!(mp.len(), MAX_MEMPOOL_SIZE);

        // 2. We have a valid incoming transaction with a higher fee (Input 16, Output 8 -> Fee = 8)
        let tx = make_reveal_tx(&seed, 16, input_salt, commit_salt, output);
        assert_eq!(tx.fee(), 8);

        // 3. Attempting to add should succeed and evict the fee=2 dummy reveal.
        assert!(mp.add(tx.clone(), &state).is_ok());
        assert_eq!(mp.len(), MAX_MEMPOOL_SIZE);
        
        // 4. Verify the dummy input was removed from the index, and the new one was added
        assert!(!mp.seen_inputs.contains(&dummy_input.coin_id()), "Evicted input should be removed from seen index");
        assert!(mp.seen_inputs.contains(&tx.input_coin_ids()[0]), "New input should be added to seen index");
    }

    #[test]
    fn mempool_rbf_rejects_lower_fee_reveal() {
        let (state, seed, _coin_id, input_salt, commit_salt, output) = state_with_committed_coin();
        let mut mp = Mempool::new();

        // 1. Fill mempool to capacity with dummy Commits and exactly one Reveal with a HIGH fee (fee = 10)
        for i in 0..(MAX_MEMPOOL_SIZE - 1) {
            let commitment = hash(&(i as u64).to_le_bytes());
            mp.transactions.push(Transaction::Commit { commitment, spam_nonce: 0 });
            mp.seen_commitments.insert(commitment);
        }

        let dummy_input = InputReveal { owner_pk: [0; 32], value: 20, salt: [0; 32] };
        let dummy_output = OutputData { address: [0; 32], value: 10, salt: [0; 32] }; // fee = 10
        let dummy_reveal = Transaction::Reveal {
            inputs: vec![dummy_input.clone()],
            signatures: vec![],
            outputs: vec![dummy_output],
            salt: [0; 32],
        };
        mp.transactions.push(dummy_reveal);
        mp.seen_inputs.insert(dummy_input.coin_id());

        assert_eq!(mp.len(), MAX_MEMPOOL_SIZE);

        // 2. Incoming transaction has a lower fee (Input 16, Output 8 -> Fee = 8)
        let tx = make_reveal_tx(&seed, 16, input_salt, commit_salt, output);
        assert_eq!(tx.fee(), 8);

        // 3. Attempting to add should fail because 8 is not > 10.
        let err = mp.add(tx, &state).unwrap_err();
        assert!(err.to_string().contains("too low to replace existing tx"));
    }
    
}




══════════════════════════════════════════════════════════════
 FILE: metrics.rs
══════════════════════════════════════════════════════════════

use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;

#[derive(Clone)]
pub struct Metrics {
    transactions_processed: Arc<AtomicU64>,
    batches_processed: Arc<AtomicU64>,
    batches_mined: Arc<AtomicU64>,
    invalid_batches: Arc<AtomicU64>,
    invalid_transactions: Arc<AtomicU64>,
    reorgs: Arc<AtomicU64>,
}

impl Metrics {
    pub fn new() -> Self {
        Self {
            transactions_processed: Arc::new(AtomicU64::new(0)),
            batches_processed: Arc::new(AtomicU64::new(0)),
            batches_mined: Arc::new(AtomicU64::new(0)),
            invalid_batches: Arc::new(AtomicU64::new(0)),
            invalid_transactions: Arc::new(AtomicU64::new(0)),
            reorgs: Arc::new(AtomicU64::new(0)),
        }
    }
    
    pub fn inc_transactions_processed(&self) {
        self.transactions_processed.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn inc_batches_processed(&self) {
        self.batches_processed.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn inc_batches_mined(&self) {
        self.batches_mined.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn inc_invalid_batches(&self) {
        self.invalid_batches.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn inc_invalid_transactions(&self) {
        self.invalid_transactions.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn inc_reorgs(&self) {
        self.reorgs.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn report(&self) {
        tracing::info!(
            "Metrics: txs={} batches={} mined={} invalid_batches={} invalid_txs={} reorgs={}",
            self.transactions_processed.load(Ordering::Relaxed),
            self.batches_processed.load(Ordering::Relaxed),
            self.batches_mined.load(Ordering::Relaxed),
            self.invalid_batches.load(Ordering::Relaxed),
            self.invalid_transactions.load(Ordering::Relaxed),
            self.reorgs.load(Ordering::Relaxed),
        );
    }
}




══════════════════════════════════════════════════════════════
 FILE: mix.rs
══════════════════════════════════════════════════════════════

//! Node-side CoinJoin mix coordination.
//!
//! Wraps [`wallet::coinjoin::MixSession`] with peer tracking, signature
//! collection, and phase management. The [`MixManager`] is shared between
//! the node event loop (which drives p2p messages) and the RPC layer
//! (which drives local wallet interactions).

use crate::core::types::*;
use crate::wallet::coinjoin::{MixSession, MixProposal};
use anyhow::{bail, Result};
use libp2p::PeerId;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Session timeout in seconds. Stale sessions are garbage collected.
const MIX_SESSION_TIMEOUT: u64 = 300;

/// Maximum concurrent mix sessions per node.
const MAX_MIX_SESSIONS: usize = 16;

#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub enum MixPhase {
    /// Collecting registrations from participants.
    Collecting,
    /// Proposal built, awaiting signatures.
    Signing,
    /// All signatures collected, Commit transaction submitted.
    CommitSubmitted,
    /// Reveal transaction submitted, mix complete.
    Complete,
    /// Mix failed or timed out.
    Failed(String),
}

/// A participant in the mix — either local (via RPC) or remote (via p2p).
#[derive(Clone, Debug)]
struct Participant {
    /// None if this participant registered via local RPC.
    peer: Option<PeerId>,
}

/// Node-side state for a single CoinJoin session.
pub struct NodeMixSession {
    session: MixSession,
    participants: Vec<Participant>,
    fee_participant: Option<Participant>,
    proposal: Option<MixProposal>,
    signatures: HashMap<usize, Vec<u8>>,
    pub phase: MixPhase,
    created_at: u64,
    /// True if this node initiated the session (is the coordinator).
    pub is_coordinator: bool,
    /// The coordinator peer (if we're a joiner).
    pub coordinator_peer: Option<PeerId>,
}

/// Snapshot of a mix session exposed to the RPC layer.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MixStatusSnapshot {
    pub mix_id: String,
    pub denomination: u64,
    pub participants: usize,
    pub phase: MixPhase,
    /// Set when phase == Signing; the commitment the wallet needs to sign.
    pub commitment: Option<String>,
    /// Set when phase == Signing; serialized proposal inputs for the wallet
    /// to find its own input index.
    pub input_coin_ids: Vec<String>,
}

/// Manages all active mix sessions for a node.
pub struct MixManager {
    sessions: HashMap<[u8; 32], NodeMixSession>,
}

impl MixManager {
    pub fn new() -> Self {
        Self {
            sessions: HashMap::new(),
        }
    }

    /// Fails any active mix sessions reliant on a disconnected peer
    pub fn handle_peer_disconnect(&mut self, peer: PeerId) {
        for (_, ns) in self.sessions.iter_mut() {
            if matches!(ns.phase, MixPhase::Complete | MixPhase::Failed(_)) {
                continue;
            }
            let is_involved = ns.coordinator_peer == Some(peer)
                || ns.participants.iter().any(|p| p.peer == Some(peer))
                || ns.fee_participant.as_ref().map_or(false, |p| p.peer == Some(peer));

            if is_involved {
                ns.phase = MixPhase::Failed(format!("peer {} disconnected", peer));
            }
        }
    }

    /// Create a new mix session as coordinator. Returns the mix_id.
    pub fn create_session(&mut self, denomination: u64, min_participants: usize) -> Result<[u8; 32]> {
        if self.sessions.len() >= MAX_MIX_SESSIONS {
            bail!("too many active mix sessions");
        }
        let session = MixSession::new(denomination, min_participants)?;
        let mix_id: [u8; 32] = rand::random();
        self.sessions.insert(mix_id, NodeMixSession {
            session,
            participants: Vec::new(),
            fee_participant: None,
            proposal: None,
            signatures: HashMap::new(),
            phase: MixPhase::Collecting,
            created_at: now(),
            is_coordinator: true,
            coordinator_peer: None,
        });
        Ok(mix_id)
    }

    /// Create a session as a joiner (responding to a peer's MixAnnounce).
    pub fn create_joining_session(
        &mut self,
        mix_id: [u8; 32],
        denomination: u64,
        coordinator: PeerId,
    ) -> Result<()> {
        if self.sessions.contains_key(&mix_id) {
            bail!("session already exists");
        }
        if self.sessions.len() >= MAX_MIX_SESSIONS {
            bail!("too many active mix sessions");
        }
        let session = MixSession::new(denomination, 2)?;
        self.sessions.insert(mix_id, NodeMixSession {
            session,
            participants: Vec::new(),
            fee_participant: None,
            proposal: None,
            signatures: HashMap::new(),
            phase: MixPhase::Collecting,
            created_at: now(),
            is_coordinator: false,
            coordinator_peer: Some(coordinator),
        });
        Ok(())
    }

    /// Register a participant (local or remote).
    pub fn register(
        &mut self,
        mix_id: &[u8; 32],
        input: InputReveal,
        output: OutputData,
        peer: Option<PeerId>,
    ) -> Result<()> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        if ns.phase != MixPhase::Collecting {
            bail!("session not accepting registrations (phase: {:?})", ns.phase);
        }

        ns.session.register(input.clone(), output.clone())?;
        ns.participants.push(Participant { peer });
        Ok(())
    }

    /// Set the fee input for a session.
    pub fn set_fee_input(
        &mut self,
        mix_id: &[u8; 32],
        input: InputReveal,
        peer: Option<PeerId>,
    ) -> Result<()> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        if ns.phase != MixPhase::Collecting {
            bail!("session not accepting registrations");
        }

        ns.session.set_fee_input(input.clone())?;
        ns.fee_participant = Some(Participant { peer });
        Ok(())
    }

    /// Try to advance a session to the Signing phase.
    /// Returns the proposal if the session just became ready.
    pub fn try_finalize(&mut self, mix_id: &[u8; 32]) -> Result<Option<MixProposal>> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        if ns.phase != MixPhase::Collecting || !ns.session.is_ready() {
            return Ok(None);
        }

        let proposal = ns.session.proposal()?;
        ns.proposal = Some(proposal.clone());
        ns.phase = MixPhase::Signing;
        Ok(Some(proposal))
    }

    /// Record a signature for an input in the proposal.
    pub fn add_signature(
        &mut self,
        mix_id: &[u8; 32],
        input_index: usize,
        signature: Vec<u8>,
    ) -> Result<()> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        if ns.phase != MixPhase::Signing {
            bail!("session not in signing phase");
        }

        let proposal = ns.proposal.as_ref()
            .ok_or_else(|| anyhow::anyhow!("no proposal"))?;

        if input_index >= proposal.inputs.len() {
            bail!("input_index {} out of range ({})", input_index, proposal.inputs.len());
        }

        ns.signatures.insert(input_index, signature);
        Ok(())
    }

    /// Check if all signatures are collected and build the final transaction.
    pub fn try_build_transaction(&mut self, mix_id: &[u8; 32]) -> Result<Option<Transaction>> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        if ns.phase != MixPhase::Signing {
            return Ok(None);
        }

        let proposal = ns.proposal.as_ref()
            .ok_or_else(|| anyhow::anyhow!("no proposal"))?;

        if ns.signatures.len() != proposal.inputs.len() {
            return Ok(None); // still waiting
        }

        // Collect signatures in canonical order
        let sigs: Vec<Vec<u8>> = (0..proposal.inputs.len())
            .map(|i| ns.signatures.get(&i).cloned()
                .ok_or_else(|| anyhow::anyhow!("missing signature for input {}", i)))
            .collect::<Result<_>>()?;

        let tx = ns.session.build_reveal(sigs)?;
        Ok(Some(tx))
    }

    /// Mark a session phase.
    pub fn set_phase(&mut self, mix_id: &[u8; 32], phase: MixPhase) {
        if let Some(ns) = self.sessions.get_mut(mix_id) {
            ns.phase = phase;
        }
    }

    /// Apply a received proposal from a coordinator peer.
    pub fn apply_remote_proposal(
        &mut self,
        mix_id: &[u8; 32],
        inputs: Vec<InputReveal>,
        outputs: Vec<OutputData>,
        salt: [u8; 32],
        commitment: [u8; 32],
    ) -> Result<()> {
        let ns = self.sessions.get_mut(mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        // Verify the commitment matches
        let input_ids: Vec<[u8; 32]> = inputs.iter().map(|i| i.coin_id()).collect();
        let output_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();
        let expected = compute_commitment(&input_ids, &output_ids, &salt);
        if expected != commitment {
            bail!("proposal commitment mismatch");
        }

        ns.proposal = Some(MixProposal {
            inputs: inputs.clone(),
            outputs,
            salt,
            commitment,
        });
        ns.phase = MixPhase::Signing;
        Ok(())
    }

    /// Get status snapshot for RPC.
    pub fn status(&self, mix_id: &[u8; 32]) -> Option<MixStatusSnapshot> {
        let ns = self.sessions.get(mix_id)?;
        Some(MixStatusSnapshot {
            mix_id: hex::encode(mix_id),
            denomination: ns.session.denomination(),
            participants: ns.session.participant_count(),
            phase: ns.phase.clone(),
            commitment: ns.proposal.as_ref().map(|p| hex::encode(p.commitment)),
            input_coin_ids: ns.proposal.as_ref()
                .map(|p| p.inputs.iter().map(|i| hex::encode(i.coin_id())).collect())
                .unwrap_or_default(),
        })
    }

    /// List all active mix sessions.
    pub fn list_sessions(&self) -> Vec<MixStatusSnapshot> {
        self.sessions.iter()
            .map(|(id, _)| self.status(id).unwrap())
            .collect()
    }

    /// Get remote peers to notify for a session.
    pub fn remote_participants(&self, mix_id: &[u8; 32]) -> Vec<PeerId> {
        let Some(ns) = self.sessions.get(mix_id) else { return vec![]; };
        let mut peers: Vec<PeerId> = ns.participants.iter()
            .filter_map(|p| p.peer)
            .collect();
        if let Some(fp) = &ns.fee_participant {
            if let Some(peer) = fp.peer {
                if !peers.contains(&peer) {
                    peers.push(peer);
                }
            }
        }
        peers
    }

    /// Find the input index in the proposal for a given coin_id.
    pub fn find_input_index(&self, mix_id: &[u8; 32], coin_id: &[u8; 32]) -> Option<usize> {
        let ns = self.sessions.get(mix_id)?;
        let proposal = ns.proposal.as_ref()?;
        proposal.inputs.iter().position(|i| i.coin_id() == *coin_id)
    }

    /// Get session existence and coordinator status.
    pub fn get_session_info(&self, mix_id: &[u8; 32]) -> Option<(bool, Option<PeerId>)> {
        self.sessions.get(mix_id).map(|ns| (ns.is_coordinator, ns.coordinator_peer))
    }

    /// Remove timed-out and completed sessions.
    pub fn cleanup(&mut self) {
        let now = now();
        self.sessions.retain(|id, ns| {
            match &ns.phase {
                MixPhase::Complete | MixPhase::Failed(_) => {
                    // Keep for 30s after completion so wallet can poll final status
                    now.saturating_sub(ns.created_at) < MIX_SESSION_TIMEOUT + 30
                }
                _ => {
                    if now.saturating_sub(ns.created_at) > MIX_SESSION_TIMEOUT {
                        tracing::debug!("Mix session {} timed out", hex::encode(id));
                        false
                    } else {
                        true
                    }
                }
            }
        });
    }

    pub fn session_count(&self) -> usize {
        self.sessions.len()
    }
}

fn now() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::wots;

    fn make_input(name: &[u8], value: u64) -> InputReveal {
        let seed = hash(name);
        let pk = wots::keygen(&seed);
        InputReveal { owner_pk: pk, value, salt: hash_concat(name, b"salt") }
    }

    fn make_output(name: &[u8], value: u64) -> OutputData {
        OutputData { address: hash_concat(name, b"dest"), value, salt: hash_concat(name, b"osalt") }
    }

    #[test]
    fn create_and_register() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();

        mgr.register(&mix_id, make_input(b"alice", 8), make_output(b"alice", 8), None).unwrap();
        mgr.register(&mix_id, make_input(b"bob", 8), make_output(b"bob", 8), None).unwrap();

        let status = mgr.status(&mix_id).unwrap();
        assert_eq!(status.participants, 2);
        assert_eq!(status.phase, MixPhase::Collecting);
    }

    #[test]
    fn rejects_wrong_denomination() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();
        assert!(mgr.register(&mix_id, make_input(b"bad", 4), make_output(b"bad", 8), None).is_err());
    }

    #[test]
    fn finalize_produces_proposal() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();

        mgr.register(&mix_id, make_input(b"alice", 8), make_output(b"alice", 8), None).unwrap();
        mgr.register(&mix_id, make_input(b"bob", 8), make_output(b"bob", 8), None).unwrap();
        mgr.set_fee_input(&mix_id, make_input(b"fee", 1), None).unwrap();

        let proposal = mgr.try_finalize(&mix_id).unwrap();
        assert!(proposal.is_some());

        let status = mgr.status(&mix_id).unwrap();
        assert_eq!(status.phase, MixPhase::Signing);
        assert!(status.commitment.is_some());
    }

    #[test]
    fn finalize_returns_none_when_not_ready() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();
        mgr.register(&mix_id, make_input(b"alice", 8), make_output(b"alice", 8), None).unwrap();
        // Only 1 of 2 participants
        assert!(mgr.try_finalize(&mix_id).unwrap().is_none());
    }

    #[test]
    fn full_signing_flow() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();

        let seed_a = hash(b"alice");
        let seed_b = hash(b"bob");
        let seed_f = hash(b"fee");

        mgr.register(&mix_id, make_input(b"alice", 8), make_output(b"alice", 8), None).unwrap();
        mgr.register(&mix_id, make_input(b"bob", 8), make_output(b"bob", 8), None).unwrap();
        mgr.set_fee_input(&mix_id, make_input(b"fee", 1), None).unwrap();

        let proposal = mgr.try_finalize(&mix_id).unwrap().unwrap();

        // Sign each input
        for (i, input) in proposal.inputs.iter().enumerate() {
            let pk = input.owner_pk;
            let seed = if pk == wots::keygen(&seed_a) { seed_a }
                else if pk == wots::keygen(&seed_b) { seed_b }
                else { seed_f };
            let sig = wots::sig_to_bytes(&wots::sign(&seed, &proposal.commitment));
            mgr.add_signature(&mix_id, i, sig).unwrap();
        }

        let tx = mgr.try_build_transaction(&mix_id).unwrap();
        assert!(tx.is_some());
        match tx.unwrap() {
            Transaction::Reveal { inputs, signatures, outputs, .. } => {
                assert_eq!(inputs.len(), 3);
                assert_eq!(signatures.len(), 3);
                assert_eq!(outputs.len(), 2);
            }
            _ => panic!("expected Reveal"),
        }
    }

    #[test]
    fn try_build_returns_none_until_all_sigs() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();

        mgr.register(&mix_id, make_input(b"alice", 8), make_output(b"alice", 8), None).unwrap();
        mgr.register(&mix_id, make_input(b"bob", 8), make_output(b"bob", 8), None).unwrap();
        mgr.set_fee_input(&mix_id, make_input(b"fee", 1), None).unwrap();

        mgr.try_finalize(&mix_id).unwrap();
        mgr.add_signature(&mix_id, 0, vec![0u8; 576]).unwrap();

        // Still missing sigs for inputs 1 and 2
        assert!(mgr.try_build_transaction(&mix_id).unwrap().is_none());
    }

    #[test]
    fn max_sessions_enforced() {
        let mut mgr = MixManager::new();
        for _ in 0..MAX_MIX_SESSIONS {
            mgr.create_session(8, 2).unwrap();
        }
        assert!(mgr.create_session(8, 2).is_err());
    }

    #[test]
    fn list_sessions() {
        let mut mgr = MixManager::new();
        mgr.create_session(8, 2).unwrap();
        mgr.create_session(16, 2).unwrap();
        let list = mgr.list_sessions();
        assert_eq!(list.len(), 2);
    }

    #[test]
    fn cleanup_removes_completed() {
        let mut mgr = MixManager::new();
        let mix_id = mgr.create_session(8, 2).unwrap();
        mgr.set_phase(&mix_id, MixPhase::Complete);
        // Won't be cleaned immediately (30s grace)
        mgr.cleanup();
        assert_eq!(mgr.session_count(), 1);
    }
}




══════════════════════════════════════════════════════════════
 FILE: network/mod.rs
══════════════════════════════════════════════════════════════

pub mod protocol;

pub use protocol::{Message, MidstateCodec, MIDSTATE_PROTOCOL, MAX_GETBATCHES_COUNT};

use anyhow::Result;
use futures::StreamExt;
use libp2p::{
    autonat,
    dcutr,
    identify, kad,
    noise,
    relay,
    request_response::{
        self, Config as RequestResponseConfig, OutboundRequestId, ProtocolSupport,
        ResponseChannel,
    },
    swarm::{NetworkBehaviour, SwarmEvent},
    tcp, yamux,
    identity::Keypair,
    Multiaddr, PeerId, Swarm,
    core::ConnectedPoint
};
use std::collections::{HashMap, HashSet};
use std::net::SocketAddr;
use std::time::Duration;

/// Max addresses to send in a single PEX Addr message.
pub const MAX_PEX_ADDRS: usize = 50;

// ── Behaviour ───────────────────────────────────────────────────────────────

#[derive(NetworkBehaviour)]
pub struct MidstateBehaviour {
    pub rr: request_response::Behaviour<MidstateCodec>,
    pub kademlia: kad::Behaviour<kad::store::MemoryStore>,
    pub identify: identify::Behaviour,
    pub relay_client: relay::client::Behaviour,
    pub relay_server: relay::Behaviour,
    pub dcutr: dcutr::Behaviour,
    pub autonat: autonat::Behaviour,
}

// ── NAT status ──────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NatStatus {
    Unknown,
    Public,
    Private,
}

// ── Events ──────────────────────────────────────────────────────────────────

pub enum NetworkEvent {
    MessageReceived {
        peer: PeerId,
        message: Message,
        channel: Option<ResponseChannel<Message>>,
    },
    PeerConnected(PeerId),
    PeerDisconnected(PeerId),
}

// ── Network API ─────────────────────────────────────────────────────────────

pub struct MidstateNetwork {
    swarm: Swarm<MidstateBehaviour>,
    connected: HashMap<PeerId, ConnectedPoint>,
    pending_requests: HashMap<OutboundRequestId, PeerId>,
    nat_status: NatStatus,
    relay_reservations: HashSet<PeerId>,
    listen_addrs: Vec<Multiaddr>,
    external_addrs: Vec<Multiaddr>,
}

impl MidstateNetwork {
    pub async fn new(
        keypair: Keypair,
        listen_addr: Multiaddr,
        bootstrap_peers: Vec<Multiaddr>,
    ) -> Result<Self> {
        let peer_id = keypair.public().to_peer_id();
        tracing::info!("Local peer id: {}", peer_id);

        // Removed outdated libp2p::swarm::ConnectionLimits block.
        // We handle inbound limits manually in the event loop now.

        let swarm = libp2p::SwarmBuilder::with_existing_identity(keypair.clone())
            .with_tokio()
            .with_tcp(
                tcp::Config::default().nodelay(true),
                noise::Config::new,
                yamux::Config::default,
            )?
            .with_quic()
            .with_relay_client(noise::Config::new, yamux::Config::default)?
            .with_behaviour(|key, relay_client| {
                let local_peer = key.public().to_peer_id();

                let rr = request_response::Behaviour::new(
                    [(MIDSTATE_PROTOCOL, ProtocolSupport::Full)],
                    RequestResponseConfig::default(),
                );

                let kad_store = kad::store::MemoryStore::new(local_peer);
                let mut kademlia = kad::Behaviour::new(local_peer, kad_store);
                kademlia.set_mode(Some(kad::Mode::Client));

                let identify = identify::Behaviour::new(
                    identify::Config::new(
                        "/midstate/id/1.0.0".to_string(),
                        key.public(),
                    )
                    .with_push_listen_addr_updates(true)
                    .with_interval(Duration::from_secs(60)),
                );

                let relay_server = relay::Behaviour::new(
                    local_peer,
                    relay::Config::default(),
                );

                let dcutr = dcutr::Behaviour::new(local_peer);

                let autonat = autonat::Behaviour::new(
                    local_peer,
                    autonat::Config {
                        boot_delay: Duration::from_secs(10),
                        refresh_interval: Duration::from_secs(120),
                        retry_interval: Duration::from_secs(60),
                        throttle_server_period: Duration::from_secs(15),
                        only_global_ips: true,
                        ..Default::default()
                    },
                );

                MidstateBehaviour {
                    rr,
                    kademlia,
                    identify,
                    relay_client,
                    relay_server,
                    dcutr,
                    autonat,
                }
            })?
            .with_swarm_config(|c| {
                c.with_idle_connection_timeout(Duration::from_secs(120))
                // Removed .with_connection_limits(limits)
            })
            .build();

        let mut net = Self {
            swarm,
            connected: HashMap::new(), // Fixed: using HashMap instead of HashSet
            pending_requests: HashMap::new(),
            nat_status: NatStatus::Unknown,
            relay_reservations: HashSet::new(),
            listen_addrs: Vec::new(),
            external_addrs: Vec::new(),
        };

        net.swarm.listen_on(listen_addr.clone())?;

        if let Some(quic_addr) = tcp_to_quic(&listen_addr) {
            match net.swarm.listen_on(quic_addr.clone()) {
                Ok(_) => tracing::info!("Also listening on QUIC: {}", quic_addr),
                Err(e) => tracing::debug!("QUIC listen failed (non-fatal): {}", e),
            }
        }

        for addr in &bootstrap_peers {
            if let Some(peer) = extract_peer_id(addr) {
                net.swarm.behaviour_mut().kademlia.add_address(&peer, addr.clone());
                let relay_addr = addr.clone()
                    .with(libp2p::multiaddr::Protocol::P2pCircuit);
                match net.swarm.listen_on(relay_addr.clone()) {
                    Ok(_) => tracing::info!("Relay-listening through {}", addr),
                    Err(e) => tracing::debug!("Relay listen failed (non-fatal): {}", e),
                }
            }
            if let Err(e) = net.swarm.dial(addr.clone()) {
                tracing::warn!("Failed to dial {}: {}", addr, e);
            }
        }

        if !bootstrap_peers.is_empty() {
            if let Err(e) = net.swarm.behaviour_mut().kademlia.bootstrap() {
                tracing::debug!("Kademlia bootstrap not ready: {}", e);
            }
        }

        Ok(net)
    }

    // ── Public API ──────────────────────────────────────────────────────

    pub fn local_peer_id(&self) -> PeerId {
        *self.swarm.local_peer_id()
    }

    pub fn nat_status(&self) -> NatStatus {
        self.nat_status
    }

    pub fn send(&mut self, peer: PeerId, msg: Message) {
        let req_id = self.swarm.behaviour_mut().rr.send_request(&peer, msg);
        self.pending_requests.insert(req_id, peer);
    }

    pub fn broadcast(&mut self, msg: Message) {
        let peers: Vec<PeerId> = self.connected.keys().copied().collect();
        for peer in peers {
            self.send(peer, msg.clone());
        }
    }

    pub fn broadcast_except(&mut self, exclude: Option<PeerId>, msg: Message) {
        let peers: Vec<PeerId> = self.connected
            .keys()
            .filter(|&&p| Some(p) != exclude)
            .copied()
            .collect();
        for peer in peers {
            self.send(peer, msg.clone());
        }
    }

    pub fn respond(&mut self, channel: ResponseChannel<Message>, msg: Message) {
        if let Err(_) = self.swarm.behaviour_mut().rr.send_response(channel, msg) {
            tracing::warn!("Failed to send response (channel closed)");
        }
    }

    pub fn peer_count(&self) -> usize {
        self.connected.len()
    }

    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.connected.keys().copied().collect()
    }

    pub fn peer_addrs(&self) -> Vec<String> {
        self.connected.keys().map(|p| p.to_string()).collect()
    }

    pub fn add_kad_address(&mut self, peer: PeerId, addr: Multiaddr) {
        self.swarm.behaviour_mut().kademlia.add_address(&peer, addr);
    }

    pub fn random_peer(&self) -> Option<PeerId> {
        use rand::seq::IteratorRandom;
        self.connected
            .keys()
            .copied()
            .choose(&mut rand::thread_rng())
    }

    // ── PEX (Peer Exchange) ─────────────────────────────────────────

    /// Our own externally-reachable addresses (for advertising to peers).
    /// Prefers confirmed external addrs; falls back to listen addrs.
    pub fn advertisable_addrs(&self) -> Vec<String> {
        let local_id = *self.swarm.local_peer_id();
        let p2p_suffix = libp2p::multiaddr::Protocol::P2p(local_id);

        let base = if !self.external_addrs.is_empty() {
            &self.external_addrs
        } else {
            &self.listen_addrs
        };

        base.iter()
            .filter(|a| !is_localhost(a))
            .map(|a| {
                // Append /p2p/<our_id> if not already present
                if extract_peer_id(a).is_some() {
                    a.to_string()
                } else {
                    a.clone().with(p2p_suffix.clone()).to_string()
                }
            })
            .collect()
    }

    /// Multiaddrs of connected peers (from Kademlia routing table).
    /// These are candidates to send in PEX Addr messages.
    pub fn connected_peer_addrs(&mut self) -> Vec<String> {
        let mut addrs = Vec::new();
        for bucket in self.swarm.behaviour_mut().kademlia.kbuckets() {
            for entry in bucket.iter() {
                let peer = entry.node.key.preimage();
                if !self.connected.contains_key(peer) {
                    continue;
                }
                for addr in entry.node.value.iter() {
                    if is_localhost(addr) {
                        continue;
                    }
                    let full = addr.clone()
                        .with(libp2p::multiaddr::Protocol::P2p(*peer));
                    addrs.push(full.to_string());
                }
            }
        }
        addrs.truncate(MAX_PEX_ADDRS);
        addrs
    }

    /// Build the PEX addr list: our own addrs + known peer addrs.
    pub fn pex_addrs(&mut self) -> Vec<String> {
        let mut all = self.advertisable_addrs();
        all.extend(self.connected_peer_addrs());
        all.sort();
        all.dedup();
        all.truncate(MAX_PEX_ADDRS);
        all
    }

    /// Try to dial a multiaddr string. Ignores bad parses or dial failures.
    pub fn dial_addr(&mut self, addr_str: &str) {
        match addr_str.parse::<Multiaddr>() {
            Ok(addr) => {
                // Feed into Kademlia if it has a peer ID
                if let Some(peer) = extract_peer_id(&addr) {
                    if self.connected.contains_key(&peer) || peer == *self.swarm.local_peer_id() {
                        return; // already connected or it's us
                    }
                    self.swarm.behaviour_mut().kademlia.add_address(&peer, addr.clone());
                }
                if let Err(e) = self.swarm.dial(addr.clone()) {
                    tracing::debug!("PEX dial {} failed: {}", addr_str, e);
                }
            }
            Err(e) => {
                tracing::debug!("PEX ignoring bad multiaddr '{}': {}", addr_str, e);
            }
        }
    }

    // ── Event loop ──────────────────────────────────────────────────────

    pub async fn next_event(&mut self) -> NetworkEvent {
        loop {
            match self.swarm.select_next_some().await {
                // ── Request-Response ────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Rr(
                    request_response::Event::Message { peer, message },
                )) => match message {
                    request_response::Message::Request {
                        request, channel, ..
                    } => {
                        return NetworkEvent::MessageReceived {
                            peer,
                            message: request,
                            channel: Some(channel),
                        };
                    }
                    request_response::Message::Response {
                        request_id,
                        response,
                    } => {
                        self.pending_requests.remove(&request_id);
                        return NetworkEvent::MessageReceived {
                            peer,
                            message: response,
                            channel: None,
                        };
                    }
                },
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Rr(
                    request_response::Event::OutboundFailure {
                        peer,
                        request_id,
                        error,
                    },
                )) => {
                    self.pending_requests.remove(&request_id);
                    tracing::warn!("Outbound request to {} failed: {}", peer, error);
                }
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Rr(
                    request_response::Event::InboundFailure { peer, error, .. },
                )) => {
                    tracing::warn!("Inbound request from {} failed: {}", peer, error);
                }
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Rr(
                    request_response::Event::ResponseSent { .. },
                )) => {}

                // ── Identify ────────────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Identify(
                    identify::Event::Received { peer_id, info, .. },
                )) => {
                    for addr in &info.listen_addrs {
                        self.swarm
                            .behaviour_mut()
                            .kademlia
                            .add_address(&peer_id, addr.clone());
                    }
                    self.swarm
                        .behaviour_mut()
                        .autonat
                        .add_server(peer_id, Some(info.observed_addr.clone()));
                }

                // ── AutoNAT ─────────────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Autonat(
                    autonat::Event::StatusChanged { old, new },
                )) => {
                    tracing::info!("AutoNAT status: {:?} → {:?}", old, new);
                    match new {
                        autonat::NatStatus::Public(_addr) => {
                            self.nat_status = NatStatus::Public;
                            self.swarm
                                .behaviour_mut()
                                .kademlia
                                .set_mode(Some(kad::Mode::Server));
                            tracing::info!(
                                "Node is PUBLIC — Kademlia server mode, serving as relay"
                            );
                        }
                        autonat::NatStatus::Private => {
                            self.nat_status = NatStatus::Private;
                            self.swarm
                                .behaviour_mut()
                                .kademlia
                                .set_mode(Some(kad::Mode::Client));
                            tracing::info!(
                                "Node is behind NAT — using relays and hole-punching"
                            );
                        }
                        autonat::NatStatus::Unknown => {
                            self.nat_status = NatStatus::Unknown;
                        }
                    }
                }

                // ── Relay client ────────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::RelayClient(
                    relay::client::Event::ReservationReqAccepted {
                        relay_peer_id, ..
                    },
                )) => {
                    self.relay_reservations.insert(relay_peer_id);
                    tracing::info!(
                        "Relay reservation accepted by {} (total: {})",
                        relay_peer_id,
                        self.relay_reservations.len()
                    );
                }
                SwarmEvent::Behaviour(MidstateBehaviourEvent::RelayClient(event)) => {
                    tracing::debug!("Relay client event: {:?}", event);
                }

                // ── DCUtR (hole-punch) ──────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Dcutr(event)) => {
                    tracing::info!("DCUtR event: {:?}", event);
                }

                // ── Relay server ────────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::RelayServer(event)) => {
                    tracing::debug!("Relay server event: {:?}", event);
                }

                // ── Kademlia ────────────────────────────────────────
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Kademlia(
                    kad::Event::RoutingUpdated { peer, .. },
                )) => {
                    tracing::debug!("Kademlia routing updated: {}", peer);
                }
                SwarmEvent::Behaviour(MidstateBehaviourEvent::Kademlia(_)) => {}

                // ── Catch-all ───────────────────────────────────────
                SwarmEvent::Behaviour(_) => {}

                // ── Connection lifecycle ─────────────────────────────
                SwarmEvent::ConnectionEstablished { peer_id, endpoint, .. } => {
                    // Eclipse Protection: Enforce inbound limit manually
                    if endpoint.is_listener() {
                        let inbound_count = self.connected.values().filter(|e| e.is_listener()).count();
                        if inbound_count >= 40 {
                            tracing::warn!("Max inbound peers (40) reached, dropping {}", peer_id);
                            let _ = self.swarm.disconnect_peer_id(peer_id);
                            continue; // Skip further processing, let them disconnect
                        }
                    }

                    self.connected.insert(peer_id, endpoint.clone());
                    tracing::info!(
                        "Peer connected: {} via {:?} (total: {})",
                        peer_id,
                        endpoint.get_remote_address(),
                        self.connected.len()
                    );
                    return NetworkEvent::PeerConnected(peer_id);
                }
                SwarmEvent::ConnectionClosed {
                    peer_id,
                    num_established,
                    ..
                } => {
                    if num_established == 0 {
                        self.connected.remove(&peer_id);
                        tracing::info!(
                            "Peer disconnected: {} (total: {})",
                            peer_id,
                            self.connected.len()
                        );
                        return NetworkEvent::PeerDisconnected(peer_id);
                    }
                }
                SwarmEvent::NewListenAddr { address, .. } => {
                    tracing::info!("Listening on {}", address);
                    self.listen_addrs.push(address);
                }
                SwarmEvent::ExternalAddrConfirmed { address } => {
                    tracing::info!("External address confirmed: {}", address);
                    if !self.external_addrs.contains(&address) {
                        self.external_addrs.push(address);
                    }
                }
                _ => {}
            }
        }
    }
    
    pub fn outbound_peer_count(&self) -> usize {
        self.connected.values().filter(|e| e.is_dialer()).count()
    }
}

// ── Helpers ──────────────────────────────────────────────────────────────────

pub fn socket_to_multiaddr(addr: SocketAddr) -> Multiaddr {
    use std::net::IpAddr;
    let mut ma = Multiaddr::empty();
    match addr.ip() {
        IpAddr::V4(ip) => ma.push(libp2p::multiaddr::Protocol::Ip4(ip)),
        IpAddr::V6(ip) => ma.push(libp2p::multiaddr::Protocol::Ip6(ip)),
    }
    ma.push(libp2p::multiaddr::Protocol::Tcp(addr.port()));
    ma
}

/// Convert a TCP multiaddr to its QUIC-v1 equivalent.
/// /ip4/0.0.0.0/tcp/9333 → /ip4/0.0.0.0/udp/9333/quic-v1
fn tcp_to_quic(addr: &Multiaddr) -> Option<Multiaddr> {
    let mut components = addr.iter().collect::<Vec<_>>();
    let tcp_idx = components.iter().position(|p| matches!(p, libp2p::multiaddr::Protocol::Tcp(_)))?;
    let port = match components[tcp_idx] {
        libp2p::multiaddr::Protocol::Tcp(p) => p,
        _ => return None,
    };
    components[tcp_idx] = libp2p::multiaddr::Protocol::Udp(port);
    components.insert(tcp_idx + 1, libp2p::multiaddr::Protocol::QuicV1);
    let mut ma = Multiaddr::empty();
    for c in components {
        ma.push(c);
    }
    Some(ma)
}

/// Extract PeerId from a multiaddr like /ip4/.../tcp/.../p2p/<peer_id>
fn extract_peer_id(addr: &Multiaddr) -> Option<PeerId> {
    addr.iter().find_map(|p| match p {
        libp2p::multiaddr::Protocol::P2p(peer_id) => Some(peer_id),
        _ => None,
    })
}

/// Check if a multiaddr points to localhost/loopback.
fn is_localhost(addr: &Multiaddr) -> bool {
    addr.iter().any(|p| match p {
        libp2p::multiaddr::Protocol::Ip4(ip) => ip.is_loopback(),
        libp2p::multiaddr::Protocol::Ip6(ip) => ip.is_loopback(),
        _ => false,
    })
}

// ── Tests ────────────────────────────────────────────────────────────────────

#[cfg(test)]
mod tests {
    use super::*;
    use std::net::Ipv4Addr;

    // ── Helper function tests ───────────────────────────────────────

    #[test]
    fn tcp_to_quic_basic() {
        let tcp: Multiaddr = "/ip4/0.0.0.0/tcp/9333".parse().unwrap();
        let quic = tcp_to_quic(&tcp).unwrap();
        assert_eq!(quic.to_string(), "/ip4/0.0.0.0/udp/9333/quic-v1");
    }

    #[test]
    fn tcp_to_quic_with_ip6() {
        let tcp: Multiaddr = "/ip6/::/tcp/9333".parse().unwrap();
        let quic = tcp_to_quic(&tcp).unwrap();
        assert_eq!(quic.to_string(), "/ip6/::/udp/9333/quic-v1");
    }

    #[test]
    fn tcp_to_quic_no_tcp_returns_none() {
        let udp: Multiaddr = "/ip4/0.0.0.0/udp/9333".parse().unwrap();
        assert!(tcp_to_quic(&udp).is_none());
    }

    #[test]
    fn tcp_to_quic_preserves_peer_id() {
        let tcp: Multiaddr = "/ip4/1.2.3.4/tcp/9333/p2p/12D3KooWDpJ7As7BWAwRMfu1VU2WCqNjvq387JEYKDBj4kx6nXTN"
            .parse().unwrap();
        let quic = tcp_to_quic(&tcp).unwrap();
        let quic_str = quic.to_string();
        assert!(quic_str.contains("/udp/9333/quic-v1"));
        assert!(quic_str.contains("/p2p/12D3KooW"));
    }

    #[test]
    fn extract_peer_id_present() {
        let addr: Multiaddr = "/ip4/1.2.3.4/tcp/9333/p2p/12D3KooWDpJ7As7BWAwRMfu1VU2WCqNjvq387JEYKDBj4kx6nXTN"
            .parse().unwrap();
        assert!(extract_peer_id(&addr).is_some());
    }

    #[test]
    fn extract_peer_id_absent() {
        let addr: Multiaddr = "/ip4/1.2.3.4/tcp/9333".parse().unwrap();
        assert!(extract_peer_id(&addr).is_none());
    }

    #[test]
    fn is_localhost_ipv4_loopback() {
        let addr: Multiaddr = "/ip4/127.0.0.1/tcp/9333".parse().unwrap();
        assert!(is_localhost(&addr));
    }

    #[test]
    fn is_localhost_ipv6_loopback() {
        let addr: Multiaddr = "/ip6/::1/tcp/9333".parse().unwrap();
        assert!(is_localhost(&addr));
    }

    #[test]
    fn is_localhost_public_ip() {
        let addr: Multiaddr = "/ip4/203.0.113.10/tcp/9333".parse().unwrap();
        assert!(!is_localhost(&addr));
    }

    #[test]
    fn socket_to_multiaddr_ipv4() {
        let sa: SocketAddr = "1.2.3.4:9333".parse().unwrap();
        let ma = socket_to_multiaddr(sa);
        assert_eq!(ma.to_string(), "/ip4/1.2.3.4/tcp/9333");
    }

    #[test]
    fn socket_to_multiaddr_ipv6() {
        let sa: SocketAddr = "[::1]:9333".parse().unwrap();
        let ma = socket_to_multiaddr(sa);
        assert_eq!(ma.to_string(), "/ip6/::1/tcp/9333");
    }

    // ── NAT status tests ────────────────────────────────────────────

    #[test]
    fn nat_status_default() {
        // Can't construct MidstateNetwork without async, but we can
        // test the enum directly
        let status = NatStatus::Unknown;
        assert_eq!(status, NatStatus::Unknown);
        assert_ne!(status, NatStatus::Public);
        assert_ne!(status, NatStatus::Private);
    }

    // ── PEX constant tests ──────────────────────────────────────────

    #[test]
    fn max_pex_addrs_is_reasonable() {
        // Sanity: the limit should be between 10 and 1000
        assert!(MAX_PEX_ADDRS >= 10);
        assert!(MAX_PEX_ADDRS <= 1000);
    }
}




══════════════════════════════════════════════════════════════
 FILE: network/protocol.rs
══════════════════════════════════════════════════════════════

use crate::core::{Batch, BatchHeader, Transaction};
use futures::{AsyncRead, AsyncWrite, AsyncReadExt, AsyncWriteExt};
use libp2p::StreamProtocol;
use serde::{Deserialize, Serialize};
use std::io;
use async_trait::async_trait;

pub const MAX_GETBATCHES_COUNT: u64 = 100;

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum Message {
    Transaction(Transaction),
    Batch(Batch),
    GetState,
    StateInfo {
        height: u64,
        depth: u64,
        midstate: [u8; 32],
    },
    GetAddr,
    /// Peer exchange: list of multiaddr strings peers can dial
    Addr(Vec<String>),
    Ping { nonce: u64 },
    Pong { nonce: u64 },
    GetBatches {
        start_height: u64,
        count: u64,
    },
    Batches {
        start_height: u64,
        batches: Vec<Batch>,
    },
    /// Request headers-only to verify PoW quickly
    GetHeaders {
        start_height: u64,
        count: u64,
    },
    /// Response with headers
    Headers {
        start_height: u64,
        headers: Vec<BatchHeader>,
    },

    // ── Dark Pool CoinJoin ──────────────────────────────────────────

    /// Announce intent to mix a specific denomination.
    MixAnnounce {
        mix_id: [u8; 32],
        denomination: u64,
    },
    /// Join an announced mix session with input and desired output.
    MixJoin {
        mix_id: [u8; 32],
        input: crate::core::InputReveal,
        output: crate::core::OutputData,
    },
    MixFee {
        mix_id: [u8; 32],
        input: crate::core::InputReveal,
    },
    /// Broadcast the canonical proposal so all participants can sign.
    MixProposal {
        mix_id: [u8; 32],
        inputs: Vec<crate::core::InputReveal>,
        outputs: Vec<crate::core::OutputData>,
        salt: [u8; 32],
        commitment: [u8; 32],
    },
    /// A participant's signature for their input in the mix.
    MixSign {
        mix_id: [u8; 32],
        /// Index into the proposal's canonical input list.
        input_index: usize,
        signature: Vec<u8>,
    },
}

impl Message {
    pub fn serialize_bin(&self) -> Vec<u8> {
        use bincode::Options;
        bincode::DefaultOptions::new()
            .with_limit(MAX_MSG_SIZE as u64)
            .serialize(self)
            .expect("Serialization failed")
    }

    pub fn deserialize_bin(bytes: &[u8]) -> anyhow::Result<Self> {
        use bincode::Options;
        Ok(bincode::DefaultOptions::new()
            .with_limit(MAX_MSG_SIZE as u64)
            .deserialize(bytes)?)
    }
}

// ── libp2p request-response codec ───────────────────────────────────────────

pub const MIDSTATE_PROTOCOL: StreamProtocol = StreamProtocol::new("/midstate/1.0.0");
const MAX_MSG_SIZE: usize = 10_000_000;

#[derive(Debug, Clone, Default)]
pub struct MidstateCodec;

#[async_trait] // <--- Add this attribute
impl libp2p::request_response::Codec for MidstateCodec {
    type Protocol = StreamProtocol;
    type Request = Message;
    type Response = Message;

    async fn read_request<T>(
        &mut self,
        _protocol: &Self::Protocol,
        io: &mut T,
    ) -> io::Result<Self::Request>
    where
        T: AsyncRead + Unpin + Send,
    {
        read_message(io).await
    }

    async fn read_response<T>(
        &mut self,
        _protocol: &Self::Protocol,
        io: &mut T,
    ) -> io::Result<Self::Response>
    where
        T: AsyncRead + Unpin + Send,
    {
        read_message(io).await
    }

    async fn write_request<T>(
        &mut self,
        _protocol: &Self::Protocol,
        io: &mut T,
        req: Self::Request,
    ) -> io::Result<()>
    where
        T: AsyncWrite + Unpin + Send,
    {
        write_message(io, &req).await
    }

    async fn write_response<T>(
        &mut self,
        _protocol: &Self::Protocol,
        io: &mut T,
        res: Self::Response,
    ) -> io::Result<()>
    where
        T: AsyncWrite + Unpin + Send,
    {
        write_message(io, &res).await
    }
}

async fn read_message<T: AsyncRead + Unpin + Send>(io: &mut T) -> io::Result<Message> {
    let mut len_bytes = [0u8; 4];
    io.read_exact(&mut len_bytes).await?;
    let len = u32::from_le_bytes(len_bytes) as usize;
    if len > MAX_MSG_SIZE {
        return Err(io::Error::new(io::ErrorKind::InvalidData, "message too large"));
    }
    let mut buf = vec![0u8; len];
    io.read_exact(&mut buf).await?;
    Message::deserialize_bin(&buf).map_err(|e| io::Error::new(io::ErrorKind::InvalidData, e))
}

async fn write_message<T: AsyncWrite + Unpin + Send>(io: &mut T, msg: &Message) -> io::Result<()> {
    let bytes = msg.serialize_bin();
    let len = (bytes.len() as u32).to_le_bytes();
    io.write_all(&len).await?;
    io.write_all(&bytes).await?;
    io.close().await?;
    Ok(())
}

// ============================================================
// ADD THIS ENTIRE BLOCK at the bottom of src/network/protocol.rs
// ============================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn message_serialize_deserialize_transaction() {
        let tx = Transaction::Commit { commitment: [0xAA; 32], spam_nonce: 42 };
        let msg = Message::Transaction(tx);
        let bytes = msg.serialize_bin();
        let msg2 = Message::deserialize_bin(&bytes).unwrap();
        match msg2 {
            Message::Transaction(Transaction::Commit { commitment, spam_nonce }) => {
                assert_eq!(commitment, [0xAA; 32]);
                assert_eq!(spam_nonce, 42);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn message_serialize_deserialize_state_info() {
        let msg = Message::StateInfo {
            height: 100,
            depth: 5000,
            midstate: [0xBB; 32],
        };
        let bytes = msg.serialize_bin();
        let msg2 = Message::deserialize_bin(&bytes).unwrap();
        match msg2 {
            Message::StateInfo { height, depth, midstate } => {
                assert_eq!(height, 100);
                assert_eq!(depth, 5000);
                assert_eq!(midstate, [0xBB; 32]);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn message_serialize_deserialize_get_batches() {
        let msg = Message::GetBatches { start_height: 50, count: 20 };
        let bytes = msg.serialize_bin();
        let msg2 = Message::deserialize_bin(&bytes).unwrap();
        match msg2 {
            Message::GetBatches { start_height, count } => {
                assert_eq!(start_height, 50);
                assert_eq!(count, 20);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn message_deserialize_garbage_fails() {
        let garbage = vec![0xFF, 0xFE, 0xFD, 0xFC];
        assert!(Message::deserialize_bin(&garbage).is_err());
    }

    #[test]
    fn message_all_variants_round_trip() {
        use crate::core::types::{InputReveal, OutputData};
        let messages = vec![
            Message::GetState,
            Message::GetAddr,
            Message::Ping { nonce: 12345 },
            Message::Pong { nonce: 54321 },
            Message::Addr(vec![
                "/ip4/203.0.113.10/tcp/9333/p2p/12D3KooWTest".to_string(),
                "/ip4/10.0.0.1/udp/9333/quic-v1/p2p/12D3KooWOther".to_string(),
            ]),
            Message::GetBatches { start_height: 0, count: 100 },
            Message::Batches { start_height: 0, batches: vec![] },
            Message::GetHeaders { start_height: 0, count: 50 },
            Message::Headers { start_height: 0, headers: vec![] },
            Message::MixAnnounce { mix_id: [0; 32], denomination: 8 },
            Message::MixJoin {
                mix_id: [0; 32],
                input: InputReveal { owner_pk: [1; 32], value: 8, salt: [2; 32] },
                output: OutputData { address: [3; 32], value: 8, salt: [4; 32] },
            },
            Message::MixProposal {
                mix_id: [0; 32],
                inputs: vec![],
                outputs: vec![],
                salt: [0; 32],
                commitment: [0; 32],
            },
            Message::MixSign {
                mix_id: [0; 32],
                input_index: 0,
                signature: vec![],
            },
        ];

        for msg in messages {
            let bytes = msg.serialize_bin();
            assert!(Message::deserialize_bin(&bytes).is_ok());
        }
    }

    // ── PEX message tests ───────────────────────────────────────────

    #[test]
    fn addr_message_preserves_multiaddr_strings() {
        let addrs = vec![
            "/ip4/1.2.3.4/tcp/9333/p2p/12D3KooWAbCdEf".to_string(),
            "/ip4/5.6.7.8/udp/9333/quic-v1/p2p/12D3KooWGhIjKl".to_string(),
            "/ip4/10.0.0.1/tcp/9333/p2p/12D3KooWRelay/p2p-circuit/p2p/12D3KooWNatted".to_string(),
        ];
        let msg = Message::Addr(addrs.clone());
        let bytes = msg.serialize_bin();
        let msg2 = Message::deserialize_bin(&bytes).unwrap();
        match msg2 {
            Message::Addr(got) => assert_eq!(got, addrs),
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn addr_message_empty_vec() {
        let msg = Message::Addr(vec![]);
        let bytes = msg.serialize_bin();
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::Addr(addrs) => assert!(addrs.is_empty()),
            _ => panic!("wrong variant"),
        }
    }

    // ── CoinJoin dark pool messages ─────────────────────────────────

    #[test]
    fn mix_announce_round_trip() {
        let msg = Message::MixAnnounce {
            mix_id: [0xAA; 32],
            denomination: 16,
        };
        let bytes = msg.serialize_bin();
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::MixAnnounce { mix_id, denomination } => {
                assert_eq!(mix_id, [0xAA; 32]);
                assert_eq!(denomination, 16);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn mix_join_round_trip() {
        use crate::core::types::{InputReveal, OutputData};
        let msg = Message::MixJoin {
            mix_id: [0xBB; 32],
            input: InputReveal { owner_pk: [1; 32], value: 8, salt: [2; 32] },
            output: OutputData { address: [3; 32], value: 8, salt: [4; 32] },
        };
        let bytes = msg.serialize_bin();
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::MixJoin { mix_id, input, output } => {
                assert_eq!(mix_id, [0xBB; 32]);
                assert_eq!(input.value, 8);
                assert_eq!(output.value, 8);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn mix_proposal_round_trip() {
        use crate::core::types::{InputReveal, OutputData};
        let msg = Message::MixProposal {
            mix_id: [0xCC; 32],
            inputs: vec![
                InputReveal { owner_pk: [1; 32], value: 8, salt: [2; 32] },
                InputReveal { owner_pk: [3; 32], value: 1, salt: [4; 32] },
            ],
            outputs: vec![
                OutputData { address: [5; 32], value: 8, salt: [6; 32] },
            ],
            salt: [0xDD; 32],
            commitment: [0xEE; 32],
        };
        let bytes = msg.serialize_bin();
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::MixProposal { mix_id, inputs, outputs, salt, commitment } => {
                assert_eq!(mix_id, [0xCC; 32]);
                assert_eq!(inputs.len(), 2);
                assert_eq!(outputs.len(), 1);
                assert_eq!(salt, [0xDD; 32]);
                assert_eq!(commitment, [0xEE; 32]);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn mix_sign_round_trip() {
        let msg = Message::MixSign {
            mix_id: [0xFF; 32],
            input_index: 2,
            signature: vec![0xAB; 576],
        };
        let bytes = msg.serialize_bin();
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::MixSign { mix_id, input_index, signature } => {
                assert_eq!(mix_id, [0xFF; 32]);
                assert_eq!(input_index, 2);
                assert_eq!(signature.len(), 576);
            }
            _ => panic!("wrong variant"),
        }
    }

    #[test]
    fn addr_message_large_peer_list() {
        // PEX should handle up to ~1000 addrs without hitting MAX_MSG_SIZE
        let addrs: Vec<String> = (0..1000)
            .map(|i| format!("/ip4/10.{}.{}.{}/tcp/9333/p2p/12D3KooWTest{}", i / 65536, (i / 256) % 256, i % 256, i))
            .collect();
        let msg = Message::Addr(addrs.clone());
        let bytes = msg.serialize_bin();
        assert!(bytes.len() < MAX_MSG_SIZE);
        match Message::deserialize_bin(&bytes).unwrap() {
            Message::Addr(got) => assert_eq!(got.len(), 1000),
            _ => panic!("wrong variant"),
        }
    }
}




══════════════════════════════════════════════════════════════
 FILE: node.rs
══════════════════════════════════════════════════════════════

use crate::core::*;
use crate::core::types::compute_address;
use crate::core::types::{CoinbaseOutput, BatchHeader};
use crate::core::state::{apply_batch, choose_best_state};
use crate::core::extension::{mine_extension, create_extension};
use crate::core::transaction::{apply_transaction, validate_transaction};
use crate::mempool::Mempool;
use crate::metrics::Metrics;
use crate::mix::{MixManager, MixPhase, MixStatusSnapshot};
use crate::network::{Message, MidstateNetwork, NetworkEvent, MAX_GETBATCHES_COUNT};
use crate::storage::Storage;
use crate::wallet::{coinbase_seed, coinbase_salt};
use crate::core::mss;
use crate::core::wots;
use crate::sync::Syncer;
use anyhow::{bail, Result};
use libp2p::{request_response::ResponseChannel, PeerId, Multiaddr, identity::Keypair};
use std::collections::{HashMap, HashSet, VecDeque};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use tokio::time;
use rayon::prelude::*;

const MAX_ORPHAN_BATCHES: usize = 256;
const SYNC_TIMEOUT_SECS: u64 = 120;
const CATCH_UP_THRESHOLD: u64 = 20;

/// Non-blocking sync session driven by the main event loop.
/// Replaces the old blocking `Syncer::sync_via_network` which hijacked the
/// network and dropped unrelated messages.
struct SyncSession {
    peer: PeerId,
    peer_height: u64,
    peer_depth: u64,
    phase: SyncPhase,
    started_at: std::time::Instant,
}

enum SyncPhase {
    /// Downloading headers from genesis to peer_height.
    Headers {
        accumulated: Vec<BatchHeader>,
        cursor: u64,
    },
    /// Headers verified, now downloading batches from fork_height forward.
    Batches {
        headers: Vec<BatchHeader>,
        fork_height: u64,
        candidate_state: State,
        cursor: u64,
        new_history: Vec<(u64, [u8; 32], Batch)>,
    },
}

pub struct Node {
    state: State,
    mempool: Mempool,
    storage: Storage,
    network: MidstateNetwork,
    syncer: Syncer,
    metrics: Metrics,
    is_mining: bool,
     recent_headers: VecDeque<u64>,
    orphan_batches: HashMap<u64, Batch>,
    sync_in_progress: bool,
    sync_requested_up_to: u64,
    sync_session: Option<SyncSession>,
    mining_seed: [u8; 32],
    data_dir: PathBuf,
    chain_history: Vec<(u64, [u8; 32])>,
    finality: crate::core::finality::FinalityEstimator,
    known_pex_addrs: HashSet<String>,
    connected_peers: HashSet<PeerId>,
    // Background mining concurrency
    mining_cancel: Option<Arc<AtomicBool>>,
    mined_batch_rx: tokio::sync::mpsc::UnboundedReceiver<Batch>,
    mined_batch_tx: tokio::sync::mpsc::UnboundedSender<Batch>,
    // CoinJoin mix coordinator
    mix_manager: Arc<RwLock<MixManager>>,
    /// Reveals waiting for their Commit to be mined.
    /// Key: commitment hash, Value: (mix_id, Reveal transaction)
    pending_mix_reveals: HashMap<[u8; 32], ([u8; 32], Transaction)>,
}

#[derive(Clone)]
pub struct NodeHandle {
    state: Arc<RwLock<State>>,
    safe_depth: Arc<RwLock<u64>>,
    mempool_size: Arc<RwLock<usize>>,
    mempool_txs: Arc<RwLock<Vec<Transaction>>>,
    peer_addrs: Arc<RwLock<Vec<String>>>,
    tx_sender: tokio::sync::mpsc::UnboundedSender<NodeCommand>,
    batches_path: PathBuf,
    pub mix_manager: Arc<RwLock<MixManager>>,
}

pub enum NodeCommand {
    SendTransaction(Transaction),
    SubmitMixTransaction { mix_id: [u8; 32], tx: Transaction },
    // --- NEW: P2P Mix Coordination Commands ---
    BroadcastMixAnnounce { mix_id: [u8; 32], denomination: u64 },
    SendMixJoin { coordinator: PeerId, mix_id: [u8; 32], input: InputReveal, output: OutputData },
    SendMixFee { coordinator: PeerId, mix_id: [u8; 32], input: InputReveal },
    SendMixSign { coordinator: PeerId, mix_id: [u8; 32], input_index: usize, signature: Vec<u8> },
    BroadcastMixProposal { mix_id: [u8; 32], proposal: crate::wallet::coinjoin::MixProposal, peers: Vec<PeerId> },
}

#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct ScannedCoin {
    pub address: [u8; 32],
    pub value: u64,
    pub salt: [u8; 32],
    pub coin_id: [u8; 32],
    pub height: u64,
}

impl NodeHandle {
    pub async fn get_state(&self) -> State {
        self.state.read().await.clone()
    }

    /// Returns the current dynamic safe depth calculated by the Bayesian finality estimator.
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use midstate::node::NodeHandle;
    /// # async fn example(handle: NodeHandle) {
    /// let depth = handle.get_safe_depth().await;
    /// println!("Transactions older than {} blocks are final.", depth);
    /// # }
    /// ```
    pub async fn get_safe_depth(&self) -> u64 {
        *self.safe_depth.read().await
    }

    pub async fn check_coin(&self, coin: [u8; 32]) -> bool {
        self.state.read().await.coins.contains(&coin)
    }

    pub async fn get_mempool_info(&self) -> (usize, Vec<Transaction>) {
        let size = *self.mempool_size.read().await;
        let txs = self.mempool_txs.read().await.clone();
        (size, txs)
    }

    pub async fn get_peers(&self) -> Vec<String> {
        self.peer_addrs.read().await.clone()
    }

    pub async fn send_transaction(&self, tx: Transaction) -> Result<()> {
        let state_guard = self.state.read().await;
        validate_transaction(&state_guard, &tx)?;
        drop(state_guard);
        self.tx_sender.send(NodeCommand::SendTransaction(tx))?;
        Ok(())
    }

    pub fn scan_addresses(&self, addresses: &[[u8; 32]], start: u64, end: u64) -> Result<Vec<ScannedCoin>> {
        let store = crate::storage::BatchStore::new(&self.batches_path)?;
        let mut found = Vec::new();
        for height in start..end {
            if let Some(batch) = store.load(height)? {
                for tx in &batch.transactions {
                    if let Transaction::Reveal { outputs, .. } = tx {
                        for out in outputs {
                            if addresses.contains(&out.address) {
                                found.push(ScannedCoin {
                                    address: out.address,
                                    value: out.value,
                                    salt: out.salt,
                                    coin_id: out.coin_id(),
                                    height,
                                });
                            }
                        }
                    }
                }
                for cb in &batch.coinbase {
                    if addresses.contains(&cb.address) {
                        found.push(ScannedCoin {
                            address: cb.address,
                            value: cb.value,
                            salt: cb.salt,
                            coin_id: cb.coin_id(),
                            height,
                        });
                    }
                }
            }
        }
        Ok(found)
    }
    pub fn scan_mss_index(&self, master_pk: &[u8; 32], height: u64) -> Result<u64> {
        let store = crate::storage::BatchStore::new(&self.batches_path)?;
        let mut max_idx: u64 = 0;
        for h in 0..height {
            if let Some(batch) = store.load(h)? {
                max_idx = max_idx.max(scan_txs_for_mss_index(&batch.transactions, master_pk));
            }
        }
        Ok(max_idx)
    }

    // ── CoinJoin mix helpers ────────────────────────────────────────────

    pub async fn mix_create(&self, denomination: u64, min_participants: usize) -> Result<[u8; 32]> {
        let mut mgr = self.mix_manager.write().await;
        let mix_id = mgr.create_session(denomination, min_participants)?;
        drop(mgr); // Drop lock before sending over channel
        
        // Broadcast the announcement to the network
        self.tx_sender.send(NodeCommand::BroadcastMixAnnounce { mix_id, denomination })?;
        Ok(mix_id)
    }

    pub async fn mix_register(
        &self, mix_id: [u8; 32], input: InputReveal, output: OutputData,
    ) -> Result<()> {
        let mut mgr = self.mix_manager.write().await;
        let (is_coord, coord_peer) = mgr.get_session_info(&mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        mgr.register(&mix_id, input.clone(), output.clone(), None)?;

        if !is_coord {
            if let Some(peer) = coord_peer {
                self.tx_sender.send(NodeCommand::SendMixJoin { coordinator: peer, mix_id, input, output })?;
            }
        } else if let Some(proposal) = mgr.try_finalize(&mix_id)? {
            // We are the coordinator and the mix is full. Broadcast proposal!
            let peers = mgr.remote_participants(&mix_id);
            self.tx_sender.send(NodeCommand::BroadcastMixProposal { mix_id, proposal, peers })?;
        }
        Ok(())
    }

    pub async fn mix_set_fee(&self, mix_id: [u8; 32], input: InputReveal) -> Result<()> {
        let mut mgr = self.mix_manager.write().await;
        let (is_coord, coord_peer) = mgr.get_session_info(&mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        mgr.set_fee_input(&mix_id, input.clone(), None)?;

        if !is_coord {
            if let Some(peer) = coord_peer {
                self.tx_sender.send(NodeCommand::SendMixFee { coordinator: peer, mix_id, input })?;
            }
        } else if let Some(proposal) = mgr.try_finalize(&mix_id)? {
            let peers = mgr.remote_participants(&mix_id);
            self.tx_sender.send(NodeCommand::BroadcastMixProposal { mix_id, proposal, peers })?;
        }
        Ok(())
    }

    pub async fn mix_sign(&self, mix_id: [u8; 32], input_index: usize, signature: Vec<u8>) -> Result<()> {
        let mut mgr = self.mix_manager.write().await;
        let (is_coord, coord_peer) = mgr.get_session_info(&mix_id)
            .ok_or_else(|| anyhow::anyhow!("mix session not found"))?;

        mgr.add_signature(&mix_id, input_index, signature.clone())?;

        if !is_coord {
            if let Some(peer) = coord_peer {
                self.tx_sender.send(NodeCommand::SendMixSign { coordinator: peer, mix_id, input_index, signature })?;
            }
        } else if let Some(tx) = mgr.try_build_transaction(&mix_id)? {
            mgr.set_phase(&mix_id, MixPhase::CommitSubmitted);
            self.tx_sender.send(NodeCommand::SubmitMixTransaction { mix_id, tx })?;
        }
        Ok(())
    }

    pub async fn mix_status(&self, mix_id: [u8; 32]) -> Option<MixStatusSnapshot> {
        let mgr = self.mix_manager.read().await;
        mgr.status(&mix_id)
    }

    pub async fn mix_list(&self) -> Vec<MixStatusSnapshot> {
        let mgr = self.mix_manager.read().await;
        mgr.list_sessions()
    }

    pub async fn mix_find_input_index(&self, mix_id: [u8; 32], coin_id: [u8; 32]) -> Option<usize> {
        let mgr = self.mix_manager.read().await;
        mgr.find_input_index(&mix_id, &coin_id)
    }
    
}

pub fn scan_txs_for_mss_index(txs: &[Transaction], master_pk: &[u8; 32]) -> u64 {
    let mut max_idx: u64 = 0;
    for tx in txs {
        if let Transaction::Reveal { inputs, signatures, .. } = tx {
            for (input, sig_bytes) in inputs.iter().zip(signatures.iter()) {
                if input.owner_pk == *master_pk && sig_bytes.len() > wots::SIG_SIZE {
                    if let Ok(mss_sig) = mss::MssSignature::from_bytes(sig_bytes) {
                        // leaf_index is 0-based, so next usable = leaf_index + 1
                        max_idx = max_idx.max(mss_sig.leaf_index + 1);
                    }
                }
            }
        }
    }
    max_idx
}

impl Node {
    pub async fn new(
        data_dir: PathBuf,
        is_mining: bool,
        listen_addr: Multiaddr,
        bootstrap_peers: Vec<Multiaddr>,
    ) -> Result<Self> {
        std::fs::create_dir_all(&data_dir)?;
        let storage = Storage::open(data_dir.join("db"))?;
        let mut state = storage.load_state()?.unwrap_or_else(|| {
            tracing::info!("No saved state, using genesis");
            State::genesis().0
        });

        tracing::info!(
            "Loaded state: height={} depth={} coins={} commitments={}",
            state.height, state.depth, state.coins.len(), state.commitments.len()
        );

        if state.height == 0 {
            match storage.load_batch(0)? {
                None => {
                    tracing::info!("Creating genesis batch (batch_0)");
                    let genesis_coinbase = State::genesis().1;

                    let mut mining_midstate = state.midstate;
                    for cb in &genesis_coinbase {
                        mining_midstate = hash_concat(&mining_midstate, &cb.coin_id());
                    }

                    let mut nonce = 0u64;
                    let extension = loop {
                        let ext = create_extension(mining_midstate, nonce);
                        if ext.final_hash < state.target {
                            tracing::info!("Found deterministic genesis nonce: {}", nonce);
                            break ext;
                        }
                        nonce += 1;
                    };

                    let genesis_batch = Batch {
                        prev_midstate: state.midstate,
                        transactions: vec![],
                        extension,
                        coinbase: genesis_coinbase,
                        timestamp: state.timestamp,
                        target: state.target,
                    };
                    storage.save_batch(0, &genesis_batch)?;
                    apply_batch(&mut state, &genesis_batch, &[])?;
                    state.target = adjust_difficulty(&state);
                    storage.save_state(&state)?;
                    tracing::info!("Genesis batch applied, height now {}", state.height);
                }
                Some(batch) => {
                    if state.height == 0 {
                        apply_batch(&mut state, &batch, &[])?;
                        state.target = adjust_difficulty(&state);
                        storage.save_state(&state)?;
                    }
                }
            }
        }

        let mining_seed = match storage.load_mining_seed()? {
            Some(seed) => {
                tracing::info!("Loaded mining seed");
                seed
            }
            None => {
                let seed: [u8; 32] = rand::random();
                storage.save_mining_seed(&seed)?;
                tracing::info!("Generated new mining seed");
                seed
            }
        };

        // Load or generate libp2p keypair
        let keypair = match load_keypair(&data_dir) {
            Some(kp) => {
                tracing::info!("Loaded peer keypair");
                kp
            }
            None => {
                let kp = Keypair::generate_ed25519();
                save_keypair(&data_dir, &kp);
                tracing::info!("Generated new peer keypair");
                kp
            }
        };

        let network = MidstateNetwork::new(keypair, listen_addr, bootstrap_peers).await?;

        let mut recent_headers = VecDeque::new();
        let window = DIFFICULTY_LOOKBACK as u64;
        let start_height = state.height.saturating_sub(window);

        for h in start_height..state.height {
            if let Some(batch) = storage.load_batch(h)? {
                recent_headers.push_back(batch.timestamp);
            }
        }

        let (mined_batch_tx, mined_batch_rx) = tokio::sync::mpsc::unbounded_channel();

        Ok(Self {
            state,
            mempool: Mempool::new(),
            storage: storage.clone(),
            syncer: Syncer::new(storage),
            network,
            metrics: Metrics::new(),
            is_mining,
            recent_headers, // Updated field
            orphan_batches: HashMap::new(),
            sync_in_progress: false,
            sync_requested_up_to: 0,
            mining_seed,
            data_dir,
            chain_history: Vec::new(),
            //lets assume a hostile environment where 80% of new connections are malicious. 
            finality: crate::core::finality::FinalityEstimator::new(2, 8), 
            sync_session: None,
            known_pex_addrs: HashSet::new(),
            connected_peers: HashSet::new(),
            mining_cancel: None,
            mined_batch_rx,
            mined_batch_tx,
            mix_manager: Arc::new(RwLock::new(MixManager::new())),
            pending_mix_reveals: HashMap::new(),
        })
    }

    pub fn create_handle(&self) -> (NodeHandle, tokio::sync::mpsc::UnboundedReceiver<NodeCommand>) {
        let (tx, rx) = tokio::sync::mpsc::unbounded_channel();
        let handle = NodeHandle {
            state: Arc::new(RwLock::new(self.state.clone())),
            safe_depth: Arc::new(RwLock::new(self.finality.calculate_safe_depth(1e-6))),
            mempool_size: Arc::new(RwLock::new(self.mempool.len())),
            mempool_txs: Arc::new(RwLock::new(self.mempool.transactions().to_vec())),
            peer_addrs: Arc::new(RwLock::new(Vec::new())),
            tx_sender: tx,
            batches_path: self.data_dir.join("db").join("batches"),
            mix_manager: Arc::clone(&self.mix_manager),
        };
        (handle, rx)
    }

    /// Abort any active background mining task so the event loop can adopt a new chain.
    fn cancel_mining(&mut self) {
        if let Some(cancel) = self.mining_cancel.take() {
            cancel.store(true, Ordering::Relaxed);
            // Drain any batch the thread may have sent before seeing the flag
            while self.mined_batch_rx.try_recv().is_ok() {}
            tracing::debug!("Cancelled active mining task for network update.");
        }
    }

    pub async fn run(
        mut self,
        handle: NodeHandle,
        mut cmd_rx: tokio::sync::mpsc::UnboundedReceiver<NodeCommand>,
    ) -> Result<()> {
        let mut mine_interval = time::interval(Duration::from_secs(5));
        let mut save_interval = time::interval(Duration::from_secs(10));
        let mut ui_interval = time::interval(Duration::from_secs(1));
        let mut metrics_interval = time::interval(Duration::from_secs(30));
        let mut sync_poll_interval = time::interval(Duration::from_secs(30));
        let mut mempool_prune_interval = time::interval(Duration::from_secs(60));
        let mut sync_timeout_interval = time::interval(Duration::from_secs(5));
        let mut pex_interval = time::interval(Duration::from_secs(120));
        let mut connection_maintenance = time::interval(Duration::from_secs(15));
        const TARGET_OUTBOUND_PEERS: usize = 8;
        
        // Initial sync: ask all peers for their height
        if self.network.peer_count() > 0 {
            tracing::info!("Requesting chain state from {} peer(s)...", self.network.peer_count());
            self.sync_in_progress = true;
            for peer in self.network.connected_peers() {
                self.network.send(peer, Message::GetState);
            }
            tokio::time::sleep(Duration::from_secs(2)).await;
        }

        loop {
            tokio::select! {
                _ = mine_interval.tick() => {
                    if self.is_mining && !self.sync_in_progress && self.mining_cancel.is_none() {
                        if let Err(e) = self.spawn_mining_task() {
                            tracing::error!("Mining error: {}", e);
                        }
                    }
                }
                Some(batch) = self.mined_batch_rx.recv() => {
                    if let Err(e) = self.handle_mined_batch(batch).await {
                        tracing::error!("Failed to process mined batch: {}", e);
                    }
                }
                _ = save_interval.tick() => {
                    if let Err(e) = self.storage.save_state(&self.state) {
                        tracing::error!("Failed to save state: {}", e);
                    }
                    self.maybe_prune_checkpoints();
                }
                _ = ui_interval.tick() => {
                    *handle.state.write().await = self.state.clone();
                    *handle.safe_depth.write().await = self.finality.calculate_safe_depth(1e-6);
                    *handle.mempool_size.write().await = self.mempool.len();
                    *handle.mempool_txs.write().await = self.mempool.transactions().to_vec();
                    *handle.peer_addrs.write().await = self.network.peer_addrs();
                }
                _ = metrics_interval.tick() => {
                    self.metrics.report();
                }
                _ = mempool_prune_interval.tick() => {
                    self.mempool.prune_invalid(&self.state);
                    // CoinJoin: clean up stale mix sessions
                    self.mix_manager.write().await.cleanup();
                    // CoinJoin: check if any pending Commits have been mined
                    self.check_pending_mix_reveals().await;
                }
                _ = sync_poll_interval.tick() => {
                    if let Some(peer) = self.network.random_peer() {
                        self.network.send(peer, Message::GetState);
                    }
                }
                _ = sync_timeout_interval.tick() => {
                    if let Some(session) = &self.sync_session {
                        if session.started_at.elapsed().as_secs() > SYNC_TIMEOUT_SECS {
                            self.abort_sync_session("timed out");
                        }
                    }
                }
                _ = pex_interval.tick() => {
                    if let Some(peer) = self.network.random_peer() {
                        tracing::debug!("PEX: requesting addrs from {}", peer);
                        self.network.send(peer, Message::GetAddr);
                    }
                }
                
                _ = connection_maintenance.tick() => {
                    let current_outbound = self.network.outbound_peer_count();
                    if current_outbound < TARGET_OUTBOUND_PEERS {
                        let needed = TARGET_OUTBOUND_PEERS - current_outbound;
                        
                        // Pick random addresses from our known pool
                        use rand::seq::IteratorRandom;
                        let mut rng = rand::thread_rng();
                        let to_dial: Vec<String> = self.known_pex_addrs
                            .iter()
                            .choose_multiple(&mut rng, needed)
                            .into_iter()
                            .cloned()
                            .collect();

                        for addr in to_dial {
                            tracing::info!("Maintenance: Dialing {} to maintain outbound ratio", addr);
                            self.network.dial_addr(&addr);
                        }
                    }
                }                
                
                
                Some(cmd) = cmd_rx.recv() => {
                    match cmd {
                        NodeCommand::SendTransaction(tx) => {
                            if let Err(e) = self.handle_new_transaction(tx, None).await {
                                tracing::error!("Failed to handle transaction: {}", e);
                            }
                        }
                        NodeCommand::SubmitMixTransaction { mix_id, tx } => {
                            if let Err(e) = self.handle_mix_transaction(mix_id, tx).await {
                                tracing::error!("Failed to submit mix transaction: {}", e);
                                let mut mgr = self.mix_manager.write().await;
                                mgr.set_phase(&mix_id, MixPhase::Failed(e.to_string()));
                            }
                        }
                        NodeCommand::BroadcastMixAnnounce { mix_id, denomination } => {
                            self.network.broadcast(Message::MixAnnounce { mix_id, denomination });
                        }
                        NodeCommand::SendMixJoin { coordinator, mix_id, input, output } => {
                            self.network.send(coordinator, Message::MixJoin { mix_id, input, output });
                        }
                        NodeCommand::SendMixFee { coordinator, mix_id, input } => {
                            self.network.send(coordinator, Message::MixFee { mix_id, input });
                        }
                        NodeCommand::SendMixSign { coordinator, mix_id, input_index, signature } => {
                            self.network.send(coordinator, Message::MixSign { mix_id, input_index, signature });
                        }
                        NodeCommand::BroadcastMixProposal { mix_id, proposal, peers } => {
                            for peer in peers {
                                self.network.send(peer, Message::MixProposal {
                                    mix_id,
                                    inputs: proposal.inputs.clone(),
                                    outputs: proposal.outputs.clone(),
                                    salt: proposal.salt,
                                    commitment: proposal.commitment,
                                });
                            }
                        }
                    }
                }
                event = self.network.next_event() => {
                    match event {
                        NetworkEvent::MessageReceived { peer, message, channel } => {
                            if let Err(e) = self.handle_message(peer, message, channel).await {
                                tracing::warn!("Error from peer {}: {}", peer, e);
                            }
                        }
                        NetworkEvent::PeerConnected(peer) => {
                            if !self.connected_peers.insert(peer) {
                                // Already connected via another transport — skip
                                continue;
                            }
                            tracing::info!("Peer connected: {}", peer);
                            self.network.send(peer, Message::GetState);
                            self.network.send(peer, Message::GetAddr);
                        }
                        NetworkEvent::PeerDisconnected(peer) => {
                            self.connected_peers.remove(&peer);
                            tracing::info!("Peer disconnected: {}", peer);
                            if self.sync_session.as_ref().map_or(false, |s| s.peer == peer) {
                                self.abort_sync_session("sync peer disconnected");
                            }
                            
                            // Fail any mixes relying on this peer
                            self.mix_manager.write().await.handle_peer_disconnect(peer);
                        }
                    }
                }
            }
        }
    }

    fn send_response(&mut self, channel: Option<ResponseChannel<Message>>, msg: Message) {
        if let Some(ch) = channel {
            self.network.respond(ch, msg);
        }
    }

    fn ack(&mut self, channel: Option<ResponseChannel<Message>>) {
        self.send_response(channel, Message::Pong { nonce: 0 });
    }

    async fn handle_message(
        &mut self,
        from: PeerId,
        msg: Message,
        channel: Option<ResponseChannel<Message>>,
    ) -> Result<()> {
        match msg {
            Message::Transaction(tx) => {
                self.ack(channel);
                self.handle_new_transaction(tx, Some(from)).await?;
            }
            Message::Batch(batch) => {
                self.ack(channel);
                self.handle_new_batch(batch, Some(from)).await?;
            }
            Message::GetState => {
                let response = Message::StateInfo {
                    height: self.state.height,
                    depth: self.state.depth,
                    midstate: self.state.midstate,
                };
                self.send_response(channel, response);
            }

            Message::StateInfo { height, depth, midstate } => {
                self.ack(channel);
                tracing::debug!("Peer {} state: height={} depth={}", from, height, depth);

                if midstate == self.state.midstate && height == self.state.height {
                    self.sync_in_progress = false;
                    self.sync_session = None;
                } else if depth > self.state.depth || height > self.state.height
                    || (depth == self.state.depth && midstate < self.state.midstate)
                {
                    // Don't restart sync if we're already syncing from this peer
                    if self.sync_session.as_ref().map_or(false, |s| s.peer == from) {
                        tracing::debug!("Already syncing from this peer, ignoring duplicate StateInfo");
                    } else {
                        let gap = height.saturating_sub(self.state.height);
                        if gap > 0 && gap <= CATCH_UP_THRESHOLD {
                            // Small gap — just request the missing batches directly.
                            // handle_batches_response will apply them as a linear
                            // extension, or detect a fork and handle it.  This avoids
                            // the O(chain_height) header download + verify cycle.
                            tracing::info!(
                                "Peer {} blocks ahead (h={} vs h={}), requesting batches directly",
                                gap, height, self.state.height
                            );
                            self.cancel_mining();
                            self.sync_in_progress = true;
                            self.sync_requested_up_to = height;
                            let count = gap.min(MAX_GETBATCHES_COUNT);
                            self.network.send(from, Message::GetBatches {
                                start_height: self.state.height,
                                count,
                            });
                        } else {
                            self.start_sync_session(from, height, depth);
                        }
                    }
                } else {
                    tracing::debug!(
                        "Peer {} at equal/lower depth (h={}, d={}) with different chain, resuming mining",
                        from, height, depth
                    );
                    self.sync_in_progress = false;
                }
            }
            
            Message::Ping { nonce } => {
                self.send_response(channel, Message::Pong { nonce });
            }
            Message::Pong { .. } => {
                self.ack(channel);
            }
            Message::GetAddr => {
                let addrs = self.network.pex_addrs();
                tracing::debug!("PEX: sending {} addrs to {}", addrs.len(), from);
                self.send_response(channel, Message::Addr(addrs));
            }
            Message::Addr(addrs) => {
                self.ack(channel);
                let mut new_count = 0;
                for addr_str in &addrs {
                    if !self.known_pex_addrs.contains(addr_str) {
                        self.known_pex_addrs.insert(addr_str.clone());
                     //  self.network.dial_addr(addr_str); lets not autodial, could be someone we don't like
                        new_count += 1;
                    }
                }
                if new_count > 0 {
                    tracing::info!("PEX: received {} addrs from {}, {} new", addrs.len(), from, new_count);
                }
            }
            Message::GetBatches { start_height, count } => {
                let count = count.min(MAX_GETBATCHES_COUNT);
                let end = (start_height + count).min(self.state.height);
                match self.storage.load_batches(start_height, end) {
                    Ok(tagged) => {
                        let actual_start = tagged.first().map(|(h, _)| *h).unwrap_or(start_height);
                        let batches: Vec<Batch> = tagged.into_iter().map(|(_, b)| b).collect();
                        self.send_response(channel, Message::Batches {
                            start_height: actual_start,
                            batches,
                        });
                    }
                    Err(e) => {
                        tracing::warn!("Failed to load batches: {}", e);
                        self.send_response(channel, Message::Batches {
                            start_height,
                            batches: vec![],
                        });
                    }
                }
            }
            Message::Batches { start_height: batch_start, batches } => {
                self.ack(channel);
                if !batches.is_empty() {
                    // If we have an active sync session in the Batches phase from this
                    // peer, route through the sync state machine instead of the normal
                    // batch handler.
                    let is_sync_batches = self.sync_session.as_ref().map_or(false, |s| {
                        s.peer == from && matches!(s.phase, SyncPhase::Batches { .. })
                    });
                    if is_sync_batches {
                        if let Err(e) = self.handle_sync_batches(from, batches).await {
                            tracing::warn!("Error processing sync batches: {}", e);
                            self.abort_sync_session("batch processing error");
                        }
                    } else {
                        self.handle_batches_response(batch_start, batches, from).await?;
                    }
                }
            }
            Message::GetHeaders { start_height, count } => {
                let count = count.min(MAX_GETBATCHES_COUNT);
                let end = (start_height + count).min(self.state.height + 1);
                
                match self.storage.batches.load_headers(start_height, end) {
                    Ok(headers) => {
                        self.send_response(channel, Message::Headers { 
                            start_height, 
                            headers 
                        });
                    }
                    Err(e) => {
                        tracing::warn!("Failed to load headers: {}", e);
                    }
                }
            }
            Message::Headers { start_height: _, headers } => {
                self.ack(channel);
                if let Err(e) = self.handle_sync_headers(from, headers).await {
                    tracing::warn!("Error processing sync headers: {}", e);
                    self.abort_sync_session("header processing error");
                }
            }

            // ── CoinJoin mix messages ───────────────────────────────────

            Message::MixAnnounce { mix_id, denomination } => {
                self.ack(channel);
                let mut mgr = self.mix_manager.write().await;
                if mgr.get_session_info(&mix_id).is_none() {
                    match mgr.create_joining_session(mix_id, denomination, from) {
                        Ok(()) => tracing::info!(
                            "Joined mix session {} (denom={}) from peer {}",
                            hex::encode(mix_id), denomination, from
                        ),
                        Err(e) => tracing::debug!("Ignoring MixAnnounce: {}", e),
                    }
                }
            }

            Message::MixJoin { mix_id, input, output } => {
                self.ack(channel);
                let mut mgr = self.mix_manager.write().await;
                match mgr.register(&mix_id, input, output, Some(from)) {
                    Ok(()) => {
                        tracing::info!("Peer {} joined mix {}", from, hex::encode(mix_id));
                        // Auto-finalize if ready
                        if let Ok(Some(proposal)) = mgr.try_finalize(&mix_id) {
                            let peers = mgr.remote_participants(&mix_id);
                            drop(mgr);
                            // Broadcast proposal to all participants
                            for peer in peers {
                                self.network.send(peer, Message::MixProposal {
                                    mix_id,
                                    inputs: proposal.inputs.clone(),
                                    outputs: proposal.outputs.clone(),
                                    salt: proposal.salt,
                                    commitment: proposal.commitment,
                                });
                            }
                        }
                    }
                    Err(e) => tracing::debug!("MixJoin rejected: {}", e),
                }
            }
            
            Message::MixFee { mix_id, input } => {
                self.ack(channel);
                let mut mgr = self.mix_manager.write().await;
                match mgr.set_fee_input(&mix_id, input, Some(from)) {
                    Ok(()) => {
                        tracing::info!("Peer {} provided fee for mix {}", from, hex::encode(mix_id));
                        // Auto-finalize if ready
                        if let Ok(Some(proposal)) = mgr.try_finalize(&mix_id) {
                            let peers = mgr.remote_participants(&mix_id);
                            drop(mgr);
                            for peer in peers {
                                self.network.send(peer, Message::MixProposal {
                                    mix_id,
                                    inputs: proposal.inputs.clone(),
                                    outputs: proposal.outputs.clone(),
                                    salt: proposal.salt,
                                    commitment: proposal.commitment,
                                });
                            }
                        }
                    }
                    Err(e) => tracing::debug!("MixFee rejected: {}", e),
                }
            }
            
            Message::MixProposal { mix_id, inputs, outputs, salt, commitment } => {
                self.ack(channel);
                let mut mgr = self.mix_manager.write().await;
                match mgr.apply_remote_proposal(&mix_id, inputs, outputs, salt, commitment) {
                    Ok(()) => tracing::info!(
                        "Applied remote mix proposal for {}",
                        hex::encode(mix_id)
                    ),
                    Err(e) => tracing::debug!("MixProposal rejected: {}", e),
                }
            }

            Message::MixSign { mix_id, input_index, signature } => {
                self.ack(channel);
                let mut mgr = self.mix_manager.write().await;
                if let Err(e) = mgr.add_signature(&mix_id, input_index, signature) {
                    tracing::debug!("MixSign rejected: {}", e);
                } else {
                    // Auto-build if all sigs collected
                    if let Ok(Some(tx)) = mgr.try_build_transaction(&mix_id) {
                        tracing::info!("Mix {} complete from p2p signatures", hex::encode(mix_id));
                        mgr.set_phase(&mix_id, MixPhase::CommitSubmitted);
                        drop(mgr);
                        if let Err(e) = self.handle_mix_transaction(mix_id, tx).await {
                            tracing::error!("Failed to submit p2p mix tx: {}", e);
                            self.mix_manager.write().await
                                .set_phase(&mix_id, MixPhase::Failed(e.to_string()));
                        }
                    }
                }
            }
        }
        Ok(())
    }

    // ── Non-blocking sync state machine ─────────────────────────────────

    fn start_sync_session(&mut self, peer: PeerId, peer_height: u64, peer_depth: u64) {
        self.cancel_mining();
        tracing::info!(
            "Starting headers-first sync: peer(h={}, d={}) vs us(h={}, d={})",
            peer_height, peer_depth, self.state.height, self.state.depth
        );
        self.sync_in_progress = true;
        self.sync_session = Some(SyncSession {
            peer,
            peer_height,
            peer_depth,
            phase: SyncPhase::Headers {
                accumulated: Vec::new(),
                cursor: 0,
            },
            started_at: std::time::Instant::now(),
        });

        // Request first chunk of headers from genesis
        let count = 100.min(peer_height);
        self.network.send(peer, Message::GetHeaders { start_height: 0, count });
    }

    fn abort_sync_session(&mut self, reason: &str) {
        if self.sync_session.is_some() {
            tracing::warn!("Aborting sync session: {}", reason);
        }
        self.sync_session = None;
        self.sync_in_progress = false;
    }

    async fn handle_sync_headers(&mut self, from: PeerId, headers: Vec<BatchHeader>) -> Result<()> {
        // Extract state from the session — only accept headers from the sync peer
        let (peer_height, cursor) = match &self.sync_session {
            Some(s) if s.peer == from => {
                match &s.phase {
                    SyncPhase::Headers { cursor, .. } => (s.peer_height, *cursor),
                    _ => return Ok(()), // Wrong phase, ignore
                }
            }
            _ => return Ok(()), // No session or wrong peer, ignore
        };

        if headers.is_empty() {
            self.abort_sync_session("peer sent empty headers");
            return Ok(());
        }

        // Accumulate headers
        let new_cursor = cursor + headers.len() as u64;
        match &mut self.sync_session {
            Some(s) => {
                if let SyncPhase::Headers { accumulated, cursor } = &mut s.phase {
                    accumulated.extend(headers);
                    *cursor = new_cursor;
                }
            }
            _ => unreachable!(),
        }

        if new_cursor < peer_height {
            // Need more headers — request next chunk
            let count = 100.min(peer_height - new_cursor);
            self.network.send(from, Message::GetHeaders { start_height: new_cursor, count });
            return Ok(());
        }

        // All headers received — take ownership of the session data
        let session = self.sync_session.take().unwrap();
        let all_headers = match session.phase {
            SyncPhase::Headers { accumulated, .. } => accumulated,
            _ => unreachable!(),
        };

        tracing::info!("Downloaded {} headers, verifying PoW + linkage...", all_headers.len());

        // Validate the full header chain
        if let Err(e) = Syncer::verify_header_chain(&all_headers) {
            tracing::warn!("Peer header chain invalid: {}", e);
            self.sync_in_progress = false;
            return Ok(());
        }
        tracing::info!("Header chain verified. Finding fork point...");

        // Find fork point using local storage
        let fork_height = self.syncer.find_fork_point(&all_headers, self.state.height)?;
        tracing::info!(
            "Fork point at height {}. Will download batches {}..{}",
            fork_height, fork_height, session.peer_height
        );

        if fork_height >= session.peer_height {
            // Already in sync
            tracing::info!("Already in sync with peer");
            self.sync_in_progress = false;
            return Ok(());
        }

        // Build the candidate state at the fork point
        let candidate_state = if fork_height == 0 {
            State::genesis().0
        } else if fork_height <= self.state.height {
            // Reorg: rebuild state to the fork point
            self.syncer.rebuild_state_to(fork_height)?
        } else {
            // Peer extends us — start from our current state
            self.state.clone()
        };

        // Transition to Batches phase
        self.sync_session = Some(SyncSession {
            peer: from,
            peer_height: session.peer_height,
            peer_depth: session.peer_depth,
            phase: SyncPhase::Batches {
                headers: all_headers,
                fork_height,
                candidate_state,
                cursor: fork_height,
                new_history: Vec::new(),
            },
            started_at: session.started_at,
        });

        // Request first chunk of batches from the fork point
        let count = (session.peer_height - fork_height).min(MAX_GETBATCHES_COUNT);
        self.network.send(from, Message::GetBatches { start_height: fork_height, count });

        Ok(())
    }

    async fn handle_sync_batches(&mut self, from: PeerId, batches: Vec<Batch>) -> Result<()> {
        if batches.is_empty() {
            self.abort_sync_session("peer sent empty batches");
            return Ok(());
        }

        // Take the session to work with it
        let mut session = match self.sync_session.take() {
            Some(s) if s.peer == from => s,
            other => {
                self.sync_session = other; // put it back
                return Ok(());
            }
        };

        let (headers, _fork_height, candidate_state, cursor, new_history) = match &mut session.phase {
            SyncPhase::Batches { headers, fork_height, candidate_state, cursor, new_history } => {
                (headers, *fork_height, candidate_state, cursor, new_history)
            }
            _ => {
                self.sync_session = Some(session); // wrong phase, put it back
                return Ok(());
            }
        };

        // Build a bounded sliding window of recent timestamps (mirrors
        // evaluate_alternative_chain).  Both validate_timestamp and
        // adjust_difficulty uses ASERT anchored to genesis,
        // entries, so there is no need to collect every timestamp from genesis.
        let window_size = DIFFICULTY_LOOKBACK as usize;
        let initial_cursor = *cursor as usize;
        let window_start = initial_cursor.saturating_sub(window_size);
        let mut recent_ts: Vec<u64> = headers[window_start..initial_cursor]
            .iter()
            .map(|h| h.timestamp)
            .collect();

        // Apply each batch, verifying against the already-validated headers
        for batch in &batches {
            let height = *cursor;
            let hdr_idx = height as usize;

            if hdr_idx >= headers.len() {
                tracing::warn!("Batch height {} exceeds header count {}", height, headers.len());
                self.sync_in_progress = false;
                return Ok(());
            }

            let header = &headers[hdr_idx];

            // Integrity: batch PoW must match the already-verified header
            if batch.extension.final_hash != header.extension.final_hash {
                tracing::warn!("Batch at height {} does not match verified header PoW", height);
                self.sync_in_progress = false;
                return Ok(());
            }
            let calc = batch.header();
            if calc.post_tx_midstate != header.post_tx_midstate {
                tracing::warn!("Batch at height {} tx commitment does not match header", height);
                self.sync_in_progress = false;
                return Ok(());
            }

            // Apply to candidate state (do NOT save to disk yet — if sync
            // aborts before the reorg is committed, premature writes would
            // leave a Frankenstein chain on disk: some heights from the peer,
            // some from our old chain.  Batches are persisted atomically in
            // perform_reorg only after we decide to adopt.)
            recent_ts.push(candidate_state.timestamp);
            if recent_ts.len() > window_size {
                recent_ts.remove(0);
            }
            apply_batch(candidate_state, batch, &recent_ts)?;
            candidate_state.target = adjust_difficulty(candidate_state);
            new_history.push((height, candidate_state.midstate, batch.clone()));
            *cursor += 1;
            tokio::task::yield_now().await;
        }

        let current_cursor = *cursor;
        let peer_height = session.peer_height;

        tracing::info!("Applied sync batches up to height {}/{}", current_cursor, peer_height);

        if current_cursor < peer_height {
            // Need more batches — request next chunk
            let count = (peer_height - current_cursor).min(MAX_GETBATCHES_COUNT);
            self.network.send(from, Message::GetBatches { start_height: current_cursor, count });
            self.sync_session = Some(session); // put session back
            return Ok(());
        }

        // All batches applied — check if we should adopt this chain
        let (final_state, final_history) = match session.phase {
            SyncPhase::Batches { candidate_state, new_history, .. } => {
                (candidate_state, new_history)
            }
            _ => unreachable!(),
        };

        if final_state.depth > self.state.depth
            || (final_state.depth == self.state.depth && final_state.midstate < self.state.midstate)
        {
            tracing::info!(
                "✓ Sync complete! Adopting chain: height {} -> {}, depth {} -> {}",
                self.state.height, final_state.height,
                self.state.depth, final_state.depth
            );
            self.perform_reorg(final_state, final_history)?;
            // Try to apply any broadcast blocks that were orphaned during sync
            // — they may now chain onto the adopted state.
            self.try_apply_orphans().await;
        } else {
            tracing::info!(
                "Sync complete but peer chain has less work (depth {} <= {}), keeping ours",
                final_state.depth, self.state.depth
            );
        }

        self.sync_in_progress = false;
        // sync_session is already None (we took it above)

        // Immediately check if the peer has mined more blocks while we were
        // syncing.  Without this, we'd have to wait for the next broadcast
        // (which will fail because we missed intermediate blocks) before
        // discovering we're still behind — creating a catch-up death spiral
        // where each cycle takes ~5 s and the miner advances by ~1 block.
        self.network.send(from, Message::GetState);

        Ok(())
    }

    async fn handle_new_transaction(&mut self, tx: Transaction, from: Option<PeerId>) -> Result<()> {
        match self.mempool.add(tx.clone(), &self.state) {
            Ok(_) => {
                self.metrics.inc_transactions_processed();
                self.network.broadcast_except(from, Message::Transaction(tx));
                Ok(())
            }
            Err(e) => {
                self.metrics.inc_invalid_transactions();
                Err(e)
            }
        }
    }

    // ── Fix A: New find_fork_point method ────────────────────────────────
    /// Find the height where our chain and the alternative chain diverge
    /// by comparing batches side-by-side.
    fn find_fork_point(&self, alternative_batches: &[Batch], alt_start_height: u64) -> Result<u64> {
        for (i, alt_batch) in alternative_batches.iter().enumerate() {
            let height = alt_start_height + i as u64;
            match self.storage.load_batch(height)? {
                Some(our_batch) => {
                    if our_batch.extension.final_hash != alt_batch.extension.final_hash {
                        tracing::info!(
                            "Fork point: height {} — our final_hash={} alt final_hash={}",
                            height,
                            hex::encode(&our_batch.extension.final_hash[..8]),
                            hex::encode(&alt_batch.extension.final_hash[..8])
                        );
                        return Ok(height);
                    }
                }
                None => {
                    tracing::info!("Fork point: height {} — we have no batch here (alt extends us)", height);
                    return Ok(height);
                }
            }
        }

        anyhow::bail!("No divergence found — chains are identical over the received range")
    }

    // ── Fix D: Simplified evaluate_alternative_chain ─────────────────────
    async fn evaluate_alternative_chain(
        &mut self,
        fork_height: u64,
        alternative_batches: &[Batch],
        _from: PeerId,
    ) -> Result<Option<(State, Vec<(u64, [u8; 32], Batch)>)>> {
        // Simplified fork_state derivation
        let fork_state = if fork_height == 0 {
            State::genesis().0
        } else if fork_height <= self.state.height.saturating_sub(self.finality.calculate_safe_depth(1e-6)) {
            tracing::warn!("Fork at {} exceeds statistical safe depth, rejecting", fork_height);
            return Ok(None);
        } else {
            self.rebuild_state_at_height(fork_height)?
        };

        let mut candidate_state = fork_state;
        let mut new_history = Vec::new();
        
        // Use headers instead of states
        let mut recent_headers: Vec<u64> = Vec::new();

        let window_size = DIFFICULTY_LOOKBACK as usize;
        let start_height = fork_height.saturating_sub(window_size as u64);

        for h in start_height..fork_height {
            if let Some(batch) = self.storage.load_batch(h)? {
                recent_headers.push(batch.timestamp);
            }
        }

        for (i, batch) in alternative_batches.iter().enumerate() {
            recent_headers.push(candidate_state.timestamp);
            if recent_headers.len() > window_size {
                recent_headers.remove(0);
            }

            if batch.prev_midstate != candidate_state.midstate {
                tracing::warn!(
                    "Alternative chain broken at batch index {} (height {})",
                    i, fork_height + i as u64
                );
                return Ok(None);
            }

            match apply_batch(&mut candidate_state, batch, &recent_headers) {
                Ok(_) => {
                    // Adjust difficulty via ASERT
                    candidate_state.target = adjust_difficulty(&candidate_state);
                    new_history.push((
                        fork_height + i as u64,
                        candidate_state.midstate,
                        batch.clone(),
                    ));
                }
                Err(e) => {
                    tracing::warn!("Alternative chain invalid at height {}: {}", fork_height + i as u64, e);
                    return Ok(None);
                }
            }
        }

        if candidate_state.depth > self.state.depth {
            tracing::warn!(
                "REORG DETECTED: Alternative chain has more work (depth {} > {})",
                candidate_state.depth, self.state.depth
            );
            Ok(Some((candidate_state, new_history)))
        } else {
            tracing::debug!(
                "Rejecting alternative chain: insufficient work (depth {} <= {})",
                candidate_state.depth, self.state.depth
            );
            Ok(None)
        }
    }

    // ── Fix C (Fix 3): Added sync_in_progress = false at end ────────────
    fn perform_reorg(
        &mut self,
        new_state: State,
        new_history: Vec<(u64, [u8; 32], Batch)>,
    ) -> Result<()> {
        self.cancel_mining();


        let fork_height = new_history.first().map(|(h, _, _)| *h).unwrap_or(0);

        let is_actual_reorg = fork_height < self.state.height;

        if is_actual_reorg {
            tracing::warn!(
                "CHAIN REORG at fork height {}: replacing blocks {}..{} with new chain to {}",
                fork_height, fork_height, self.state.height, new_state.height
            );
            self.finality.observe_adversarial();
        } else {
            tracing::info!(
                "Chain extension via sync: height {} -> {}",
                self.state.height, new_state.height
            );
        }

        // Load abandoned batches from disk BEFORE overwriting them
        let mut abandoned_txs = Vec::new();
        if is_actual_reorg {
            for (h, _) in self.chain_history.iter().filter(|(h, _)| *h >= fork_height) {
                if let Ok(Some(batch)) = self.storage.load_batch(*h) {
                    abandoned_txs.extend(batch.transactions);
                }
            }
        }

        for (height, _, batch) in &new_history {
            if let Err(e) = self.storage.save_batch(*height, batch) {
                tracing::error!("Failed to save reorg batch at height {}: {}", height, e);
            }
        }

        self.state = new_state;
        self.chain_history.retain(|(h, _)| *h < fork_height);
        self.chain_history.extend(new_history.iter().map(|(h, ms, _)| (*h, *ms)));

        // Rebuild headers cache from disk
        self.recent_headers.clear();
        let window = DIFFICULTY_LOOKBACK as u64;
        let start = self.state.height.saturating_sub(window);
        
        for h in start..self.state.height {
            if let Some(batch) = self.storage.load_batch(h)? {
                self.recent_headers.push_back(batch.timestamp);
            }
        }

        self.state.target = adjust_difficulty(&self.state);

        self.mempool.re_add(abandoned_txs, &self.state);

        self.mempool.prune_invalid(&self.state);
        if is_actual_reorg {
            self.metrics.inc_reorgs();
        }
        self.storage.save_state(&self.state)?;

        self.sync_in_progress = false;

        Ok(())
    }

    fn rebuild_state_at_height(&self, target_height: u64) -> Result<State> {
        let mut state = State::genesis().0;
        let mut recent_headers: Vec<u64> = Vec::new();

        for h in 0..target_height {
            if let Some(batch) = self.storage.load_batch(h)? {
                recent_headers.push(state.timestamp);
                if recent_headers.len() > DIFFICULTY_LOOKBACK as usize {
                    recent_headers.remove(0);
                }
                apply_batch(&mut state, &batch, &recent_headers)?;
                state.target = adjust_difficulty(&state);
            } else {
                anyhow::bail!("Missing batch at height {} needed for reorg", h);
            }
        }

        Ok(state)
    }

    /// Prune checkpoints from deeply finalized batches.
    ///
    /// Safe to call after every state advancement — internally tracks what's
    /// already pruned and skips redundant work.
    fn maybe_prune_checkpoints(&self) {
        if self.state.height > PRUNE_DEPTH {
            let prune_below = self.state.height - PRUNE_DEPTH;
            if let Err(e) = self.storage.prune_checkpoints(prune_below) {
                tracing::warn!("Checkpoint pruning failed: {}", e);
            }
        }
    }

    /// Handle a completed CoinJoin mix: submit Commit, queue Reveal.
    async fn handle_mix_transaction(&mut self, mix_id: [u8; 32], reveal_tx: Transaction) -> Result<()> {
        // Extract the commitment from the reveal tx
        let (input_ids, output_ids, salt) = match &reveal_tx {
            Transaction::Reveal { inputs, outputs, salt, .. } => {
                let ins: Vec<[u8; 32]> = inputs.iter().map(|i| i.coin_id()).collect();
                let outs: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();
                (ins, outs, *salt)
            }
            _ => bail!("expected Reveal transaction"),
        };

        let commitment = compute_commitment(&input_ids, &output_ids, &salt);

        // Mine spam nonce for the Commit
        let spam_nonce = {
            let mut n = 0u64;
            loop {
                let h = hash_concat(&commitment, &n.to_le_bytes());
                if u16::from_be_bytes([h[0], h[1]]) == 0x0000 {
                    break n;
                }
                n += 1;
            }
        };

        let commit_tx = Transaction::Commit { commitment, spam_nonce };
        tracing::info!(
            "CoinJoin mix {}: submitting Commit ({})",
            hex::encode(mix_id), hex::encode(commitment)
        );

        // Submit Commit to mempool
        self.handle_new_transaction(commit_tx, None).await?;

        // Queue Reveal for when the Commit gets mined
        self.pending_mix_reveals.insert(commitment, (mix_id, reveal_tx));
        Ok(())
    }

    /// Check if any pending CoinJoin Commits have been mined, and if so, submit their Reveals.
    async fn check_pending_mix_reveals(&mut self) {
        if self.pending_mix_reveals.is_empty() {
            return;
        }

        let mut to_reveal = Vec::new();
        for (commitment, _) in &self.pending_mix_reveals {
            // A commitment is "mined" when it's in the state accumulator
            if self.state.commitments.contains(commitment) {
                to_reveal.push(*commitment);
            }
        }

        for commitment in to_reveal {
            if let Some((mix_id, reveal_tx)) = self.pending_mix_reveals.remove(&commitment) {
                tracing::info!(
                    "CoinJoin mix {}: Commit mined, submitting Reveal",
                    hex::encode(mix_id)
                );
                match self.handle_new_transaction(reveal_tx, None).await {
                    Ok(()) => {
                        self.mix_manager.write().await
                            .set_phase(&mix_id, MixPhase::Complete);
                        tracing::info!("CoinJoin mix {} complete!", hex::encode(mix_id));
                    }
                    Err(e) => {
                        tracing::error!("CoinJoin Reveal failed for mix {}: {}", hex::encode(mix_id), e);
                        self.mix_manager.write().await
                            .set_phase(&mix_id, MixPhase::Failed(format!("reveal failed: {}", e)));
                    }
                }
            }
        }
    }

    async fn handle_new_batch(&mut self, batch: Batch, from: Option<PeerId>) -> Result<()> {
        // Fast pre-checks BEFORE cloning state (which copies the entire SMT).
        if batch.prev_midstate != self.state.midstate {
            tracing::debug!("Received orphan block (parent mismatch), queuing for later.");

            const ORPHAN_LIMIT: usize = 64;
            if self.orphan_batches.len() >= ORPHAN_LIMIT {
                self.orphan_batches.clear();
            }

            let estimated_height = self.state.height + 1;
            self.orphan_batches.insert(estimated_height, batch);

            if !self.sync_in_progress {
                if let Some(peer) = from {
                    self.sync_in_progress = true;
                    self.network.send(peer, Message::GetState);
                }
            }
            return Ok(());
        }

        if batch.target != self.state.target {
            tracing::debug!("Batch target mismatch, ignoring");
            return Ok(());
        }

        // Checks passed — now clone state and apply fully.
        let mut candidate_state = self.state.clone();
        match apply_batch(&mut candidate_state, &batch, self.recent_headers.make_contiguous()) {
            Ok(_) => {
                let best = choose_best_state(&self.state, &candidate_state);
                let is_reorg = best.height == self.state.height &&
                               best.midstate != self.state.midstate;

                if best.height > self.state.height || is_reorg {
                    self.cancel_mining();

                    if is_reorg {
                        tracing::warn!("REORG at height {}", self.state.height);
                        self.metrics.inc_reorgs();
                    }

                    self.recent_headers.push_back(self.state.timestamp);
                    if self.recent_headers.len() > DIFFICULTY_LOOKBACK as usize {
                        self.recent_headers.pop_front();
                    }
                    let pre_height = self.state.height;
                    self.state = candidate_state;
                    self.storage.save_batch(pre_height, &batch)?;
                    
                    self.state.target = adjust_difficulty(&self.state);
                    
                    self.metrics.inc_batches_processed();
                    self.mempool.prune_invalid(&self.state);

                    self.chain_history.push((pre_height, self.state.midstate));
                    self.finality.observe_honest();
                    let safe_depth = self.finality.calculate_safe_depth(1e-6);
                    let cutoff_height = self.state.height.saturating_sub(safe_depth);
                    self.chain_history.retain(|(h, _)| *h >= cutoff_height);

                    self.network.broadcast_except(from, Message::Batch(batch));
                    tracing::info!("Applied new batch from peer, height now {}", self.state.height);
                    self.try_apply_orphans().await;
                }
                Ok(())
            }
            Err(e) => {
                tracing::debug!("Batch rejected after full validation: {}", e);
                self.finality.observe_adversarial();
                Ok(())
            }
        }
    }

    // ── Fix B: Rewritten handle_batches_response ─────────────────────────
    async fn handle_batches_response(&mut self, batch_start_height: u64, batches: Vec<Batch>, from: PeerId) -> Result<()> {
        if batches.is_empty() { return Ok(()); }
        tracing::info!("Received {} batch(es) starting at height {} from peer {}", batches.len(), batch_start_height, from);

        // Try 1: Do they extend our current chain directly?
        // Cheap midstate check before expensive state clone.
        if batches[0].prev_midstate == self.state.midstate {
            let mut test_state = self.state.clone();
            if apply_batch(&mut test_state, &batches[0], self.recent_headers.make_contiguous()).is_ok() {
                return self.process_linear_extension(batches, from).await;
            }
        }

        // Try 2: Do any of them extend our chain? (we might already have some)
        for (i, batch) in batches.iter().enumerate() {
            if batch.prev_midstate != self.state.midstate { continue; }
            let mut candidate = self.state.clone();
            if apply_batch(&mut candidate, batch, self.recent_headers.make_contiguous()).is_ok() {
                tracing::info!("Found linear extension at batch index {}", i);
                return self.process_linear_extension(batches[i..].to_vec(), from).await;
            }
        }

        // Try 3: This is a fork. Find the fork point.
        match self.find_fork_point(&batches, batch_start_height) {
            Ok(fork_height) => {
                tracing::info!("Fork detected at height {}", fork_height);
                let offset = fork_height.saturating_sub(batch_start_height) as usize;
                let relevant = if offset < batches.len() { &batches[offset..] } else { &batches };

                match self.evaluate_alternative_chain(fork_height, relevant, from).await {
                    Ok(Some((new_state, new_history))) => {
                        self.perform_reorg(new_state, new_history)?;
                        self.try_apply_orphans().await;
                        // Check if peer has even more blocks
                        self.network.send(from, Message::GetState);
                    }
                    Ok(None) => {
                        tracing::debug!("Alternative chain rejected (insufficient work)");
                    }
                    Err(e) => {
                        tracing::warn!("Error evaluating fork: {}", e);
                    }
                }
            }
            Err(e) => {
                tracing::debug!("Could not find fork point: {}", e);
            }
        }

        // Always clear sync flag after processing batch response
        self.sync_in_progress = false;
        Ok(())
    }

    // ── Fix C (Fix 2): Added sync_in_progress clear at end ──────────────
    async fn process_linear_extension(&mut self, batches: Vec<Batch>, from: PeerId) -> Result<()> {
        self.cancel_mining();
        let mut applied = 0;
        for batch in batches {
            // Cheap check before expensive clone
            if batch.prev_midstate != self.state.midstate { break; }
            let mut candidate = self.state.clone();
            if apply_batch(&mut candidate, &batch, self.recent_headers.make_contiguous()).is_ok() {
                self.recent_headers.push_back(self.state.timestamp);
                if self.recent_headers.len() > DIFFICULTY_LOOKBACK as usize {
                    self.recent_headers.pop_front();
                }
                self.storage.save_batch(candidate.height - 1, &batch)?;
                self.state = candidate;
                
                self.state.target = adjust_difficulty(&self.state);
                self.metrics.inc_batches_processed();

                self.chain_history.push((self.state.height, self.state.midstate));
                self.finality.observe_honest();
                let safe_depth = self.finality.calculate_safe_depth(1e-6);
                let cutoff = self.state.height.saturating_sub(safe_depth);
                self.chain_history.retain(|(h, _)| *h >= cutoff);

                applied += 1;
            } else {
                break;
            }
        }

        if applied > 0 {
            tracing::info!("Synced {} batch(es), now at height {}", applied, self.state.height);
            self.mempool.prune_invalid(&self.state);
            self.try_apply_orphans().await;

            if self.state.height >= self.sync_requested_up_to {
                self.sync_in_progress = false;
            } else {
                // Still behind — request more batches from same peer
                let start = self.state.height;
                let count = (self.sync_requested_up_to.saturating_sub(start) + 1).min(MAX_GETBATCHES_COUNT);
                tracing::info!("Continuing sync from peer {} (requesting {} batches from {})", from, count, start);
                self.network.send(from, Message::GetBatches { start_height: start, count });
            }
        } else {
            self.sync_in_progress = false;
        }

        Ok(())
    }

    async fn try_apply_orphans(&mut self) {
        let mut applied = 0;
        loop {
            // First try the expected height key (fast path for normal operation)
            let height = self.state.height;
            let matching_key = if self.orphan_batches.contains_key(&height) {
                Some(height)
            } else {
                // Scan all orphans for one whose prev_midstate matches our
                // current state.  This is essential after reorgs where
                // broadcast blocks were stored at incorrect estimated heights.
                self.orphan_batches.iter()
                    .find(|(_, batch)| batch.prev_midstate == self.state.midstate)
                    .map(|(&k, _)| k)
            };

            let batch = match matching_key.and_then(|k| self.orphan_batches.remove(&k)) {
                Some(b) => b,
                None => break,
            };

            let mut candidate = self.state.clone();
            match apply_batch(&mut candidate, &batch, self.recent_headers.make_contiguous()) {
                Ok(_) => {
                    self.cancel_mining();
                    self.recent_headers.push_back(self.state.timestamp);
                    if self.recent_headers.len() > DIFFICULTY_LOOKBACK as usize {
                        self.recent_headers.pop_front();
                    }
                    self.storage.save_batch(candidate.height - 1, &batch).ok();
                    self.state = candidate;
                    
                    self.state.target = adjust_difficulty(&self.state);
                    self.metrics.inc_batches_processed();
                    self.mempool.prune_invalid(&self.state);
                    applied += 1;
                }
                Err(e) => {
                    tracing::debug!("Orphan batch still invalid: {}", e);
                    break;
                }
            }
        }

        if applied > 0 {
            tracing::info!("Applied {} orphan batch(es)", applied);
        }

        let cutoff = self.state.height.saturating_sub(10);
        self.orphan_batches.retain(|&h, _| h > cutoff);

        while self.orphan_batches.len() > MAX_ORPHAN_BATCHES {
            if let Some(&oldest) = self.orphan_batches.keys().min() {
                self.orphan_batches.remove(&oldest);
            }
        }
    }

    fn generate_coinbase(&self, height: u64, total_fees: u64) -> Vec<CoinbaseOutput> {
        let reward = block_reward(height);
        let total_value = reward + total_fees;
        let denominations = decompose_value(total_value);

        let mining_seed = self.mining_seed; // Extract seed here

        denominations.into_par_iter()
            .enumerate()
            .map(move |(i, value)| { // Add move
                let seed = coinbase_seed(&mining_seed, height, i as u64); // Use local variable
                let owner_pk = wots::keygen(&seed);
                let address = compute_address(&owner_pk);
                let salt = coinbase_salt(&mining_seed, height, i as u64); // Use local variable
                CoinbaseOutput { address, value, salt }
            })
            .collect()
    }

    fn log_coinbase(&self, height: u64, total_fees: u64) {
        let reward = block_reward(height);
        let total_value = reward + total_fees;
        let denominations = decompose_value(total_value);
        let log_path = self.data_dir.join("coinbase_seeds.jsonl");

        let mining_seed = self.mining_seed; // Extract seed here

        let entries: Vec<String> = denominations.into_par_iter()
            .enumerate()
            .map(move |(i, value)| { // Add move
                let seed = coinbase_seed(&mining_seed, height, i as u64);
                let owner_pk = wots::keygen(&seed);
                let address = compute_address(&owner_pk);
                let salt = coinbase_salt(&mining_seed, height, i as u64);
                let coin_id = compute_coin_id(&address, value, &salt);
                format!(
                    r#"{{"height":{},"index":{},"seed":"{}","coin":"{}","value":{},"salt":"{}"}}"#,
                    height, i,
                    hex::encode(seed),
                    hex::encode(coin_id),
                    value,
                    hex::encode(salt)
                )
            })
            .collect();

        if let Ok(mut file) = std::fs::OpenOptions::new()
            .create(true).append(true).open(&log_path)
        {
            use std::io::Write;
            for entry in entries {
                let _ = writeln!(file, "{}", entry);
            }
        }
    }

    /// Prepare a batch template and spawn a non-blocking background mining task.
    /// Returns immediately — the result arrives via mined_batch_rx.
    fn spawn_mining_task(&mut self) -> Result<()> {
        if self.sync_in_progress || self.mining_cancel.is_some() {
            return Ok(());
        }
        tracing::info!("Mining batch with {} transactions...", self.mempool.len());

        // Clone only valid transactions. If any became stale since entering the
        // mempool, skip them silently instead of aborting the entire mining attempt.
        let pre_mine_height = self.state.height;
        let pre_mine_midstate = self.state.midstate;
        let mut candidate_state = self.state.clone();
        let mut total_fees: u64 = 0;
        let mut transactions = Vec::new();
        for tx in self.mempool.transactions().iter().take(MAX_BATCH_SIZE) {
            match apply_transaction(&mut candidate_state, tx) {
                Ok(_) => {
                    total_fees += tx.fee();
                    transactions.push(tx.clone());
                }
                Err(e) => {
                    tracing::debug!("Skipping stale mempool tx during mining: {}", e);
                }
            }
        }

        let coinbase = self.generate_coinbase(pre_mine_height, total_fees);
        for cb in &coinbase {
            let coin_id = cb.coin_id();
            candidate_state.coins.insert(coin_id);
            candidate_state.midstate = hash_concat(&candidate_state.midstate, &coin_id);
        }

        let midstate = candidate_state.midstate;
        let target = self.state.target;

        let current_time = state::current_timestamp();
        let block_timestamp = current_time.max(self.state.timestamp + 1);

        let mut template = Batch {
            prev_midstate: pre_mine_midstate,
            transactions,
            extension: Extension { nonce: 0, final_hash: [0; 32], checkpoints: vec![] },
            coinbase,
            timestamp: block_timestamp,
            target: self.state.target,
        };

        // Spawn background mining with cancellation token
        let cancel = Arc::new(AtomicBool::new(false));
        self.mining_cancel = Some(cancel.clone());
        let tx = self.mined_batch_tx.clone();

        tokio::task::spawn_blocking(move || {
            if let Some(extension) = mine_extension(midstate, target, cancel) {
                template.extension = extension;
                let _ = tx.send(template);
            }
            // If cancelled or channel closed, silently drop — the main loop already moved on
        });

        Ok(())
    }

    /// Process a successfully mined batch received from the background task.
    async fn handle_mined_batch(&mut self, batch: Batch) -> Result<()> {
        // If state advanced while we were mining, this batch is stale.
        // Don't clear mining_cancel — a new task may already be running.
        if self.state.midstate != batch.prev_midstate {
            tracing::warn!("State advanced during mining. Discarding stale mined block.");
            return Ok(());
        }

        self.mining_cancel = None; // This batch is current — task is done

        let pre_mine_height = self.state.height;

        self.recent_headers.push_back(self.state.timestamp);
        if self.recent_headers.len() > DIFFICULTY_LOOKBACK as usize {
            self.recent_headers.pop_front();
        }

        match apply_batch(&mut self.state, &batch, self.recent_headers.make_contiguous()) {
            Ok(_) => {
                self.storage.save_batch(pre_mine_height, &batch)?;
                self.storage.save_state(&self.state)?;
                self.state.target = adjust_difficulty(&self.state);
                self.metrics.inc_batches_mined();
                self.network.broadcast(Message::Batch(batch.clone()));

                let total_fees: u64 = batch.transactions.iter().map(|tx| tx.fee()).sum();
                self.log_coinbase(pre_mine_height, total_fees);

                let coinbase_value: u64 = batch.coinbase.iter().map(|cb| cb.value).sum();
                tracing::info!(
                    "Mined batch! height={} coinbase_value={} outputs={} target={}",
                    self.state.height,
                    coinbase_value,
                    batch.coinbase.len(),
                    hex::encode(self.state.target)
                );

                self.mempool.prune_invalid(&self.state);
            }
            Err(e) => {
                tracing::error!("Failed to apply our own mined batch: {}", e);
            }
        }

        Ok(())
    }

    /// Synchronous test wrapper — spawns mining then blocks until it finishes.
    /// Preserves identical behavior for all existing tests.
    #[cfg(test)]
    pub async fn try_mine(&mut self) -> Result<()> {
        self.spawn_mining_task()?;
        if self.mining_cancel.is_none() {
            return Ok(()); // Not spawned (e.g. sync_in_progress)
        }
        if let Some(batch) = self.mined_batch_rx.recv().await {
            self.handle_mined_batch(batch).await?;
        }
        Ok(())
    }
}

// ── Keypair persistence ─────────────────────────────────────────────────────

fn load_keypair(data_dir: &PathBuf) -> Option<Keypair> {
    let path = data_dir.join("peer_key");
    let mut bytes = std::fs::read(&path).ok()?;
    let ed_kp = libp2p::identity::ed25519::Keypair::try_from_bytes(&mut bytes).ok()?;
    Some(Keypair::from(ed_kp))
}

fn save_keypair(data_dir: &PathBuf, keypair: &Keypair) {
    let path = data_dir.join("peer_key");
    if let Ok(ed_kp) = keypair.clone().try_into_ed25519() {
        let _ = std::fs::write(&path, ed_kp.to_bytes());
    }
}


#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;
    use crate::core::extension::create_extension;
    use crate::core::mss;
    use crate::core::types::hash;
    
    // Helper to create a bare-bones node for testing internal logic
    pub(crate) async fn create_test_node(dir: &std::path::Path) -> Node {
        let _keypair = libp2p::identity::Keypair::generate_ed25519();
        // Bind to port 0 to let OS assign a random available port
        let listen: Multiaddr = "/ip4/127.0.0.1/tcp/0".parse().unwrap();
        // Initialize node (this will create genesis if needed)
        Node::new(dir.to_path_buf(), false, listen, vec![]).await.unwrap()
    }

    #[tokio::test]
    async fn test_find_fork_point_logic() {
        // 1. Setup
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        // 2. Build a local chain: Genesis (0) -> B1 -> B2
        // Note: Node::new already created Genesis at height 0.
        let genesis_batch = node.storage.load_batch(0).unwrap().unwrap();
        let midstate_0 = genesis_batch.extension.final_hash;

        // Create Batch 1
        let ext1 = create_extension(midstate_0, 100);
        let midstate_1 = ext1.final_hash;
        let batch1 = Batch {
            prev_midstate: midstate_0,
            transactions: vec![],
            extension: ext1,
            coinbase: vec![],
            timestamp: 1000,
            target: node.state.target,
        };
        node.storage.save_batch(1, &batch1).unwrap();

        // Create Batch 2
        let ext2 = create_extension(midstate_1, 200);
        let midstate_2 = ext2.final_hash;
        let batch2 = Batch {
            prev_midstate: midstate_1,
            transactions: vec![],
            extension: ext2,
            coinbase: vec![],
            timestamp: 1010,
            target: node.state.target,
        };
        node.storage.save_batch(2, &batch2).unwrap();

        // Manually update node state to reflect tip is at height 2
        node.state.height = 2;
        node.state.midstate = midstate_2;

        // ---------------------------------------------------------
        // Case A: Linear Extension (No Fork)
        // ---------------------------------------------------------
        // Remote sends Batch 3, which builds on Batch 2.
        let ext3 = create_extension(midstate_2, 300);
        let batch3 = Batch {
            prev_midstate: midstate_2,
            transactions: vec![],
            extension: ext3,
            coinbase: vec![],
            timestamp: 1020,
            target: node.state.target,
        };

        // We ask: "Where does this batch attach?"
        // Since batch3.prev_midstate == node.midstate (midstate_2),
        // find_fork_point should identify it attaches at height 3.
        let fork_h = node.find_fork_point(&[batch3.clone()], 3).unwrap();
        assert_eq!(fork_h, 3, "Linear extension should attach at height 3");

        // ---------------------------------------------------------
        // Case B: Deep Fork
        // ---------------------------------------------------------
        // Remote sends Batch 2', which builds on Batch 1 (forks off before Batch 2).
        // prev_midstate = midstate_1.
        let ext2_prime = create_extension(midstate_1, 999); // Different nonce -> different hash
        let batch2_prime = Batch {
            prev_midstate: midstate_1,
            transactions: vec![],
            extension: ext2_prime,
            coinbase: vec![],
            timestamp: 1011,
            target: node.state.target,
        };

        // We ask: "Where does this batch attach?"
        // It connects to Batch 1 (height 1).
        // So the fork point (the first new block) is at height 2.
        let fork_h = node.find_fork_point(&[batch2_prime], 2).unwrap();
        assert_eq!(fork_h, 2, "Deep fork should be detected at height 2");

        // ---------------------------------------------------------
        // Case C: Genesis Fork
        // ---------------------------------------------------------
        // Remote sends a batch that builds on Genesis (height 0), replacing Batch 1.
        let ext1_prime = create_extension(midstate_0, 555);
        let batch1_prime = Batch {
            prev_midstate: midstate_0,
            transactions: vec![],
            extension: ext1_prime,
            coinbase: vec![],
            timestamp: 1001,
            target: node.state.target,
        };

        // It connects to Genesis (height 0).
        // Fork point is height 1.
        let fork_h = node.find_fork_point(&[batch1_prime], 1).unwrap();
        assert_eq!(fork_h, 1, "Genesis fork should be detected at height 1");
    }
    
    #[test]
    fn scan_txs_for_mss_index_finds_max() {
        // Create an MSS keypair and sign a few messages
        let seed = hash(b"test mss scan seed");
        let mut keypair = mss::keygen(&seed, 4).unwrap();
        let master_pk = keypair.public_key();

        // Sign 5 messages (uses leaves 0-4)
        let mut txs = Vec::new();
        for i in 0..5u8 {
            let msg = hash(&[i]);
            let sig = keypair.sign(&msg).unwrap();
            let sig_bytes = sig.to_bytes();

            // Build a minimal Reveal tx with this MSS signature
            let tx = Transaction::Reveal {
                inputs: vec![InputReveal {
                    owner_pk: master_pk,
                    value: 1,
                    salt: [i; 32],
                }],
                signatures: vec![sig_bytes],
                outputs: vec![OutputData {
                    address: [0xAA; 32],
                    value: 1,
                    salt: [i; 32],
                }],
                salt: [0; 32],
            };
            txs.push(tx);
        }

        // scan should find max index = 5 (leaf 4 used, so next = 5)
        let max_idx = scan_txs_for_mss_index(&txs, &master_pk);
        assert_eq!(max_idx, 5);
    }

    #[test]
    fn scan_txs_for_mss_index_ignores_other_keys() {
        let seed1 = hash(b"key1");
        let seed2 = hash(b"key2");
        let mut kp1 = mss::keygen(&seed1, 4).unwrap();
        let kp2 = mss::keygen(&seed2, 4).unwrap();

        let msg = hash(b"msg");
        let sig = kp1.sign(&msg).unwrap();

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal {
                owner_pk: kp1.public_key(),
                value: 1,
                salt: [0; 32],
            }],
            signatures: vec![sig.to_bytes()],
            outputs: vec![OutputData {
                address: [0xAA; 32],
                value: 1,
                salt: [0; 32],
            }],
            salt: [0; 32],
        };

        // Scanning for kp2's key should find nothing
        assert_eq!(scan_txs_for_mss_index(&[tx.clone()], &kp2.public_key()), 0);
        // Scanning for kp1's key should find index 1
        assert_eq!(scan_txs_for_mss_index(&[tx], &kp1.public_key()), 1);
    }

    #[test]
    fn scan_txs_mss_recovery_simulation() {
        // Simulates: use indices 0-4, then "restore backup" to 0,
        // scan should report 5 so wallet can jump ahead
        let seed = hash(b"recovery sim");
        let mut keypair = mss::keygen(&seed, 4).unwrap();
        let master_pk = keypair.public_key();

        let mut txs = Vec::new();
        for i in 0..5u8 {
            let msg = hash(&[i]);
            let sig = keypair.sign(&msg).unwrap();
            txs.push(Transaction::Reveal {
                inputs: vec![InputReveal {
                    owner_pk: master_pk,
                    value: 1,
                    salt: [i; 32],
                }],
                signatures: vec![sig.to_bytes()],
                outputs: vec![OutputData {
                    address: [0xBB; 32],
                    value: 1,
                    salt: [i; 32],
                }],
                salt: [0; 32],
            });
        }

        let chain_max = scan_txs_for_mss_index(&txs, &master_pk);
        assert_eq!(chain_max, 5, "should find highest used index + 1");

        // Simulate backup restore: keypair at index 0
        let mut restored = mss::keygen(&seed, 4).unwrap();
        assert_eq!(restored.next_leaf, 0);

        // Apply recovery logic (same as wallet sync)
        const SAFETY_MARGIN: u64 = 20;
        if chain_max >= restored.next_leaf {
            restored.set_next_leaf(chain_max + SAFETY_MARGIN);
        }
        assert_eq!(restored.next_leaf, 25, "should be 5 + 20 safety margin");
    }

    #[test]
    fn scan_txs_mss_mempool_race() {
        // Simulate: tx with index 10 in mempool (unmined)
        let seed = hash(b"mempool race");
        let mut keypair = mss::keygen(&seed, 5).unwrap(); // height 5 = 32 leaves

        // Advance to leaf 10 by signing 10 messages
        for i in 0..10u8 {
            keypair.sign(&hash(&[i])).unwrap();
        }

        // Sign one more (leaf index 10) — this is the "mempool tx"
        let msg = hash(b"mempool tx");
        let sig = keypair.sign(&msg).unwrap();
        assert_eq!(sig.leaf_index, 10);

        let mempool_tx = Transaction::Reveal {
            inputs: vec![InputReveal {
                owner_pk: keypair.public_key(),
                value: 1,
                salt: [0; 32],
            }],
            signatures: vec![sig.to_bytes()],
            outputs: vec![OutputData {
                address: [0xCC; 32],
                value: 1,
                salt: [0; 32],
            }],
            salt: [0; 32],
        };

        let mempool_max = scan_txs_for_mss_index(&[mempool_tx], &keypair.public_key());
        assert_eq!(mempool_max, 11, "should account for leaf 10 → next = 11");

        // Simulate restore from backup at index 0
        let mut restored = mss::keygen(&seed, 5).unwrap();

        const SAFETY_MARGIN: u64 = 20;
        let remote_idx = mempool_max; // in real code: max(chain_max, mempool_max)
        if remote_idx >= restored.next_leaf {
            restored.set_next_leaf(remote_idx + SAFETY_MARGIN);
        }
        assert_eq!(restored.next_leaf, 31, "should be 11 + 20 safety margin");
    }
    
    #[test]
    fn scan_txs_skips_wots_signatures() {
        // WOTS sigs are exactly 576 bytes — scanner should ignore them
        let seed = hash(b"wots not mss");
        let pk = wots::keygen(&seed);
        let msg = hash(b"test");
        let sig = wots::sign(&seed, &msg);
        let sig_bytes = wots::sig_to_bytes(&sig);
        assert_eq!(sig_bytes.len(), wots::SIG_SIZE);

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal {
                owner_pk: pk,
                value: 1,
                salt: [0; 32],
            }],
            signatures: vec![sig_bytes],
            outputs: vec![OutputData {
                address: [0xAA; 32],
                value: 1,
                salt: [0; 32],
            }],
            salt: [0; 32],
        };

        // Should return 0 — no MSS signatures found
        assert_eq!(scan_txs_for_mss_index(&[tx], &pk), 0);
    }

    #[test]
    fn scan_txs_empty_returns_zero() {
        let pk = hash(b"any key");
        assert_eq!(scan_txs_for_mss_index(&[], &pk), 0);
    }

    #[test]
    fn mss_recovery_does_not_roll_back() {
        // If local index is ahead of remote, should NOT decrease
        let seed = hash(b"no rollback");
        let mut keypair = mss::keygen(&seed, 4).unwrap();

        // Advance local to leaf 10
        for i in 0..10u8 {
            keypair.sign(&hash(&[i])).unwrap();
        }
        assert_eq!(keypair.next_leaf, 10);

        // Remote only knows about 3 usages
        let remote_idx: u64 = 3;

        // Recovery logic should NOT roll back
        const SAFETY_MARGIN: u64 = 20;
        if remote_idx >= keypair.next_leaf {
            keypair.set_next_leaf(remote_idx + SAFETY_MARGIN);
        }
        // Should still be 10, not 23
        assert_eq!(keypair.next_leaf, 10);
    }

    #[test]
    fn scan_txs_corrupt_mss_sig_no_panic() {
        // Garbage bytes longer than WOTS SIG_SIZE should not crash
        let pk = hash(b"corrupt test");
        let garbage = vec![0xFFu8; wots::SIG_SIZE + 100]; // longer than WOTS → tries MSS parse

        let tx = Transaction::Reveal {
            inputs: vec![InputReveal {
                owner_pk: pk,
                value: 1,
                salt: [0; 32],
            }],
            signatures: vec![garbage],
            outputs: vec![OutputData {
                address: [0xAA; 32],
                value: 1,
                salt: [0; 32],
            }],
            salt: [0; 32],
        };

        // Should not panic, just return 0
        assert_eq!(scan_txs_for_mss_index(&[tx], &pk), 0);
    }
}

// ── Complex Integration Tests ───────────────────────────────────────────────
#[cfg(test)]
mod complex_tests {
    use super::*;
    use tempfile::tempdir;
    use crate::core::types::hash;

    // --- Test Helpers ---

    /// Creates a node instance isolated in a temp directory.
    /// 
    /// Note: Node::new() automatically creates and applies the genesis block (Height 0).
    /// Therefore, any node returned by this function is sitting at Height 1, ready 
    /// to mine or receive Block 1.
    async fn create_test_node(dir: &std::path::Path) -> Node {
        let _keypair = libp2p::identity::Keypair::generate_ed25519();
        let listen: Multiaddr = "/ip4/127.0.0.1/tcp/0".parse().unwrap();
        
        // Initialize node (creates Genesis internally)
        Node::new(dir.to_path_buf(), false, listen, vec![]).await.unwrap()
    }

    /// specific helper to manually construct a valid batch structure.
    /// 
    /// This bypasses the main mining loop but performs all necessary cryptographic 
    /// operations to create a valid block:
    /// 1. Simulates transaction application to calculate the correct `midstate`.
    /// 2. Generates the correct deterministic Coinbase outputs.
    /// 3. Finds a valid Proof-of-Work nonce for the *real* target of the previous state.
    fn make_valid_batch(prev_state: &State, timestamp_offset: u64, transactions: Vec<Transaction>) -> Batch {
        let timestamp = prev_state.timestamp + timestamp_offset;
        let mut midstate_after_txs = prev_state.midstate;
        let mut tx_fees = 0;

        // 1. Simulate applying transactions to get the post-tx midstate
        for tx in &transactions {
            tx_fees += tx.fee();
            if let Transaction::Commit { commitment, .. } = tx {
                midstate_after_txs = hash_concat(&midstate_after_txs, commitment);
            } else if let Transaction::Reveal { inputs, outputs, salt, .. } = tx {
                let mut hasher = blake3::Hasher::new();
                for i in inputs { hasher.update(&i.coin_id()); }
                for o in outputs { hasher.update(&o.coin_id()); }
                hasher.update(salt);
                let tx_hash = *hasher.finalize().as_bytes();
                midstate_after_txs = hash_concat(&midstate_after_txs, &tx_hash);
            }
        }

        // 2. Generate Coinbase outputs (deterministic based on previous midstate/height)
        let reward = crate::core::block_reward(prev_state.height);
        let total_value = reward + tx_fees;
        let mining_seed = prev_state.midstate; 
        
        let denominations = crate::core::types::decompose_value(total_value);
        let coinbase: Vec<CoinbaseOutput> = denominations.iter().enumerate().map(|(i, &val)| {
             let seed = coinbase_seed(&mining_seed, prev_state.height, i as u64);
             let pk = wots::keygen(&seed);
             let addr = compute_address(&pk);
             let salt = coinbase_salt(&mining_seed, prev_state.height, i as u64);
             CoinbaseOutput { address: addr, value: val, salt }
        }).collect();

        // 3. Update midstate with coinbase
        for cb in &coinbase {
            midstate_after_txs = hash_concat(&midstate_after_txs, &cb.coin_id());
        }

        // 4. Mine a valid nonce.
        let target = prev_state.target;
        let mut nonce = 0u64;
        let extension = loop {
            let ext = create_extension(midstate_after_txs, nonce);
            if ext.final_hash < target {
                break ext;
            }
            nonce += 1;
        };

        Batch {
            prev_midstate: prev_state.midstate,
            transactions,
            extension,
            coinbase,
            timestamp,
            target,
        }
    }

    // --- Tests ---

    /// Verifies that the node stops mining activities when a sync is triggered.
    /// This prevents wasting resources extending a chain that might be obsolete.
    #[tokio::test]
    async fn mining_pauses_during_sync() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;
        
        // Node starts at Height 1 (Genesis applied)
        assert_eq!(node.state.height, 1);

        // 1. Verify mining works normally
        // The node is in "mining mode" but we call try_mine() manually to step through it.
        node.try_mine().await.expect("Mining should succeed");
        assert_eq!(node.state.height, 2);

        // 2. Set sync flag manually (simulating a "GetBatches" request sent to a peer)
        node.sync_in_progress = true;

        // 3. Try mine again. It should return Ok() immediately without doing work.
        node.try_mine().await.expect("Should return Ok implicitly");
        
        // 4. Assert height did NOT increase
        assert_eq!(node.state.height, 2, "Mining should be paused during sync");

        // 5. Unset flag (simulating sync completion) and verify mining resumes
        node.sync_in_progress = false;
        node.try_mine().await.expect("Mining should succeed");
        assert_eq!(node.state.height, 3, "Mining should resume after sync");
    }

    /// Verifies the critical safety mechanism of Reorgs:
    /// When switching to a longer chain, transactions in the abandoned blocks 
    /// must be returned to the mempool so they are not lost.
    #[tokio::test]
    async fn reorg_restores_mempool_transactions() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;
        // Start at H=1

        // --- Chain A: H=1 -> B1(H=2) -> B2(H=3) ---
        // This is the "local" chain the node initially follows.
        
        // Block 1 (Height 1->2)
        let b1 = make_valid_batch(&node.state, 10, vec![]);
        node.handle_new_batch(b1.clone(), None).await.unwrap();
        assert_eq!(node.state.height, 2);
        
        // Create a unique transaction that will be mined into Block 2 of Chain A.
        let commit_hash = hash(b"tx_on_chain_a");
        let mut nonce = 0u64;
        loop {
            let h = hash_concat(&commit_hash, &nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            nonce += 1;
        }
        let valid_tx = Transaction::Commit { commitment: commit_hash, spam_nonce: nonce };

        // Block 2 (Height 2->3) contains the transaction
        let b2 = make_valid_batch(&node.state, 10, vec![valid_tx.clone()]);
        node.handle_new_batch(b2.clone(), None).await.unwrap();

        assert_eq!(node.state.height, 3);
        assert!(node.state.commitments.contains(&commit_hash), "Tx confirmed in Chain A");
        assert_eq!(node.mempool.len(), 0, "Mempool empty after mining");

        // --- Chain B: Fork at H=2 ---
        // Chain B: H=1 -> B1(H=2) -> B2'(H=3) -> B3'(H=4)
        // This is the "competitor" chain. It is longer, so it should trigger a reorg.
        // It forks *after* B1.
        
        // We need to reconstruct the state at H=1 (Genesis) to build the fork.
        let mut state_at_2 = State::genesis().0; 
        
        // REASONING: We cannot simply use State::genesis() because we need to load 
        // the *exact* genesis batch saved by the node to ensure midstates align.
        let genesis_batch = node.storage.load_batch(0).unwrap().unwrap();
        apply_batch(&mut state_at_2, &genesis_batch, &[]).unwrap(); // H=1
        state_at_2.target = adjust_difficulty(&state_at_2);
        
        // Apply B1 (shared history)
       let ts_at_1 = vec![state_at_2.timestamp];
        apply_batch(&mut state_at_2, &b1, &ts_at_1).unwrap(); // H = 2
        state_at_2.target = adjust_difficulty(&state_at_2);

        // B2' (Alternative block at H=3). Empty, does NOT have the transaction.
        let b2_prime = make_valid_batch(&state_at_2, 20, vec![]); 
        let mut state_at_3_prime = state_at_2.clone();
        let ts_at_2 = vec![state_at_2.timestamp];
        apply_batch(&mut state_at_3_prime, &b2_prime, &ts_at_2).unwrap();
        state_at_3_prime.target = adjust_difficulty(&state_at_3_prime);
        
        // B3' (extends B2', making Chain B longer)
        let b3_prime = make_valid_batch(&state_at_3_prime, 10, vec![]);

        // --- Submit Chain B ---
        let peer = PeerId::random();
        
        // We simulate receiving the fork batches.
        // The node detects the fork at H=2, evaluates Chain B, sees it is longer (Len 4 vs 3),
        // and performs the reorg.
        node.handle_batches_response(2, vec![b2_prime, b3_prime], peer).await.unwrap();

        // --- Assertions ---
        assert_eq!(node.state.height, 4, "Node should have switched to longer chain (H=4)");
        
        // The transaction `commit_hash` was in Block 2 (Chain A), but NOT in Block 2' (Chain B).
        // Therefore, it is no longer in the confirmed state.
        assert!(!node.state.commitments.contains(&commit_hash), "Tx from abandoned chain should be gone from state");
        
        // However, the reorg logic should have rescued it from the abandoned block 
        // and placed it back into the mempool.
        assert_eq!(node.mempool.len(), 1, "Mempool should have 1 restored tx");
        let mempool_txs = node.mempool.transactions();
        if let Transaction::Commit { commitment, .. } = mempool_txs[0] {
            assert_eq!(commitment, commit_hash);
        } else {
            panic!("Restored transaction mismatch");
        }
    }

    /// Verifies that the node can ingest a linear sequence of blocks during sync.
    #[tokio::test]
    async fn sync_ingests_batches_and_completes() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;
        
        // Snapshot the start state (H=1) so we can reset the node later to simulate being behind.
        let start_state = node.state.clone();

        // Generate 50 blocks extending H=1.
        // We use the node's internal state to generate them validly, then revert.
        // IMPORTANT: must call adjust_difficulty after each batch, exactly as
        // process_linear_extension does, so the target carried inside each batch
        // matches what the replaying node will expect. recent_headers are still
        // needed for timestamp validation (MTP) in apply_batch.
        let mut batches = Vec::new();
        let mut recent_headers: Vec<u64> = vec![node.state.timestamp];
        let window_size = DIFFICULTY_LOOKBACK as usize;
        for _ in 0..50 {
            let b = make_valid_batch(&node.state, 10, vec![]);
            apply_batch(&mut node.state, &b, &recent_headers).unwrap();
            recent_headers.push(node.state.timestamp);
            if recent_headers.len() > window_size { recent_headers.remove(0); }
            node.state.target = adjust_difficulty(&node.state);
            node.storage.save_batch(node.state.height - 1, &b).unwrap();
            batches.push(b);
        }
        
        assert_eq!(node.state.height, 51);

        // Reset node to H=1 (the "behind" node).
        node.state = start_state;
        node.recent_headers = VecDeque::from(vec![node.state.timestamp]);
        
        // Simulate sync state
        node.sync_in_progress = true;
        node.sync_requested_up_to = 51;
        
        // Feed the 50 batches we generated.
        let peer = PeerId::random();
        node.handle_batches_response(1, batches, peer).await.unwrap();

        // Verify the node caught up.
        assert_eq!(node.state.height, 51);
        assert_eq!(node.sync_in_progress, false, "Sync should complete automatically");
    }

    // ── Crash Recovery ──────────────────────────────────────────────────

    /// Verify that a node can be killed and restarted from the same data dir
    /// and resume at the correct height with the correct state.
    #[tokio::test]
    async fn crash_recovery_preserves_state() {
        let dir = tempdir().unwrap();

        // 1. Create a node, mine 5 blocks, save state
        let height_before;
        let midstate_before;
        let coins_before;
        {
            let mut node = create_test_node(dir.path()).await;
            assert_eq!(node.state.height, 1);

            for _ in 0..5 {
                node.try_mine().await.unwrap();
            }
            assert_eq!(node.state.height, 6);

            // Save state (normally happens on interval)
            node.storage.save_state(&node.state).unwrap();

            height_before = node.state.height;
            midstate_before = node.state.midstate;
            coins_before = node.state.coins.len();
            // node is dropped here — simulates crash
        }

        // 2. Recreate node from same data dir
        let node2 = create_test_node(dir.path()).await;

        // 3. Verify state matches
        assert_eq!(node2.state.height, height_before, "height must survive restart");
        assert_eq!(node2.state.midstate, midstate_before, "midstate must survive restart");
        assert_eq!(node2.state.coins.len(), coins_before, "coin set must survive restart");
    }

    /// Verify that after crash recovery, mining can resume and produce valid blocks.
    #[tokio::test]
    async fn crash_recovery_can_resume_mining() {
        let dir = tempdir().unwrap();

        {
            let mut node = create_test_node(dir.path()).await;
            for _ in 0..3 {
                node.try_mine().await.unwrap();
            }
            node.storage.save_state(&node.state).unwrap();
        }

        let mut node2 = create_test_node(dir.path()).await;
        assert_eq!(node2.state.height, 4);

        // Mining should work after restart
        node2.is_mining = true;
        node2.try_mine().await.unwrap();
        assert_eq!(node2.state.height, 5, "mining must resume after crash recovery");
    }

    // ── Sync From Scratch ───────────────────────────────────────────────

    /// A fresh node with no history receives the full chain and catches up.
    #[tokio::test]
    async fn fresh_node_syncs_full_chain() {
        let dir_miner = tempdir().unwrap();
        let dir_fresh = tempdir().unwrap();

        // 1. Build a chain of 20 blocks on the "miner" node
        let mut miner = create_test_node(dir_miner.path()).await;
        for _ in 0..20 {
            miner.try_mine().await.unwrap();
        }
        assert_eq!(miner.state.height, 21);

        // 2. Collect all batches from storage
        let mut batches = Vec::new();
        for h in 1..21 {
            let batch = miner.storage.load_batch(h).unwrap().unwrap();
            batches.push(batch);
        }

        // 3. Create a fresh node
        let mut fresh = create_test_node(dir_fresh.path()).await;
        assert_eq!(fresh.state.height, 1);

        // 4. Feed batches as if received from a peer
        fresh.sync_in_progress = true;
        fresh.sync_requested_up_to = 21;
        let peer = PeerId::random();
        fresh.handle_batches_response(1, batches, peer).await.unwrap();

        // 5. Verify state matches
        assert_eq!(fresh.state.height, miner.state.height);
        assert_eq!(fresh.state.midstate, miner.state.midstate);
        assert_eq!(fresh.state.coins.len(), miner.state.coins.len());
        assert!(!fresh.sync_in_progress);
    }

    /// A node that is partially synced receives remaining blocks.
    #[tokio::test]
    async fn partial_sync_completes() {
        let dir_miner = tempdir().unwrap();
        let dir_behind = tempdir().unwrap();

        let mut miner = create_test_node(dir_miner.path()).await;
        for _ in 0..15 {
            miner.try_mine().await.unwrap();
        }

        // Collect batches 1..15
        let mut all_batches = Vec::new();
        for h in 1..16 {
            all_batches.push(miner.storage.load_batch(h).unwrap().unwrap());
        }

        // Fresh node syncs first 5 blocks
        let mut behind = create_test_node(dir_behind.path()).await;
        behind.sync_in_progress = true;
        behind.sync_requested_up_to = 16;
        let peer = PeerId::random();
        behind.handle_batches_response(1, all_batches[..5].to_vec(), peer).await.unwrap();
        assert_eq!(behind.state.height, 6);

        // Now feed remaining blocks
        behind.handle_batches_response(6, all_batches[5..].to_vec(), peer).await.unwrap();
        assert_eq!(behind.state.height, 16);
        assert_eq!(behind.state.midstate, miner.state.midstate);
    }

    // ── Full Commit-Reveal Transaction Cycle ────────────────────────────

    /// Exercises the complete transaction lifecycle through the node:
    /// mine → get coins → commit → mine commit → reveal → mine reveal → verify
    #[tokio::test]
    async fn full_commit_reveal_cycle() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;
        let mining_seed = node.mining_seed;

        // 1. Mine a block to get coinbase coins
        let pre_mine_height = node.state.height; // = 1
        node.try_mine().await.unwrap();
        assert_eq!(node.state.height, 2);

        // 2. Derive the coinbase coin we just mined (index 0 of that block)
        let cb_seed = coinbase_seed(&mining_seed, pre_mine_height, 0);
        let cb_owner_pk = wots::keygen(&cb_seed);
        let cb_address = compute_address(&cb_owner_pk);
        let cb_salt = coinbase_salt(&mining_seed, pre_mine_height, 0);
        let cb_value = block_reward(pre_mine_height); // first denomination
        // Actual coinbase uses decompose_value, so first output is the largest power of 2
        let denominations = decompose_value(cb_value);
        let first_denom = denominations[0];
        let cb_coin_id = compute_coin_id(&cb_address, first_denom, &cb_salt);
        assert!(node.state.coins.contains(&cb_coin_id), "coinbase coin must be in UTXO set");

        // 3. Build a transaction spending the coinbase coin
        let recipient_seed: [u8; 32] = hash(b"recipient seed");
        let recipient_pk = wots::keygen(&recipient_seed);
        let recipient_addr = compute_address(&recipient_pk);
        let output_salt: [u8; 32] = hash(b"output salt");

        // Output must be power-of-2 and less than input (fee > 0)
        let send_value = first_denom / 2;
        assert!(send_value > 0 && send_value.is_power_of_two());

        // Change output
        let change_seed: [u8; 32] = hash(b"change seed");
        let change_pk = wots::keygen(&change_seed);
        let change_addr = compute_address(&change_pk);
        let change_salt: [u8; 32] = hash(b"change salt");
        let change_value = first_denom / 4; // fee = first_denom - send - change
        assert!(send_value + change_value < first_denom, "must leave fee");

        let outputs = vec![
            OutputData { address: recipient_addr, value: send_value, salt: output_salt },
            OutputData { address: change_addr, value: change_value, salt: change_salt },
        ];

        let input_coin_ids = vec![cb_coin_id];
        let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();
        let tx_salt: [u8; 32] = hash(b"tx salt");
        let commitment = compute_commitment(&input_coin_ids, &output_coin_ids, &tx_salt);

        // 4. Find a valid spam nonce for commit PoW
        let mut spam_nonce = 0u64;
        loop {
            let h = hash_concat(&commitment, &spam_nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            spam_nonce += 1;
        }

        let commit_tx = Transaction::Commit { commitment, spam_nonce };

        // 5. Mine the commit into a block
        let commit_batch = make_valid_batch(&node.state, 10, vec![commit_tx]);
        node.handle_new_batch(commit_batch, None).await.unwrap();
        assert!(node.state.commitments.contains(&commitment), "commitment must be in state");

        // 6. Build the reveal
        let sig = wots::sign(&cb_seed, &commitment);
        let reveal_tx = Transaction::Reveal {
            inputs: vec![InputReveal {
                owner_pk: cb_owner_pk,
                value: first_denom,
                salt: cb_salt,
            }],
            signatures: vec![wots::sig_to_bytes(&sig)],
            outputs,
            salt: tx_salt,
        };

        // 7. Mine the reveal into a block
        let reveal_batch = make_valid_batch(&node.state, 10, vec![reveal_tx]);
        node.handle_new_batch(reveal_batch, None).await.unwrap();

        // 8. Verify: old coin gone, new coins exist
        assert!(!node.state.coins.contains(&cb_coin_id), "spent coin must be removed");
        let recipient_coin_id = compute_coin_id(&recipient_addr, send_value, &output_salt);
        let change_coin_id = compute_coin_id(&change_addr, change_value, &change_salt);
        assert!(node.state.coins.contains(&recipient_coin_id), "recipient coin must exist");
        assert!(node.state.coins.contains(&change_coin_id), "change coin must exist");
    }

    // ── Wallet Send Flow End-to-End ─────────────────────────────────────

    /// Full wallet lifecycle: create → mine → scan → send → verify
    #[tokio::test]
    async fn wallet_send_flow_end_to_end() {
        use crate::wallet::{Wallet, coinbase_seed, coinbase_salt};

        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;
        let mining_seed = node.mining_seed;

        // 1. Mine a block
        let mine_height = node.state.height;
        node.try_mine().await.unwrap();

        // 2. Create a wallet and import the coinbase coin
        let wallet_path = dir.path().join("test_wallet.dat");
        let mut wallet = Wallet::create(&wallet_path, b"pass").unwrap();

        let denominations = decompose_value(block_reward(mine_height));
        let cb_seed = coinbase_seed(&mining_seed, mine_height, 0);
        let cb_salt = coinbase_salt(&mining_seed, mine_height, 0);
        let coin_id = wallet.import_coin(cb_seed, denominations[0], cb_salt, Some("coinbase".into())).unwrap();
        assert!(node.state.coins.contains(&coin_id));

        // 3. Generate a recipient address in the same wallet (self-send)
        let recv_addr = wallet.generate_key(Some("recv".into())).unwrap();
        let send_value = denominations[0] / 2;
        let send_denoms = decompose_value(send_value);

        // 4. Build outputs via wallet
        let live_coins: Vec<[u8; 32]> = wallet.coins().iter().map(|c| c.coin_id).collect();
        let selected = wallet.select_coins(send_value + 1, &live_coins).unwrap(); // +1 for fee room
        assert_eq!(selected.len(), 1);

        let in_value: u64 = selected.iter()
            .filter_map(|id| wallet.find_coin(id))
            .map(|c| c.value)
            .sum();
        let _change_value = in_value - send_value - 1; // 1 unit fee (must be > 0)
        // change_value may not work if it's not representable, let's adjust
        // Actually fee = in_value - out_value, we need out_value < in_value and all outputs power-of-2
        // Let's keep it simple: send half, no change, the rest is fee
        let (outputs, change_seeds) = wallet.build_outputs(&recv_addr, &send_denoms, 0).unwrap();
        let out_sum: u64 = outputs.iter().map(|o| o.value).sum();
        assert!(in_value > out_sum, "fee must be positive");

        // 5. Build commit
        let (commitment, _salt) = wallet.prepare_commit(&selected, &outputs, change_seeds, false).unwrap();

        // Find spam nonce
        let mut spam_nonce = 0u64;
        loop {
            let h = hash_concat(&commitment, &spam_nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 { break; }
            spam_nonce += 1;
        }

        let commit_tx = Transaction::Commit { commitment, spam_nonce };
        let commit_batch = make_valid_batch(&node.state, 10, vec![commit_tx]);
        node.handle_new_batch(commit_batch, None).await.unwrap();
        assert!(node.state.commitments.contains(&commitment));

        // 6. Build reveal
        let pending = wallet.find_pending(&commitment).unwrap().clone();
        let (input_reveals, signatures) = wallet.sign_reveal(&pending).unwrap();
        let reveal_tx = Transaction::Reveal {
            inputs: input_reveals,
            signatures,
            outputs: pending.outputs.clone(),
            salt: pending.salt,
        };

        let reveal_batch = make_valid_batch(&node.state, 10, vec![reveal_tx]);
        node.handle_new_batch(reveal_batch, None).await.unwrap();

        // 7. Verify old coin spent, new coin(s) exist
        assert!(!node.state.coins.contains(&coin_id), "spent coin removed");
        for out in &pending.outputs {
            assert!(node.state.coins.contains(&out.coin_id()), "output coin must exist in UTXO set");
        }

        // 8. Complete reveal in wallet
        wallet.complete_reveal(&commitment).unwrap();
    }

    // ── Reorg Under Concurrent Mining ───────────────────────────────────

    /// Two miners find blocks at the same height. The node initially follows one,
    /// then switches when the other chain becomes longer.
    #[tokio::test]
    async fn concurrent_miners_reorg() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        // Mine 3 blocks for common history
        for _ in 0..3 {
            node.try_mine().await.unwrap();
        }
        assert_eq!(node.state.height, 4);
        let fork_state = node.state.clone();

        // Chain A: node mines 1 more block (height 5)
        node.try_mine().await.unwrap();
        assert_eq!(node.state.height, 5);

        // Chain B: built offline from fork_state, 3 blocks (heights 5, 6, 7)
        let mut chain_b_state = fork_state.clone();
        let mut chain_b_batches = Vec::new();
        let mut recent_headers: Vec<u64> = vec![chain_b_state.timestamp];

        for i in 0..3 {
            let b = make_valid_batch(&chain_b_state, 10 + i, vec![]);
            apply_batch(&mut chain_b_state, &b, &recent_headers).unwrap();
            recent_headers.push(chain_b_state.timestamp);
            if recent_headers.len() > 11 { recent_headers.remove(0); }
            chain_b_state.target = adjust_difficulty(&chain_b_state);
            chain_b_batches.push(b);
        }
        assert_eq!(chain_b_state.height, 7);

        // Feed Chain B to the node — should trigger reorg
        let peer = PeerId::random();
        node.handle_batches_response(4, chain_b_batches, peer).await.unwrap();

        assert_eq!(node.state.height, 7, "should switch to longer chain");
        assert_eq!(node.state.midstate, chain_b_state.midstate, "must adopt chain B's midstate");
    }
    
    // ── Interrupted Sync / Frankenstein Chain Tests ────────────────────

    /// Build a divergent chain from a fork point, returning the batches
    /// and the headers the peer would serve.
    fn build_divergent_chain(
        fork_state: &State,
        length: usize,
    ) -> (Vec<Batch>, Vec<BatchHeader>) {
        let mut state = fork_state.clone();
        let mut batches = Vec::new();
        let mut headers = Vec::new();
        let mut recent_headers: Vec<u64> = vec![state.timestamp];
        
        for i in 0..length {
            // Use a different timestamp offset so PoW differs from the main chain
            let batch = make_valid_batch(&state, 50 + i as u64, vec![]);
            let mut hdr = batch.header();
            hdr.height = state.height;
            headers.push(hdr);
            apply_batch(&mut state, &batch, &recent_headers).unwrap();
            recent_headers.push(state.timestamp);
            if recent_headers.len() > 11 { recent_headers.remove(0); }
            state.target = adjust_difficulty(&state);
            batches.push(batch);
        }
        (batches, headers)
    }

    /// Core invariant: after an aborted sync, on-disk headers must still
    /// form a valid chain (no Frankenstein mixing of two chains).
    ///
    /// Before the fix, handle_sync_batches called save_batch() for each
    /// incoming peer batch *during* sync.  If the session was then aborted
    /// (timeout, disconnect, new session), some heights on disk held the
    /// peer's blocks while others still held the original chain's blocks.
    /// A subsequent load_headers() would produce a chain with broken
    /// prev_midstate linkage — the "Frankenstein chain" bug.
    #[tokio::test]
    async fn aborted_sync_does_not_corrupt_disk() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        // 1. Mine 10 blocks on the "main" chain so we have something on disk
        for _ in 0..10 {
            node.try_mine().await.unwrap();
        }
        assert_eq!(node.state.height, 11);

        // Snapshot — this is what disk should look like after abort
        let pre_sync_state = node.state.clone();

        // 2. Build a divergent chain that forks at height 5
        //    (longer than ours so the node would want to adopt it)
        let fork_state = node.rebuild_state_at_height(5).unwrap();
        let (alt_batches, alt_headers) = build_divergent_chain(&fork_state, 15);

        // We need the full header chain from genesis for the sync session.
        // Heights 0..5 are shared, 5..20 are the alt chain.
        let mut full_headers = node.storage.batches.load_headers(0, 5).unwrap();
        full_headers.extend(alt_headers);

        // 3. Manually construct a sync session in the Batches phase
        let peer = PeerId::random();
        let peer_height = 5 + alt_batches.len() as u64; // = 20
        node.sync_in_progress = true;
        node.sync_session = Some(SyncSession {
            peer,
            peer_height,
            peer_depth: peer_height * 1_000_000,
            phase: SyncPhase::Batches {
                headers: full_headers,
                fork_height: 5,
                candidate_state: fork_state.clone(),
                cursor: 5,
                new_history: Vec::new(),
            },
            started_at: std::time::Instant::now(),
        });

        // 4. Feed only half the alt batches (simulating partial download)
        let half = alt_batches.len() / 2;
        node.handle_sync_batches(peer, alt_batches[..half].to_vec())
            .await
            .unwrap();

        // Session should still be in progress (waiting for more batches)
        assert!(node.sync_session.is_some(), "Session should still be active");

        // 5. ABORT the sync (simulates timeout or peer disconnect)
        node.abort_sync_session("simulated timeout");

        // 6. THE KEY ASSERTION: on-disk headers must still be a valid chain.
        //    Load all headers from disk and verify linkage.
        let disk_headers = node.storage.batches.load_headers(0, pre_sync_state.height).unwrap();
        assert!(
            !disk_headers.is_empty(),
            "Should have headers on disk"
        );
        assert!(
            Syncer::verify_header_chain(&disk_headers).is_ok(),
            "On-disk header chain must have valid linkage after aborted sync. \
             If this fails, sync wrote peer batches to disk before committing the reorg."
        );

        // 7. The node's in-memory state should be unchanged
        assert_eq!(node.state.height, pre_sync_state.height);
        assert_eq!(node.state.midstate, pre_sync_state.midstate);
    }

    /// Verify that a node which aborted a sync can still serve valid
    /// headers to other peers.  This is the downstream consequence of
    /// the Frankenstein bug: even if the node's own state is fine, any
    /// peer trying to sync FROM it would see broken header linkage and
    /// abort its own sync.
    #[tokio::test]
    async fn headers_served_after_aborted_sync_are_valid() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        // Mine 8 blocks
        for _ in 0..8 {
            node.try_mine().await.unwrap();
        }
        let original_height = node.state.height; // 9

        // Build alt chain forking at height 3, longer than ours
        let fork_state = node.rebuild_state_at_height(3).unwrap();
        let (alt_batches, alt_headers) = build_divergent_chain(&fork_state, 12);

        let mut full_headers = node.storage.batches.load_headers(0, 3).unwrap();
        full_headers.extend(alt_headers);

        // Set up sync session and feed some batches
        let peer = PeerId::random();
        node.sync_in_progress = true;
        node.sync_session = Some(SyncSession {
            peer,
            peer_height: 3 + alt_batches.len() as u64,
            peer_depth: (3 + alt_batches.len() as u64) * 1_000_000,
            phase: SyncPhase::Batches {
                headers: full_headers,
                fork_height: 3,
                candidate_state: fork_state,
                cursor: 3,
                new_history: Vec::new(),
            },
            started_at: std::time::Instant::now(),
        });

        // Feed 4 of 12 alt batches, then abort
        node.handle_sync_batches(peer, alt_batches[..4].to_vec())
            .await
            .unwrap();
        node.abort_sync_session("peer disconnected");

        // Now simulate what happens when another peer asks us for headers.
        // This is exactly what GetHeaders does internally.
        let served_headers = node.storage.batches
            .load_headers(0, original_height)
            .unwrap();

        assert!(
            Syncer::verify_header_chain(&served_headers).is_ok(),
            "Headers served to peers must have valid linkage. \
             Broken linkage here means peers cannot sync from us."
        );
    }

    /// Verify that a COMPLETED sync (not aborted) does persist the new
    /// chain to disk correctly and the headers are valid.
    #[tokio::test]
    async fn completed_sync_persists_valid_chain() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        // Mine 5 blocks
        for _ in 0..5 {
            node.try_mine().await.unwrap();
        }
        assert_eq!(node.state.height, 6);

        // Build alt chain forking at genesis (height 1), longer than ours
        let fork_state = node.rebuild_state_at_height(1).unwrap();
        let (alt_batches, alt_headers) = build_divergent_chain(&fork_state, 10);

        let mut full_headers = node.storage.batches.load_headers(0, 1).unwrap();
        full_headers.extend(alt_headers);

        let peer = PeerId::random();
        let peer_height = 1 + alt_batches.len() as u64; // 11
        node.sync_in_progress = true;
        node.sync_session = Some(SyncSession {
            peer,
            peer_height,
            peer_depth: peer_height * 1_000_000,
            phase: SyncPhase::Batches {
                headers: full_headers,
                fork_height: 1,
                candidate_state: fork_state,
                cursor: 1,
                new_history: Vec::new(),
            },
            started_at: std::time::Instant::now(),
        });

        // Feed ALL batches — sync should complete and adopt the chain
        node.handle_sync_batches(peer, alt_batches.clone())
            .await
            .unwrap();

        assert_eq!(node.state.height, peer_height);
        assert!(!node.sync_in_progress);

        // Verify that the NEW chain on disk is coherent
        let disk_headers = node.storage.batches
            .load_headers(0, node.state.height)
            .unwrap();
        assert_eq!(disk_headers.len(), (node.state.height) as usize);
        assert!(
            Syncer::verify_header_chain(&disk_headers).is_ok(),
            "After a completed sync+reorg, on-disk headers must be a valid chain"
        );
    }

    /// A second sync replacing a first mid-flight must not leave the disk
    /// in a mixed state.  This simulates: start syncing from peer A,
    /// receive some batches, then peer B shows up with a better chain
    /// and we start a new session (dropping the old one).
    #[tokio::test]
    async fn replaced_sync_session_does_not_corrupt_disk() {
        let dir = tempdir().unwrap();
        let mut node = create_test_node(dir.path()).await;

        for _ in 0..8 {
            node.try_mine().await.unwrap();
        }
        let original_state = node.state.clone();

        // Build two different alt chains forking at different points
        let fork_a_state = node.rebuild_state_at_height(3).unwrap();
        let (alt_a_batches, alt_a_headers) = build_divergent_chain(&fork_a_state, 10);

        let mut full_headers_a = node.storage.batches.load_headers(0, 3).unwrap();
        full_headers_a.extend(alt_a_headers);

        // Start sync session with peer A
        let peer_a = PeerId::random();
        node.sync_in_progress = true;
        node.sync_session = Some(SyncSession {
            peer: peer_a,
            peer_height: 13,
            peer_depth: 13_000_000,
            phase: SyncPhase::Batches {
                headers: full_headers_a,
                fork_height: 3,
                candidate_state: fork_a_state,
                cursor: 3,
                new_history: Vec::new(),
            },
            started_at: std::time::Instant::now(),
        });

        // Feed some of A's batches
        node.handle_sync_batches(peer_a, alt_a_batches[..3].to_vec())
            .await
            .unwrap();

        // Now peer B shows up — start_sync_session replaces the session
        let peer_b = PeerId::random();
        node.start_sync_session(peer_b, 20, 20_000_000);

        // Disk should still be coherent (old session's writes should not exist)
        let disk_headers = node.storage.batches
            .load_headers(0, original_state.height)
            .unwrap();
        assert!(
            Syncer::verify_header_chain(&disk_headers).is_ok(),
            "Replacing a sync session mid-flight must not leave corrupted headers on disk"
        );
    }

    // ── Database Lock Retry Test ────────────────────────────────────────

    /// Verify that Storage::open retries when the database lock is held,
    /// rather than failing immediately.
    #[tokio::test]
    async fn storage_open_retries_on_lock() {
        use crate::storage::Storage;

        let dir = tempdir().unwrap();
        let db_path = dir.path().join("db");

        // Open the database (acquires the exclusive lock)
        let _storage1 = Storage::open(&db_path).unwrap();

        // Spawn a thread that will drop storage1 after a short delay,
        // simulating a previous process releasing the lock
        let db_path2 = db_path.clone();
        let handle = std::thread::spawn(move || {
            // Hold the lock for 300ms then drop
            std::thread::sleep(std::time::Duration::from_millis(300));
            drop(_storage1);
            // Now try to open — should succeed after retry
            Storage::open(&db_path2)
        });

        // The spawned thread should eventually acquire the lock
        let result = handle.join().unwrap();
        assert!(result.is_ok(), "Storage::open should succeed after lock is released");
    }

    /// Verify that Storage::open still fails if the lock is permanently held.
    #[test]
    fn storage_open_fails_after_max_retries() {
        use crate::storage::Storage;

        let dir = tempdir().unwrap();
        let db_path = dir.path().join("db");

        // Hold the lock for the duration of the test
        let _storage1 = Storage::open(&db_path).unwrap();

        // Second open should fail after exhausting retries
        let result = Storage::open(&db_path);
        assert!(result.is_err(), "Should fail when lock is permanently held");
    }
    
}




══════════════════════════════════════════════════════════════
 FILE: rpc/handlers.rs
══════════════════════════════════════════════════════════════

use super::types::*;
use crate::core::{compute_commitment, compute_address, hash_concat, wots,
                  block_reward, Transaction, InputReveal, OutputData};
use crate::node::NodeHandle;
use axum::{
    extract::State,
    http::StatusCode,
    response::{IntoResponse, Response},
    Json,
};

type AppState = NodeHandle;

impl IntoResponse for ErrorResponse {
    fn into_response(self) -> Response {
        (StatusCode::BAD_REQUEST, Json(self)).into_response()
    }
}

pub async fn health() -> &'static str {
    "OK"
}
pub async fn get_mss_state(
    State(node): State<AppState>,
    Json(req): Json<GetMssStateRequest>,
) -> Result<Json<GetMssStateResponse>, ErrorResponse> {
    let master_pk = parse_hex32(&req.master_pk, "master_pk")?;
    let state = node.get_state().await;

    // Scan chain history
    let chain_max = node.scan_mss_index(&master_pk, state.height)
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    // Scan mempool
    let (_, mempool_txs) = node.get_mempool_info().await;
    let mempool_max = crate::node::scan_txs_for_mss_index(&mempool_txs, &master_pk);

    let next_index = chain_max.max(mempool_max);

    Ok(Json(GetMssStateResponse { next_index }))
}
pub async fn get_state(State(node): State<AppState>) -> Json<GetStateResponse> {
    let state = node.get_state().await;
    let safe_depth = node.get_safe_depth().await;

    Json(GetStateResponse {
        height: state.height,
        depth: state.depth,
        safe_depth,
        midstate: hex::encode(state.midstate),
        num_coins: state.coins.len(),
        num_commitments: state.commitments.len(),
        target: hex::encode(state.target),
        block_reward: block_reward(state.height),
    })
}

fn parse_hex32(hex_str: &str, label: &str) -> Result<[u8; 32], ErrorResponse> {
    let bytes = hex::decode(hex_str)
        .map_err(|e| ErrorResponse { error: format!("Invalid {} hex: {}", label, e) })?;
    if bytes.len() != 32 {
        return Err(ErrorResponse { error: format!("{} must be 32 bytes", label) });
    }
    Ok(<[u8; 32]>::try_from(bytes).unwrap())
}

pub async fn commit_transaction(
    State(node): State<AppState>,
    Json(req): Json<CommitRequest>,
) -> Result<Json<CommitResponse>, ErrorResponse> {
    if req.coins.is_empty() {
        return Err(ErrorResponse { error: "Must provide at least one coin".into() });
    }
    if req.destinations.is_empty() {
        return Err(ErrorResponse { error: "Must provide at least one destination".into() });
    }

    let input_coins: Vec<[u8; 32]> = req.coins.iter()
        .map(|h| parse_hex32(h, "coin"))
        .collect::<Result<_, _>>()?;

    let destinations: Vec<[u8; 32]> = req.destinations.iter()
        .map(|h| parse_hex32(h, "destination"))
        .collect::<Result<_, _>>()?;

    let salt: [u8; 32] = rand::random();
    let commitment = compute_commitment(&input_coins, &destinations, &salt);

    // Mine PoW nonce for anti-spam
    let spam_nonce = {
        let mut n = 0u64;
        loop {
            let h = hash_concat(&commitment, &n.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 {
                break n;
            }
            n += 1;
        }
    };

    let tx = Transaction::Commit { commitment, spam_nonce };
    
    node.send_transaction(tx)
        .await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(CommitResponse {
        commitment: hex::encode(commitment),
        salt: hex::encode(salt),
        status: "committed".to_string(),
    }))
}

pub async fn send_transaction(
    State(node): State<AppState>,
    Json(req): Json<SendTransactionRequest>,
) -> Result<Json<SendTransactionResponse>, ErrorResponse> {
    if req.inputs.is_empty() {
        return Err(ErrorResponse { error: "Must provide at least one input".into() });
    }
    if req.signatures.len() != req.inputs.len() {
        return Err(ErrorResponse {
            error: "Signature count must match input count".into(),
        });
    }

    let inputs: Vec<InputReveal> = req.inputs.iter().map(|i| {
        Ok(InputReveal {
            owner_pk: parse_hex32(&i.owner_pk, "owner_pk")?,
            value: i.value,
            salt: parse_hex32(&i.salt, "input_salt")?,
        })
    }).collect::<Result<_, ErrorResponse>>()?;

    let mut signatures = Vec::new();
    for sig_hex in &req.signatures {
        let sig_bytes = hex::decode(sig_hex)
            .map_err(|e| ErrorResponse { error: format!("Invalid signature hex: {}", e) })?;
        signatures.push(sig_bytes);
    }

    let outputs: Vec<OutputData> = req.outputs.iter().map(|o| {
        Ok(OutputData {
            address: parse_hex32(&o.address, "address")?,
            value: o.value,
            salt: parse_hex32(&o.salt, "output_salt")?,
        })
    }).collect::<Result<_, ErrorResponse>>()?;

    let salt = parse_hex32(&req.salt, "salt")?;

    let input_coin_ids: Vec<String> = inputs.iter().map(|i| hex::encode(i.coin_id())).collect();
    let output_coin_ids: Vec<String> = outputs.iter().map(|o| hex::encode(o.coin_id())).collect();
    let fee = {
        let in_sum: u64 = inputs.iter().map(|i| i.value).sum();
        let out_sum: u64 = outputs.iter().map(|o| o.value).sum();
        in_sum.saturating_sub(out_sum)
    };

    let tx = Transaction::Reveal { inputs, signatures, outputs, salt };

    node.send_transaction(tx)
        .await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(SendTransactionResponse {
        input_coins: input_coin_ids,
        output_coins: output_coin_ids,
        fee,
        status: "submitted".to_string(),
    }))
}

pub async fn check_coin(
    State(node): State<AppState>,
    Json(req): Json<CheckCoinRequest>,
) -> Result<Json<CheckCoinResponse>, ErrorResponse> {
    let coin = parse_hex32(&req.coin, "coin")?;
    let exists = node.check_coin(coin).await;

    Ok(Json(CheckCoinResponse {
        exists,
        coin: hex::encode(coin),
    }))
}

pub async fn get_mempool(State(node): State<AppState>) -> Json<GetMempoolResponse> {
    let (size, transactions) = node.get_mempool_info().await;

    let tx_info: Vec<_> = transactions
        .iter()
        .map(|tx| match tx {
            Transaction::Commit { commitment, .. } => TransactionInfo  {
                commitment: Some(hex::encode(commitment)),
                input_coins: None,
                output_coins: None,
                fee: None,
            },
            Transaction::Reveal { inputs, outputs, .. } => TransactionInfo {
                commitment: None,
                input_coins: Some(inputs.iter().map(|i| hex::encode(i.coin_id())).collect()),
                output_coins: Some(outputs.iter().map(|o| hex::encode(o.coin_id())).collect()),
                fee: Some(tx.fee()),
            },
        })
        .collect();

    Json(GetMempoolResponse { size, transactions: tx_info })
}
pub async fn scan_addresses(
    State(node): State<AppState>,
    Json(req): Json<ScanRequest>,
) -> Result<Json<ScanResponse>, ErrorResponse> {
    let addresses: Vec<[u8; 32]> = req.addresses.iter()
        .map(|h| parse_hex32(h, "address"))
        .collect::<Result<_, _>>()?;

    let coins = node.scan_addresses(&addresses, req.start_height, req.end_height)
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(ScanResponse {
        coins: coins.into_iter().map(|c| ScanCoin {
            address: hex::encode(c.address),
            value: c.value,
            salt: hex::encode(c.salt),
            coin_id: hex::encode(c.coin_id),
            height: c.height,
        }).collect(),
    }))
}

pub async fn generate_key() -> Json<GenerateKeyResponse> {
    let seed: [u8; 32] = rand::random();
    let owner_pk = wots::keygen(&seed);
    let address = compute_address(&owner_pk);

    Json(GenerateKeyResponse {
        seed: hex::encode(seed),
        address: hex::encode(address),
    })
}

pub async fn get_peers(State(node): State<AppState>) -> Json<GetPeersResponse> {
    let peers = node.get_peers().await;
    Json(GetPeersResponse { peers })
}

// ── CoinJoin Mix Handlers ───────────────────────────────────────────────

pub async fn mix_create(
    State(node): State<AppState>,
    Json(req): Json<MixCreateRequest>,
) -> Result<Json<MixCreateResponse>, ErrorResponse> {
    let mix_id = node.mix_create(req.denomination, req.min_participants).await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(MixCreateResponse {
        mix_id: hex::encode(mix_id),
        denomination: req.denomination,
        status: "collecting".to_string(),
    }))
}

pub async fn mix_register(
    State(node): State<AppState>,
    Json(req): Json<MixRegisterRequest>,
) -> Result<Json<serde_json::Value>, ErrorResponse> {
    let mix_id = parse_hex32(&req.mix_id, "mix_id")?;

    let input = InputReveal {
        owner_pk: parse_hex32(&req.input.owner_pk, "owner_pk")?,
        value: req.input.value,
        salt: parse_hex32(&req.input.salt, "input_salt")?,
    };
    let output = OutputData {
        address: parse_hex32(&req.output.address, "address")?,
        value: req.output.value,
        salt: parse_hex32(&req.output.salt, "output_salt")?,
    };

    node.mix_register(mix_id, input, output).await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(serde_json::json!({ "status": "registered" })))
}

pub async fn mix_fee(
    State(node): State<AppState>,
    Json(req): Json<MixFeeRequest>,
) -> Result<Json<serde_json::Value>, ErrorResponse> {
    let mix_id = parse_hex32(&req.mix_id, "mix_id")?;

    let input = InputReveal {
        owner_pk: parse_hex32(&req.input.owner_pk, "owner_pk")?,
        value: req.input.value,
        salt: parse_hex32(&req.input.salt, "input_salt")?,
    };

    node.mix_set_fee(mix_id, input).await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(serde_json::json!({ "status": "fee_set" })))
}

pub async fn mix_sign(
    State(node): State<AppState>,
    Json(req): Json<MixSignRequest>,
) -> Result<Json<serde_json::Value>, ErrorResponse> {
    let mix_id = parse_hex32(&req.mix_id, "mix_id")?;
    let coin_id = parse_hex32(&req.coin_id, "coin_id")?;

    let input_index = node.mix_find_input_index(mix_id, coin_id).await
        .ok_or_else(|| ErrorResponse {
            error: format!("coin {} not found in mix proposal", req.coin_id),
        })?;

    let signature = hex::decode(&req.signature)
        .map_err(|e| ErrorResponse { error: format!("invalid signature hex: {}", e) })?;

    node.mix_sign(mix_id, input_index, signature).await
        .map_err(|e| ErrorResponse { error: e.to_string() })?;

    Ok(Json(serde_json::json!({ "status": "signed", "input_index": input_index })))
}

pub async fn mix_status(
    State(node): State<AppState>,
    axum::extract::Path(mix_id_hex): axum::extract::Path<String>,
) -> Result<Json<MixStatusResponse>, ErrorResponse> {
    let mix_id = parse_hex32(&mix_id_hex, "mix_id")?;

    let snapshot = node.mix_status(mix_id).await
        .ok_or_else(|| ErrorResponse { error: "mix session not found".to_string() })?;

    Ok(Json(snapshot_to_response(snapshot)))
}

pub async fn mix_list(
    State(node): State<AppState>,
) -> Json<MixListResponse> {
    let sessions = node.mix_list().await;
    Json(MixListResponse {
        sessions: sessions.into_iter().map(snapshot_to_response).collect(),
    })
}

fn snapshot_to_response(s: crate::mix::MixStatusSnapshot) -> MixStatusResponse {
    let phase_str = match &s.phase {
        crate::mix::MixPhase::Collecting => "collecting",
        crate::mix::MixPhase::Signing => "signing",
        crate::mix::MixPhase::CommitSubmitted => "commit_submitted",
        crate::mix::MixPhase::Complete => "complete",
        crate::mix::MixPhase::Failed(_) => "failed",
    };
    MixStatusResponse {
        mix_id: s.mix_id,
        denomination: s.denomination,
        participants: s.participants,
        phase: phase_str.to_string(),
        commitment: s.commitment,
        input_coin_ids: s.input_coin_ids,
    }
}




══════════════════════════════════════════════════════════════
 FILE: rpc/mod.rs
══════════════════════════════════════════════════════════════

pub mod server;
pub mod handlers;
pub mod types;

pub use server::RpcServer;
pub use types::*;




══════════════════════════════════════════════════════════════
 FILE: rpc/server.rs
══════════════════════════════════════════════════════════════

use super::handlers::*;
use crate::node::NodeHandle;
use anyhow::Result;
use axum::{
    routing::{get, post},
    Router,
};
use std::net::SocketAddr;
use tower_http::trace::TraceLayer;

pub struct RpcServer {
    addr: SocketAddr,
}

impl RpcServer {
    pub fn new(port: u16) -> Self {
        let addr = SocketAddr::from(([127, 0, 0, 1], port));
        Self { addr }
    }

    pub async fn run(self, node_handle: NodeHandle) -> Result<()> {
        let app = Router::new()
            .route("/health", get(health))
            .route("/state", get(get_state))
            .route("/commit", post(commit_transaction))
            .route("/send", post(send_transaction))
            .route("/check", post(check_coin))
            .route("/mempool", get(get_mempool))
            .route("/keygen", get(generate_key))
            .route("/peers", get(get_peers))
            .route("/scan", post(scan_addresses))
            .route("/mss_state", post(get_mss_state))
            // CoinJoin mix endpoints
            .route("/mix/create", post(mix_create))
            .route("/mix/register", post(mix_register))
            .route("/mix/fee", post(mix_fee))
            .route("/mix/sign", post(mix_sign))
            .route("/mix/status/:mix_id", get(mix_status))
            .route("/mix/list", get(mix_list))
            .layer(TraceLayer::new_for_http())
            .with_state(node_handle);

        tracing::info!("RPC server listening on {}", self.addr);

        let listener = tokio::net::TcpListener::bind(self.addr).await?;
        axum::serve(listener, app).await?;

        Ok(())
    }
}




══════════════════════════════════════════════════════════════
 FILE: rpc/types.rs
══════════════════════════════════════════════════════════════

use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct CommitRequest {
    /// Input coin IDs being spent (hex, 32 bytes each)
    pub coins: Vec<String>,
    /// Output coin IDs (hex, 32 bytes each)
    pub destinations: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CommitResponse {
    pub commitment: String,
    pub salt: String,
    pub status: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct InputRevealJson {
    pub owner_pk: String,  
    pub value: u64,
    pub salt: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct OutputDataJson {
    pub address: String, 
    pub value: u64,
    pub salt: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SendTransactionRequest {
    pub inputs: Vec<InputRevealJson>,
    pub signatures: Vec<String>,
    pub outputs: Vec<OutputDataJson>,
    /// Commitment salt (hex)
    pub salt: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SendTransactionResponse {
    pub input_coins: Vec<String>,
    pub output_coins: Vec<String>,
    pub fee: u64,
    pub status: String,
}
#[derive(Debug, Serialize, Deserialize)]
pub struct ScanRequest {
    pub addresses: Vec<String>,
    pub start_height: u64,
    pub end_height: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ScanResponse {
    pub coins: Vec<ScanCoin>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ScanCoin {
    pub address: String,
    pub value: u64,
    pub salt: String,
    pub coin_id: String,
    pub height: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GetStateResponse {
    pub height: u64,
    pub depth: u64,
    /// Number of blocks required for 1e-6 finality risk.
    pub safe_depth: u64,
    pub midstate: String,
    pub num_coins: usize,
    pub num_commitments: usize,
    pub target: String,
    pub block_reward: u64,
}
#[derive(Debug, Serialize, Deserialize)]
pub struct GetMssStateRequest {
    pub master_pk: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GetMssStateResponse {
    pub next_index: u64,
}
#[derive(Debug, Serialize, Deserialize)]
pub struct CheckCoinRequest {
    pub coin: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct CheckCoinResponse {
    pub exists: bool,
    pub coin: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GetMempoolResponse {
    pub size: usize,
    pub transactions: Vec<TransactionInfo>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TransactionInfo {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub commitment: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub input_coins: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub output_coins: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub fee: Option<u64>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GenerateKeyResponse {
    pub seed: String,
    pub address: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct GetPeersResponse {
    pub peers: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ErrorResponse {
    pub error: String,
}

// ── CoinJoin Mix Types ──────────────────────────────────────────────────

#[derive(Debug, Serialize, Deserialize)]
pub struct MixCreateRequest {
    pub denomination: u64,
    #[serde(default = "default_min_participants")]
    pub min_participants: usize,
}
fn default_min_participants() -> usize { 2 }

#[derive(Debug, Serialize, Deserialize)]
pub struct MixCreateResponse {
    pub mix_id: String,
    pub denomination: u64,
    pub status: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixRegisterRequest {
    pub mix_id: String,
    /// Hex coin_id of the input being mixed
    pub coin_id: String,
    /// Input reveal data
    pub input: InputRevealJson,
    /// Output data for the mixed coin
    pub output: OutputDataJson,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixFeeRequest {
    pub mix_id: String,
    /// Input reveal for the denomination-1 fee coin
    pub input: InputRevealJson,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixSignRequest {
    pub mix_id: String,
    /// Hex coin_id the wallet is signing for (used to find input_index)
    pub coin_id: String,
    /// Hex-encoded signature
    pub signature: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixStatusResponse {
    pub mix_id: String,
    pub denomination: u64,
    pub participants: usize,
    pub phase: String,
    /// Set when phase == "signing"
    pub commitment: Option<String>,
    /// Input coin IDs in canonical proposal order (for the wallet to find its index)
    pub input_coin_ids: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixListResponse {
    pub sessions: Vec<MixStatusResponse>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct MixActionResponse {
    pub status: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub input_index: Option<usize>,
}




══════════════════════════════════════════════════════════════
 FILE: storage/batch_store.rs
══════════════════════════════════════════════════════════════

use crate::core::{Batch, BatchHeader};
use anyhow::Result;
use std::path::{Path, PathBuf};
use std::fs;

#[derive(Debug, Clone)]
pub struct BatchStore {
    base_path: PathBuf,
}

impl BatchStore {
    pub fn new<P: AsRef<Path>>(path: P) -> Result<Self> {
        let base_path = path.as_ref().to_path_buf();
        fs::create_dir_all(&base_path)?;
        Ok(Self { base_path })
    }

    /// Height up to which checkpoints have already been pruned.
    /// Returns 0 if no pruning has occurred yet.
    pub fn pruned_up_to(&self) -> u64 {
        let marker = self.base_path.join("pruned_up_to");
        fs::read_to_string(marker)
            .ok()
            .and_then(|s| s.trim().parse().ok())
            .unwrap_or(0)
    }

    /// Prune checkpoints from all batch and header files below `below_height`.
    ///
    /// Skips heights already pruned (tracked via a marker file). Each pruned
    /// batch drops ~32 KB of checkpoint data, reducing storage by ~98%.
    /// Pruned batches remain verifiable via full-chain recomputation.
    ///
    /// Returns the number of batches pruned in this call.
    pub fn prune_checkpoints(&self, below_height: u64) -> Result<u64> {
        let already_pruned = self.pruned_up_to();
        if below_height <= already_pruned {
            return Ok(0);
        }

        let mut count = 0u64;
        for height in already_pruned..below_height {
            let folder = height / 1000;
            let folder_path = self.base_path.join(format!("{:06}", folder));

            // Prune batch file
            let batch_path = folder_path.join(format!("batch_{}.bin", height));
            if batch_path.exists() {
                if let Ok(bytes) = fs::read(&batch_path) {
                    if let Ok(mut batch) = bincode::deserialize::<Batch>(&bytes) {
                        if !batch.extension.checkpoints.is_empty() {
                            batch.extension.checkpoints = vec![];
                            if let Ok(pruned_bytes) = bincode::serialize(&batch) {
                                fs::write(&batch_path, pruned_bytes)?;
                            }
                        }
                    }
                }
            }

            // Prune header file
            let hdr_path = folder_path.join(format!("header_{}.bin", height));
            if hdr_path.exists() {
                if let Ok(bytes) = fs::read(&hdr_path) {
                    if let Ok(mut header) = bincode::deserialize::<BatchHeader>(&bytes) {
                        if !header.extension.checkpoints.is_empty() {
                            header.extension.checkpoints = vec![];
                            if let Ok(pruned_bytes) = bincode::serialize(&header) {
                                fs::write(&hdr_path, pruned_bytes)?;
                            }
                        }
                    }
                }
            }

            count += 1;
        }

        // Update marker
        let marker = self.base_path.join("pruned_up_to");
        fs::write(marker, below_height.to_string())?;

        if count > 0 {
            tracing::info!(
                "Pruned checkpoints from {} batches (heights {}..{})",
                count, already_pruned, below_height
            );
        }

        Ok(count)
    }
    
    /// Save a batch (and its lightweight header for fast sync)
    pub fn save(&self, height: u64, batch: &Batch) -> Result<()> {
        let folder = height / 1000; // 1000 batches per folder
        let folder_path = self.base_path.join(format!("{:06}", folder));
        fs::create_dir_all(&folder_path)?;
        
        let file_path = folder_path.join(format!("batch_{}.bin", height));
        let bytes = bincode::serialize(batch)?;
        fs::write(&file_path, bytes)?;

        // Write header separately — avoids deserializing full batch (with
        // transactions + WOTS sigs) when peers request header chains.
        let mut header = batch.header();
        header.height = height;
        let hdr_path = folder_path.join(format!("header_{}.bin", height));
        let hdr_bytes = bincode::serialize(&header)?;
        fs::write(hdr_path, hdr_bytes)?;

        Ok(())
    }
    
    /// Load a batch
    pub fn load(&self, height: u64) -> Result<Option<Batch>> {
        let folder = height / 1000;
        let file_path = self.base_path
            .join(format!("{:06}", folder))
            .join(format!("batch_{}.bin", height));
        
        if !file_path.exists() {
            return Ok(None);
        }
        
        let bytes = fs::read(file_path)?;
        let batch = bincode::deserialize(&bytes)?;
        Ok(Some(batch))
    }

    /// Load a pre-computed header (falls back to full batch if header file missing)
    fn load_header(&self, height: u64) -> Result<Option<BatchHeader>> {
        let folder = height / 1000;
        let folder_path = self.base_path.join(format!("{:06}", folder));
        let hdr_path = folder_path.join(format!("header_{}.bin", height));

        if hdr_path.exists() {
            let bytes = fs::read(hdr_path)?;
            let header: BatchHeader = bincode::deserialize(&bytes)?;
            return Ok(Some(header));
        }

        // Fallback: load full batch (for batches saved before this change)
        if let Some(batch) = self.load(height)? {
            let mut header = batch.header();
            header.height = height;
            Ok(Some(header))
        } else {
            Ok(None)
        }
    }

    /// Load headers for a range — uses lightweight header files when available
    pub fn load_headers(&self, start: u64, end: u64) -> Result<Vec<BatchHeader>> {
        let mut headers = Vec::with_capacity((end - start) as usize);
        for h in start..end {
            if let Some(header) = self.load_header(h)? {
                headers.push(header);
            }
        }
        Ok(headers)
    }

    /// Get all batches from height range
    pub fn load_range(&self, start: u64, end: u64) -> Result<Vec<(u64, Batch)>> {

        let mut batches = Vec::new();
        
        for height in start..end {
            match self.load(height) {
                Ok(Some(batch)) => batches.push((height, batch)),

                Ok(None) => {
                    tracing::warn!("Gap in batch store at height {}, returning {} contiguous batches", height, batches.len());
                    break;
                }
                Err(e) => {
                    eprintln!("[WARN] Error loading batch at height {}: {}, continuing", height, e);
                    break;
                }
            }
        }
        
        Ok(batches)
    }
    
    /// Get highest batch we have
    pub fn highest(&self) -> Result<u64> {
        let mut max = 0u64;
        
        for entry in fs::read_dir(&self.base_path)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.is_dir() {
                for file in fs::read_dir(path)? {
                    let file = file?;
                    let name = file.file_name();
                    let name_str = name.to_string_lossy();
                    
                    if let Some(height_str) = name_str.strip_prefix("batch_").and_then(|s| s.strip_suffix(".bin")) {
                        if let Ok(height) = height_str.parse::<u64>() {
                            max = max.max(height);
                        }
                    }
                }
            }
        }
        
        Ok(max)
    }
    
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::types::*;
    use crate::core::extension::create_extension;
    use tempfile::tempdir;

    fn dummy_batch(nonce: u64) -> Batch {
        let ms = hash(&nonce.to_le_bytes());
        let ext = create_extension(ms, nonce);
        Batch {
            prev_midstate: ms,
            transactions: vec![],
            extension: ext,
            coinbase: vec![],
            timestamp: 1000 + nonce,
            target: [0xff; 32],
        }
    }

    #[test]
    fn save_and_load_round_trip() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        let batch = dummy_batch(42);
        store.save(0, &batch).unwrap();
        let loaded = store.load(0).unwrap().unwrap();
        assert_eq!(loaded.prev_midstate, batch.prev_midstate);
        assert_eq!(loaded.timestamp, batch.timestamp);
        assert_eq!(loaded.extension.nonce, batch.extension.nonce);
    }

    #[test]
    fn load_nonexistent_returns_none() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();
        assert!(store.load(999).unwrap().is_none());
    }

    #[test]
    fn save_overwrite() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        store.save(0, &dummy_batch(1)).unwrap();
        store.save(0, &dummy_batch(2)).unwrap();
        let loaded = store.load(0).unwrap().unwrap();
        assert_eq!(loaded.extension.nonce, 2);
    }

    #[test]
    fn load_range_contiguous() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        for h in 0..5 {
            store.save(h, &dummy_batch(h)).unwrap();
        }

        let range = store.load_range(1, 4).unwrap();
        assert_eq!(range.len(), 3);
        assert_eq!(range[0].0, 1);
        assert_eq!(range[2].0, 3);
    }

    #[test]
    fn load_range_stops_at_gap() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        store.save(0, &dummy_batch(0)).unwrap();
        store.save(1, &dummy_batch(1)).unwrap();
        // Gap at height 2
        store.save(3, &dummy_batch(3)).unwrap();

        let range = store.load_range(0, 4).unwrap();
        assert_eq!(range.len(), 2); // stops at gap
    }

    #[test]
    fn highest_batch() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        assert_eq!(store.highest().unwrap(), 0);

        store.save(5, &dummy_batch(5)).unwrap();
        store.save(100, &dummy_batch(100)).unwrap();
        store.save(50, &dummy_batch(50)).unwrap();

        assert_eq!(store.highest().unwrap(), 100);
    }

    #[test]
    fn cross_folder_boundaries() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        // Heights 999 and 1000 go into different folders
        store.save(999, &dummy_batch(999)).unwrap();
        store.save(1000, &dummy_batch(1000)).unwrap();

        assert!(store.load(999).unwrap().is_some());
        assert!(store.load(1000).unwrap().is_some());
        assert_eq!(store.highest().unwrap(), 1000);
    }

    // ── Checkpoint pruning ──────────────────────────────────────────────

    #[test]
    fn prune_removes_checkpoints() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        for h in 0..5 {
            store.save(h, &dummy_batch(h)).unwrap();
        }

        // Before pruning: checkpoints present
        let batch = store.load(0).unwrap().unwrap();
        assert!(!batch.extension.checkpoints.is_empty());

        // Prune below height 3
        let count = store.prune_checkpoints(3).unwrap();
        assert_eq!(count, 3);

        // Heights 0-2: checkpoints gone
        for h in 0..3 {
            let batch = store.load(h).unwrap().unwrap();
            assert!(batch.extension.checkpoints.is_empty(),
                "height {} should be pruned", h);
        }

        // Heights 3-4: checkpoints intact
        for h in 3..5 {
            let batch = store.load(h).unwrap().unwrap();
            assert!(!batch.extension.checkpoints.is_empty(),
                "height {} should NOT be pruned", h);
        }
    }

    #[test]
    fn prune_also_prunes_headers() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        store.save(0, &dummy_batch(0)).unwrap();
        store.prune_checkpoints(1).unwrap();

        let headers = store.load_headers(0, 1).unwrap();
        assert_eq!(headers.len(), 1);
        assert!(headers[0].extension.checkpoints.is_empty());
    }

    #[test]
    fn prune_is_idempotent() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        for h in 0..3 {
            store.save(h, &dummy_batch(h)).unwrap();
        }

        store.prune_checkpoints(2).unwrap();
        let count = store.prune_checkpoints(2).unwrap();
        assert_eq!(count, 0, "second prune to same height should be no-op");
    }

    #[test]
    fn prune_incremental() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        for h in 0..10 {
            store.save(h, &dummy_batch(h)).unwrap();
        }

        store.prune_checkpoints(3).unwrap();
        assert_eq!(store.pruned_up_to(), 3);

        store.prune_checkpoints(7).unwrap();
        assert_eq!(store.pruned_up_to(), 7);

        // Heights 0-6 pruned, 7-9 intact
        for h in 0..7 {
            let batch = store.load(h).unwrap().unwrap();
            assert!(batch.extension.checkpoints.is_empty());
        }
        for h in 7..10 {
            let batch = store.load(h).unwrap().unwrap();
            assert!(!batch.extension.checkpoints.is_empty());
        }
    }

    #[test]
    fn prune_marker_persists() {
        let dir = tempdir().unwrap();
        let path = dir.path().join("batches");
        let store = BatchStore::new(&path).unwrap();

        store.save(0, &dummy_batch(0)).unwrap();
        store.prune_checkpoints(1).unwrap();
        assert_eq!(store.pruned_up_to(), 1);

        // New store instance reads the marker
        let store2 = BatchStore::new(&path).unwrap();
        assert_eq!(store2.pruned_up_to(), 1);
    }

    #[test]
    fn pruned_batches_still_load_correctly() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        let batch = dummy_batch(42);
        store.save(0, &batch).unwrap();
        store.prune_checkpoints(1).unwrap();

        let loaded = store.load(0).unwrap().unwrap();
        assert_eq!(loaded.prev_midstate, batch.prev_midstate);
        assert_eq!(loaded.extension.nonce, batch.extension.nonce);
        assert_eq!(loaded.extension.final_hash, batch.extension.final_hash);
        assert!(loaded.extension.checkpoints.is_empty());
    }

    #[test]
    fn pruned_batch_passes_full_chain_verification() {
        use crate::core::extension::verify_extension;

        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        let nonce = 42u64;
        let batch = dummy_batch(nonce);
        let midstate = batch.prev_midstate;

        // Verify before pruning (spot-check path)
        assert!(verify_extension(
            midstate, &batch.extension, &[0xff; 32]
        ).is_ok());

        store.save(0, &batch).unwrap();
        store.prune_checkpoints(1).unwrap();
        let pruned = store.load(0).unwrap().unwrap();

        // Verify after pruning (full-chain path)
        assert!(verify_extension(
            midstate, &pruned.extension, &[0xff; 32]
        ).is_ok());
    }

    #[test]
    fn prune_skips_missing_heights() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        // Save only height 0 and 2 (gap at 1)
        store.save(0, &dummy_batch(0)).unwrap();
        store.save(2, &dummy_batch(2)).unwrap();

        // Should not panic on missing height 1
        let count = store.prune_checkpoints(3).unwrap();
        assert_eq!(count, 3); // iterates 0,1,2 — silently skips missing

        let b0 = store.load(0).unwrap().unwrap();
        assert!(b0.extension.checkpoints.is_empty());
        let b2 = store.load(2).unwrap().unwrap();
        assert!(b2.extension.checkpoints.is_empty());
    }

    #[test]
    fn prune_storage_savings() {
        let dir = tempdir().unwrap();
        let store = BatchStore::new(dir.path().join("batches")).unwrap();

        store.save(0, &dummy_batch(0)).unwrap();

        let folder = dir.path().join("batches/000000");
        let batch_path = folder.join("batch_0.bin");
        let header_path = folder.join("header_0.bin");

        let pre_batch = std::fs::metadata(&batch_path).unwrap().len();
        let pre_header = std::fs::metadata(&header_path).unwrap().len();

        store.prune_checkpoints(1).unwrap();

        let post_batch = std::fs::metadata(&batch_path).unwrap().len();
        let post_header = std::fs::metadata(&header_path).unwrap().len();

        // Pruned files should be dramatically smaller
        assert!(post_batch < pre_batch / 2,
            "batch should shrink: {} -> {}", pre_batch, post_batch);
        assert!(post_header < pre_header / 2,
            "header should shrink: {} -> {}", pre_header, post_header);
    }
}




══════════════════════════════════════════════════════════════
 FILE: storage.rs
══════════════════════════════════════════════════════════════

mod batch_store;
pub use batch_store::BatchStore;

use crate::core::State;
use anyhow::Result;
use redb::{Database, TableDefinition};
use std::path::Path;
use std::sync::Arc;

const STATE_TABLE: TableDefinition<&str, &[u8]> = TableDefinition::new("state");
const MINING_SEED_TABLE: TableDefinition<&str, &[u8]> = TableDefinition::new("mining_seed");

#[derive(Debug, Clone)]
pub struct Storage {
    db: Arc<Database>,
    pub batches: BatchStore,
}

impl Storage {
    pub fn open<P: AsRef<Path>>(path: P) -> Result<Self> {
        let path = path.as_ref();
        std::fs::create_dir_all(path)?;

        // redb acquires an exclusive file lock.  If a previous node process
        // is still shutting down (race between kill and restart), the lock
        // may not yet be released.  Retry with back-off before giving up.
        let db_path = path.join("state.redb");
        let mut last_err = None;
        for attempt in 0..10 {
            match Database::create(&db_path) {
                Ok(db) => {
                    if attempt > 0 {
                        tracing::info!("Database lock acquired after {} retries", attempt);
                    }
                    // Initialize tables
                    let write_txn = db.begin_write()?;
                    {
                        let _ = write_txn.open_table(STATE_TABLE)?;
                        let _ = write_txn.open_table(MINING_SEED_TABLE)?;
                    }
                    write_txn.commit()?;

                    let batches = BatchStore::new(path.join("batches"))?;

                    return Ok(Self {
                        db: Arc::new(db),
                        batches,
                    });
                }
                Err(e) => {
                    last_err = Some(e);
                    let delay = std::time::Duration::from_millis(100 * (1 << attempt.min(5)));
                    tracing::warn!(
                        "Database lock attempt {} failed, retrying in {:?}...",
                        attempt + 1, delay
                    );
                    std::thread::sleep(delay);
                }
            }
        }
        Err(last_err.unwrap().into())
    }

    pub fn save_state(&self, state: &State) -> Result<()> {
        let bytes = bincode::serialize(state)?;
        let write_txn = self.db.begin_write()?;
        {
            let mut table = write_txn.open_table(STATE_TABLE)?;
            table.insert("current", bytes.as_slice())?;
        }
        write_txn.commit()?;
        Ok(())
    }

    pub fn load_state(&self) -> Result<Option<State>> {
        let read_txn = self.db.begin_read()?;
        let table = read_txn.open_table(STATE_TABLE)?;
        match table.get("current")? {
            Some(bytes) => {
                let mut state: State = bincode::deserialize(bytes.value())?;
                state.coins.rebuild_tree();
                state.commitments.rebuild_tree();
                Ok(Some(state))
            }
            None => Ok(None),
        }
    }

    pub fn save_mining_seed(&self, seed: &[u8; 32]) -> Result<()> {
        let write_txn = self.db.begin_write()?;
        {
            let mut table = write_txn.open_table(MINING_SEED_TABLE)?;
            table.insert("seed", seed.as_slice())?;
        }
        write_txn.commit()?;
        Ok(())
    }

    pub fn load_mining_seed(&self) -> Result<Option<[u8; 32]>> {
        let read_txn = self.db.begin_read()?;
        let table = read_txn.open_table(MINING_SEED_TABLE)?;
        match table.get("seed")? {
            Some(bytes) => {
                let val = bytes.value();
                if val.len() != 32 {
                    anyhow::bail!("corrupt mining seed");
                }
                Ok(Some(<[u8; 32]>::try_from(val).unwrap()))
            }
            None => Ok(None),
        }
    }

    pub fn save_batch(&self, height: u64, batch: &crate::core::Batch) -> Result<()> {
        self.batches.save(height, batch)
    }

    pub fn load_batch(&self, height: u64) -> Result<Option<crate::core::Batch>> {
        self.batches.load(height)
    }

    pub fn load_batches(&self, start: u64, end: u64) -> Result<Vec<(u64, crate::core::Batch)>> {
        self.batches.load_range(start, end)
    }

    pub fn highest_batch(&self) -> Result<u64> {
        self.batches.highest()
    }

    /// Prune checkpoints from batches below the given height.
    /// See [`BatchStore::prune_checkpoints`] for details.
    pub fn prune_checkpoints(&self, below_height: u64) -> Result<u64> {
        self.batches.prune_checkpoints(below_height)
    }
}




══════════════════════════════════════════════════════════════
 FILE: sync.rs
══════════════════════════════════════════════════════════════

use crate::core::{State, BatchHeader, DIFFICULTY_LOOKBACK};
use crate::core::state::{apply_batch, adjust_difficulty};
use crate::core::extension::verify_extension;
use crate::storage::Storage;
use anyhow::{bail, Result};

pub struct Syncer {
    storage: Storage,
}

impl Syncer {
    pub fn new(storage: Storage) -> Self {
        Self { storage }
    }

    /// Verify PoW and internal header-to-header linkage on a contiguous
    /// slice of headers. The first header's prev_midstate is NOT checked
    /// here — that is handled by the fork-point logic.
    pub fn verify_header_chain(headers: &[BatchHeader]) -> Result<()> {
        for (i, header) in headers.iter().enumerate() {
            verify_extension(
                header.post_tx_midstate,
                &header.extension,
                &header.target,
            )
            .map_err(|e| anyhow::anyhow!("Invalid PoW at header index {}: {}", i, e))?;

            if i > 0 {
                let prev = &headers[i - 1];
                if header.prev_midstate != prev.extension.final_hash {
                    bail!(
                        "Header linkage broken at index {}: prev_midstate mismatch",
                        i
                    );
                }
            }
        }
        Ok(())
    }

    /// Find the first height where our locally stored chain and the peer's
    /// header chain diverge.  Everything below this height is shared history.
    ///
    /// `peer_headers` covers [0, peer_height).  We compare against our local
    /// batches stored on disk.
    pub fn find_fork_point(
        &self,
        peer_headers: &[BatchHeader],
        our_height: u64,
    ) -> Result<u64> {
        let compare_end = our_height.min(peer_headers.len() as u64);

        for h in 0..compare_end {
            match self.storage.load_batch(h)? {
                Some(our_batch) => {
                    let peer_hdr = &peer_headers[h as usize];
                    if our_batch.extension.final_hash != peer_hdr.extension.final_hash {
                        tracing::info!("Fork detected at height {}", h);
                        return Ok(h);
                    }
                }
                None => {
                    return Ok(h);
                }
            }
        }

        Ok(compare_end)
    }

    /// Rebuild local state from genesis up to (but not including) `target`,
    /// using batches already on disk.
    pub fn rebuild_state_to(&self, target: u64) -> Result<State> {
        let mut state = State::genesis().0;
        let mut recent_headers: Vec<u64> = Vec::new();
        let window_size = DIFFICULTY_LOOKBACK as usize;

        for h in 0..target {
            let batch = self
                .storage
                .load_batch(h)?
                .ok_or_else(|| anyhow::anyhow!("Missing batch at height {} during rebuild", h))?;
            
            recent_headers.push(state.timestamp);
            if recent_headers.len() > window_size { recent_headers.remove(0); }
            apply_batch(&mut state, &batch, &recent_headers)?;
            state.target = adjust_difficulty(&state);
        }
        Ok(state)
    }
}




══════════════════════════════════════════════════════════════
 FILE: wallet/coinjoin.rs
══════════════════════════════════════════════════════════════

//! Denomination-uniform CoinJoin mixing.
//!
//! Constructs joint transactions where N participants each contribute one input
//! of the same power-of-2 denomination and each receive one output of that
//! denomination. An additional denomination-1 fee input covers the mandatory
//! tx fee. On-chain, the transaction is a perfect permutation — subset sum
//! analysis yields zero information about the input→output mapping.
//!
//! # Protocol overview
//!
//! 1. Initiator creates a [`MixSession`] for a denomination (e.g. 8).
//! 2. Participants register via [`MixSession::register`] with their input
//!    coin and desired output address.
//! 3. One participant sets a denomination-1 fee input via [`MixSession::set_fee_input`].
//! 4. Once ready, [`MixSession::proposal`] returns a deterministic [`MixProposal`]
//!    containing the canonical input/output ordering and commitment hash.
//! 5. Each participant signs the commitment for their own input.
//! 6. [`MixSession::build_reveal`] assembles the final `Reveal` transaction.
//!
//! No consensus changes required. The resulting `Commit`/`Reveal` pair is
//! indistinguishable from a normal multi-input transaction.
//!
//! ```
//! use midstate::core::types::*;
//! use midstate::core::wots;
//! use midstate::wallet::coinjoin::*;
//!
//! let mut session = MixSession::new(8, 2).unwrap();
//!
//! // Alice: input 8, output 8 to fresh address
//! let seed_a = hash(b"alice");
//! let pk_a = wots::keygen(&seed_a);
//! let input_a = InputReveal { owner_pk: pk_a, value: 8, salt: [0xAA; 32] };
//! let output_a = OutputData { address: hash(b"alice-dest"), value: 8, salt: [0xBB; 32] };
//! session.register(input_a, output_a).unwrap();
//!
//! // Bob: input 8, output 8 to fresh address
//! let seed_b = hash(b"bob");
//! let pk_b = wots::keygen(&seed_b);
//! let input_b = InputReveal { owner_pk: pk_b, value: 8, salt: [0xCC; 32] };
//! let output_b = OutputData { address: hash(b"bob-dest"), value: 8, salt: [0xDD; 32] };
//! session.register(input_b, output_b).unwrap();
//!
//! // Fee coin (denomination 1)
//! let seed_f = hash(b"fee");
//! let pk_f = wots::keygen(&seed_f);
//! let fee_input = InputReveal { owner_pk: pk_f, value: 1, salt: [0xEE; 32] };
//! session.set_fee_input(fee_input).unwrap();
//!
//! let proposal = session.proposal().unwrap();
//! assert_eq!(proposal.inputs.len(), 3);  // 2 mix + 1 fee
//! assert_eq!(proposal.outputs.len(), 2); // 2 mix outputs
//!
//! // Each participant signs the commitment for their input(s)
//! let mut sigs = vec![Vec::new(); proposal.inputs.len()];
//! for (i, input) in proposal.inputs.iter().enumerate() {
//!     let seed = if input.owner_pk == pk_a { &seed_a }
//!         else if input.owner_pk == pk_b { &seed_b }
//!         else { &seed_f };
//!     sigs[i] = wots::sig_to_bytes(&wots::sign(seed, &proposal.commitment));
//! }
//!
//! let reveal = session.build_reveal(sigs).unwrap();
//! match &reveal {
//!     midstate::core::types::Transaction::Reveal { inputs, outputs, .. } => {
//!         assert_eq!(inputs.len(), 3);
//!         assert_eq!(outputs.len(), 2);
//!     }
//!     _ => panic!("expected Reveal"),
//! }
//! ```

use crate::core::types::*;
use anyhow::{bail, Result};

/// Minimum participants in a mix (excluding the fee donor).
pub const MIN_MIX_PARTICIPANTS: usize = 2;

/// Maximum participants in a single mix session.
pub const MAX_MIX_PARTICIPANTS: usize = 16;

/// A single participant's contribution to a mix.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct MixRegistration {
    pub input: InputReveal,
    pub output: OutputData,
}

/// A deterministic, canonical proposal that all participants can
/// independently verify and sign.
#[derive(Clone, Debug)]
pub struct MixProposal {
    /// Canonical inputs: mix inputs sorted by coin_id, fee input last.
    pub inputs: Vec<InputReveal>,
    /// Canonical outputs: sorted by coin_id.
    pub outputs: Vec<OutputData>,
    pub salt: [u8; 32],
    pub commitment: [u8; 32],
}

/// A denomination-uniform mix session.
///
/// Collects registrations, validates denomination uniformity, and produces
/// a canonical transaction proposal. Pure data — no IO or networking.
pub struct MixSession {
    denomination: u64,
    min_participants: usize,
    salt: [u8; 32],
    registrations: Vec<MixRegistration>,
    fee_input: Option<InputReveal>,
}

impl MixSession {
    /// Create a new mix session for the given power-of-2 denomination.
    ///
    /// `min_participants` is clamped to `[MIN_MIX_PARTICIPANTS, MAX_MIX_PARTICIPANTS]`.
    pub fn new(denomination: u64, min_participants: usize) -> Result<Self> {
        if denomination == 0 || !denomination.is_power_of_two() {
            bail!("denomination must be a nonzero power of 2, got {}", denomination);
        }
        let min = min_participants.clamp(MIN_MIX_PARTICIPANTS, MAX_MIX_PARTICIPANTS);
        let salt: [u8; 32] = rand::random();
        Ok(Self {
            denomination,
            min_participants: min,
            salt,
            registrations: Vec::new(),
            fee_input: None,
        })
    }

    /// Create a session with a specific salt (for deterministic testing).
    #[cfg(test)]
    pub fn with_salt(denomination: u64, min_participants: usize, salt: [u8; 32]) -> Result<Self> {
        if denomination == 0 || !denomination.is_power_of_two() {
            bail!("denomination must be a nonzero power of 2, got {}", denomination);
        }
        let min = min_participants.clamp(MIN_MIX_PARTICIPANTS, MAX_MIX_PARTICIPANTS);
        Ok(Self {
            denomination,
            min_participants: min,
            salt,
            registrations: Vec::new(),
            fee_input: None,
        })
    }

    pub fn denomination(&self) -> u64 {
        self.denomination
    }

    pub fn participant_count(&self) -> usize {
        self.registrations.len()
    }

    pub fn has_fee_input(&self) -> bool {
        self.fee_input.is_some()
    }

    /// Register a participant's input and output.
    ///
    /// Both must have value equal to the session denomination.
    /// Rejects duplicate inputs (same coin_id).
    pub fn register(&mut self, input: InputReveal, output: OutputData) -> Result<()> {
        if self.registrations.len() >= MAX_MIX_PARTICIPANTS {
            bail!("session full ({} participants)", MAX_MIX_PARTICIPANTS);
        }
        if input.value != self.denomination {
            bail!(
                "input value {} != session denomination {}",
                input.value, self.denomination
            );
        }
        if output.value != self.denomination {
            bail!(
                "output value {} != session denomination {}",
                output.value, self.denomination
            );
        }
        let coin_id = input.coin_id();
        if self.registrations.iter().any(|r| r.input.coin_id() == coin_id) {
            bail!("duplicate input coin");
        }
        self.registrations.push(MixRegistration { input, output });
        Ok(())
    }

    /// Set the denomination-1 fee input that covers the mandatory tx fee.
    ///
    /// Exactly one fee input per session. Must have value == 1.
    pub fn set_fee_input(&mut self, input: InputReveal) -> Result<()> {
        if input.value != 1 {
            bail!("fee input must be denomination 1, got {}", input.value);
        }
        if self.fee_input.is_some() {
            bail!("fee input already set");
        }
        // Must not collide with any mix input
        let coin_id = input.coin_id();
        if self.registrations.iter().any(|r| r.input.coin_id() == coin_id) {
            bail!("fee input collides with a mix input");
        }
        self.fee_input = Some(input);
        Ok(())
    }

    /// True when enough participants have registered and the fee input is set.
    pub fn is_ready(&self) -> bool {
        self.registrations.len() >= self.min_participants && self.fee_input.is_some()
    }

    /// Build the canonical proposal.
    ///
    /// Inputs are ordered: mix inputs sorted by coin_id, then the fee input.
    /// Outputs are sorted by coin_id. This ordering is deterministic — all
    /// participants independently compute the same commitment.
    pub fn proposal(&self) -> Result<MixProposal> {
        if self.registrations.len() < self.min_participants {
            bail!(
                "need {} participants, have {}",
                self.min_participants,
                self.registrations.len()
            );
        }
        let fee = self.fee_input.as_ref()
            .ok_or_else(|| anyhow::anyhow!("fee input not set"))?;

        // Canonical input order: mix inputs sorted by coin_id, fee last.
        let mut mix_inputs: Vec<InputReveal> = self.registrations
            .iter()
            .map(|r| r.input.clone())
            .collect();
        mix_inputs.sort_by_key(|i| i.coin_id());
        let mut inputs = mix_inputs;
        inputs.push(fee.clone());

        // Canonical output order: sorted by coin_id.
        let mut outputs: Vec<OutputData> = self.registrations
            .iter()
            .map(|r| r.output.clone())
            .collect();
        outputs.sort_by_key(|o| o.coin_id());

        let input_coin_ids: Vec<[u8; 32]> = inputs.iter().map(|i| i.coin_id()).collect();
        let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();
        let commitment = compute_commitment(&input_coin_ids, &output_coin_ids, &self.salt);

        Ok(MixProposal {
            inputs,
            outputs,
            salt: self.salt,
            commitment,
        })
    }

    /// Assemble the final `Reveal` transaction from collected signatures.
    ///
    /// `signatures` must correspond 1:1 with `proposal().inputs` in the same order.
    /// Each participant signs the commitment with their own key; the caller
    /// collects all signatures and passes them here.
    pub fn build_reveal(&self, signatures: Vec<Vec<u8>>) -> Result<Transaction> {
        let proposal = self.proposal()?;
        if signatures.len() != proposal.inputs.len() {
            bail!(
                "expected {} signatures, got {}",
                proposal.inputs.len(),
                signatures.len()
            );
        }
        Ok(Transaction::Reveal {
            inputs: proposal.inputs,
            signatures,
            outputs: proposal.outputs,
            salt: proposal.salt,
        })
    }
}

/// Validate that a `Reveal` transaction has the structure of a denomination-uniform
/// CoinJoin: all mix inputs share one denomination, all outputs share that denomination,
/// and at most one input has denomination 1 (fee).
///
/// This is a heuristic check for observers/analysis; the consensus layer validates
/// the transaction normally regardless.
pub fn is_uniform_mix(tx: &Transaction) -> bool {
    let (inputs, outputs) = match tx {
        Transaction::Reveal { inputs, outputs, .. } => (inputs, outputs),
        _ => return false,
    };

    if inputs.len() < MIN_MIX_PARTICIPANTS + 1 || outputs.len() < MIN_MIX_PARTICIPANTS {
        return false;
    }

    // All outputs must share the same power-of-2 denomination.
    let denom = outputs[0].value;
    if denom == 0 || !denom.is_power_of_two() {
        return false;
    }
    if !outputs.iter().all(|o| o.value == denom) {
        return false;
    }

    // Inputs: exactly outputs.len() inputs at `denom`, plus at most one at 1 (fee).
    let mix_count = inputs.iter().filter(|i| i.value == denom).count();
    let fee_count = inputs.iter().filter(|i| i.value == 1).count();

    mix_count == outputs.len() && fee_count <= 1 && mix_count + fee_count == inputs.len()
}


#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::wots;

    fn make_participant(name: &[u8]) -> ([u8; 32], InputReveal, OutputData) {
        let seed = hash(name);
        let pk = wots::keygen(&seed);
        let input = InputReveal {
            owner_pk: pk,
            value: 8,
            salt: hash_concat(name, b"input-salt"),
        };
        let output = OutputData {
            address: hash_concat(name, b"dest"),
            value: 8,
            salt: hash_concat(name, b"output-salt"),
        };
        (seed, input, output)
    }

    fn make_fee_participant(name: &[u8]) -> ([u8; 32], InputReveal) {
        let seed = hash(name);
        let pk = wots::keygen(&seed);
        let input = InputReveal {
            owner_pk: pk,
            value: 1,
            salt: hash_concat(name, b"fee-salt"),
        };
        (seed, input)
    }

    fn ready_session() -> (MixSession, Vec<([u8; 32], InputReveal)>) {
        let mut session = MixSession::with_salt(8, 2, [0x42; 32]).unwrap();

        let (seed_a, in_a, out_a) = make_participant(b"alice");
        let (seed_b, in_b, out_b) = make_participant(b"bob");
        let (seed_f, fee) = make_fee_participant(b"fee-donor");

        session.register(in_a.clone(), out_a).unwrap();
        session.register(in_b.clone(), out_b).unwrap();
        session.set_fee_input(fee.clone()).unwrap();

        let seeds = vec![
            (seed_a, in_a),
            (seed_b, in_b),
            (seed_f, fee),
        ];
        (session, seeds)
    }

    // ── Construction ────────────────────────────────────────────────────

    #[test]
    fn new_rejects_zero_denomination() {
        assert!(MixSession::new(0, 2).is_err());
    }

    #[test]
    fn new_rejects_non_power_of_two() {
        assert!(MixSession::new(3, 2).is_err());
        assert!(MixSession::new(6, 2).is_err());
        assert!(MixSession::new(15, 2).is_err());
    }

    #[test]
    fn new_accepts_valid_denominations() {
        for d in [1, 2, 4, 8, 16, 32, 64, 128, 256] {
            assert!(MixSession::new(d, 2).is_ok());
        }
    }

    #[test]
    fn min_participants_clamped() {
        let s = MixSession::new(8, 0).unwrap();
        assert_eq!(s.min_participants, MIN_MIX_PARTICIPANTS);

        let s = MixSession::new(8, 100).unwrap();
        assert_eq!(s.min_participants, MAX_MIX_PARTICIPANTS);
    }

    // ── Registration ────────────────────────────────────────────────────

    #[test]
    fn register_accepts_matching_denomination() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, input, output) = make_participant(b"alice");
        assert!(s.register(input, output).is_ok());
        assert_eq!(s.participant_count(), 1);
    }

    #[test]
    fn register_rejects_wrong_input_denomination() {
        let mut s = MixSession::new(8, 2).unwrap();
        let seed = hash(b"bad");
        let pk = wots::keygen(&seed);
        let input = InputReveal { owner_pk: pk, value: 4, salt: [0; 32] };
        let output = OutputData { address: [0; 32], value: 8, salt: [0; 32] };
        assert!(s.register(input, output).is_err());
    }

    #[test]
    fn register_rejects_wrong_output_denomination() {
        let mut s = MixSession::new(8, 2).unwrap();
        let seed = hash(b"bad");
        let pk = wots::keygen(&seed);
        let input = InputReveal { owner_pk: pk, value: 8, salt: [0; 32] };
        let output = OutputData { address: [0; 32], value: 4, salt: [0; 32] };
        assert!(s.register(input, output).is_err());
    }

    #[test]
    fn register_rejects_duplicate_input() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, input, output) = make_participant(b"alice");
        s.register(input.clone(), output.clone()).unwrap();

        let output2 = OutputData { address: hash(b"other"), value: 8, salt: [0xFF; 32] };
        assert!(s.register(input, output2).is_err());
    }

    #[test]
    fn register_rejects_when_full() {
        let mut s = MixSession::new(8, 2).unwrap();
        for i in 0..MAX_MIX_PARTICIPANTS {
            let (_, input, output) = make_participant(&[i as u8; 4]);
            s.register(input, output).unwrap();
        }
        let (_, input, output) = make_participant(b"overflow");
        assert!(s.register(input, output).is_err());
    }

    // ── Fee input ───────────────────────────────────────────────────────

    #[test]
    fn fee_input_accepts_denomination_1() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, fee) = make_fee_participant(b"fee");
        assert!(s.set_fee_input(fee).is_ok());
        assert!(s.has_fee_input());
    }

    #[test]
    fn fee_input_rejects_wrong_denomination() {
        let mut s = MixSession::new(8, 2).unwrap();
        let seed = hash(b"bad-fee");
        let pk = wots::keygen(&seed);
        let input = InputReveal { owner_pk: pk, value: 2, salt: [0; 32] };
        assert!(s.set_fee_input(input).is_err());
    }

    #[test]
    fn fee_input_rejects_double_set() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, fee1) = make_fee_participant(b"fee1");
        let (_, fee2) = make_fee_participant(b"fee2");
        s.set_fee_input(fee1).unwrap();
        assert!(s.set_fee_input(fee2).is_err());
    }

    #[test]
    fn fee_input_rejects_collision_with_mix_input() {
        let mut s = MixSession::new(1, 2).unwrap();
        // Register a denomination-1 mix input
        let seed = hash(b"collider");
        let pk = wots::keygen(&seed);
        let input = InputReveal { owner_pk: pk, value: 1, salt: [0xAA; 32] };
        let output = OutputData { address: hash(b"dest"), value: 1, salt: [0xBB; 32] };
        s.register(input.clone(), output).unwrap();

        // Same coin as fee should fail
        assert!(s.set_fee_input(input).is_err());
    }

    // ── Readiness ───────────────────────────────────────────────────────

    #[test]
    fn not_ready_without_enough_participants() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, fee) = make_fee_participant(b"fee");
        s.set_fee_input(fee).unwrap();

        let (_, input, output) = make_participant(b"alice");
        s.register(input, output).unwrap();
        assert!(!s.is_ready()); // only 1 of 2
    }

    #[test]
    fn not_ready_without_fee() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, in_a, out_a) = make_participant(b"alice");
        let (_, in_b, out_b) = make_participant(b"bob");
        s.register(in_a, out_a).unwrap();
        s.register(in_b, out_b).unwrap();
        assert!(!s.is_ready());
    }

    #[test]
    fn ready_with_participants_and_fee() {
        let (session, _) = ready_session();
        assert!(session.is_ready());
    }

    // ── Proposal ────────────────────────────────────────────────────────

    #[test]
    fn proposal_fails_without_enough_participants() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, in_a, out_a) = make_participant(b"alice");
        let (_, fee) = make_fee_participant(b"fee");
        s.register(in_a, out_a).unwrap();
        s.set_fee_input(fee).unwrap();
        assert!(s.proposal().is_err());
    }

    #[test]
    fn proposal_fails_without_fee() {
        let mut s = MixSession::new(8, 2).unwrap();
        let (_, in_a, out_a) = make_participant(b"alice");
        let (_, in_b, out_b) = make_participant(b"bob");
        s.register(in_a, out_a).unwrap();
        s.register(in_b, out_b).unwrap();
        assert!(s.proposal().is_err());
    }

    #[test]
    fn proposal_has_correct_counts() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();
        assert_eq!(p.inputs.len(), 3);  // 2 mix + 1 fee
        assert_eq!(p.outputs.len(), 2);
    }

    #[test]
    fn proposal_fee_input_is_last() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();
        assert_eq!(p.inputs.last().unwrap().value, 1);
        // All preceding inputs are the mix denomination
        for input in &p.inputs[..p.inputs.len() - 1] {
            assert_eq!(input.value, 8);
        }
    }

    #[test]
    fn proposal_inputs_sorted_by_coin_id() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();
        // Mix inputs (all but last) should be sorted by coin_id
        let mix_ids: Vec<[u8; 32]> = p.inputs[..p.inputs.len() - 1]
            .iter()
            .map(|i| i.coin_id())
            .collect();
        let mut sorted = mix_ids.clone();
        sorted.sort();
        assert_eq!(mix_ids, sorted);
    }

    #[test]
    fn proposal_outputs_sorted_by_coin_id() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();
        let out_ids: Vec<[u8; 32]> = p.outputs.iter().map(|o| o.coin_id()).collect();
        let mut sorted = out_ids.clone();
        sorted.sort();
        assert_eq!(out_ids, sorted);
    }

    #[test]
    fn proposal_value_conservation() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();
        let in_sum: u64 = p.inputs.iter().map(|i| i.value).sum();
        let out_sum: u64 = p.outputs.iter().map(|o| o.value).sum();
        assert!(in_sum > out_sum);
        assert_eq!(in_sum - out_sum, 1); // fee = 1
    }

    #[test]
    fn proposal_commitment_is_deterministic() {
        let (session, _) = ready_session();
        let p1 = session.proposal().unwrap();
        let p2 = session.proposal().unwrap();
        assert_eq!(p1.commitment, p2.commitment);
        assert_eq!(p1.salt, p2.salt);
    }

    #[test]
    fn proposal_commitment_matches_compute_commitment() {
        let (session, _) = ready_session();
        let p = session.proposal().unwrap();

        let input_ids: Vec<[u8; 32]> = p.inputs.iter().map(|i| i.coin_id()).collect();
        let output_ids: Vec<[u8; 32]> = p.outputs.iter().map(|o| o.coin_id()).collect();
        let expected = compute_commitment(&input_ids, &output_ids, &p.salt);
        assert_eq!(p.commitment, expected);
    }

    // ── build_reveal ────────────────────────────────────────────────────

    #[test]
    fn build_reveal_wrong_sig_count() {
        let (session, _) = ready_session();
        assert!(session.build_reveal(vec![]).is_err());
        assert!(session.build_reveal(vec![vec![]; 2]).is_err()); // need 3
    }

    #[test]
    fn build_reveal_produces_valid_reveal() {
        let (session, seeds) = ready_session();
        let proposal = session.proposal().unwrap();

        let sigs: Vec<Vec<u8>> = proposal.inputs.iter().map(|input| {
            let seed = seeds.iter()
                .find(|(_, i)| i.owner_pk == input.owner_pk)
                .unwrap().0;
            wots::sig_to_bytes(&wots::sign(&seed, &proposal.commitment))
        }).collect();

        let tx = session.build_reveal(sigs).unwrap();
        match &tx {
            Transaction::Reveal { inputs, signatures, outputs, salt } => {
                assert_eq!(inputs.len(), 3);
                assert_eq!(signatures.len(), 3);
                assert_eq!(outputs.len(), 2);
                assert_eq!(*salt, proposal.salt);
            }
            _ => panic!("expected Reveal"),
        }
    }

    #[test]
    fn build_reveal_signatures_verify() {
        let (session, seeds) = ready_session();
        let proposal = session.proposal().unwrap();

        let sigs: Vec<Vec<u8>> = proposal.inputs.iter().map(|input| {
            let seed = seeds.iter()
                .find(|(_, i)| i.owner_pk == input.owner_pk)
                .unwrap().0;
            wots::sig_to_bytes(&wots::sign(&seed, &proposal.commitment))
        }).collect();

        let tx = session.build_reveal(sigs).unwrap();
        if let Transaction::Reveal { inputs, signatures, .. } = &tx {
            for (input, sig_bytes) in inputs.iter().zip(signatures.iter()) {
                let sig = wots::sig_from_bytes(sig_bytes).unwrap();
                assert!(wots::verify(&sig, &proposal.commitment, &input.owner_pk));
            }
        }
    }

    // ── is_uniform_mix ──────────────────────────────────────────────────

    #[test]
    fn is_uniform_mix_true_for_coinjoin() {
        let (session, seeds) = ready_session();
        let proposal = session.proposal().unwrap();
        let sigs: Vec<Vec<u8>> = proposal.inputs.iter().map(|input| {
            let seed = seeds.iter()
                .find(|(_, i)| i.owner_pk == input.owner_pk)
                .unwrap().0;
            wots::sig_to_bytes(&wots::sign(&seed, &proposal.commitment))
        }).collect();
        let tx = session.build_reveal(sigs).unwrap();
        assert!(is_uniform_mix(&tx));
    }

    #[test]
    fn is_uniform_mix_false_for_commit() {
        let tx = Transaction::Commit { commitment: [0; 32], spam_nonce: 0 };
        assert!(!is_uniform_mix(&tx));
    }

    #[test]
    fn is_uniform_mix_false_for_non_uniform_outputs() {
        let tx = Transaction::Reveal {
            inputs: vec![
                InputReveal { owner_pk: [1; 32], value: 8, salt: [0; 32] },
                InputReveal { owner_pk: [2; 32], value: 8, salt: [0; 32] },
                InputReveal { owner_pk: [3; 32], value: 1, salt: [0; 32] },
            ],
            signatures: vec![vec![]; 3],
            outputs: vec![
                OutputData { address: [0; 32], value: 8, salt: [0; 32] },
                OutputData { address: [0; 32], value: 4, salt: [0; 32] }, // mismatch
            ],
            salt: [0; 32],
        };
        assert!(!is_uniform_mix(&tx));
    }

    #[test]
    fn is_uniform_mix_false_for_too_few_participants() {
        // 1 mix input + 1 fee = 2 inputs, 1 output → below MIN_MIX_PARTICIPANTS
        let tx = Transaction::Reveal {
            inputs: vec![
                InputReveal { owner_pk: [1; 32], value: 8, salt: [0; 32] },
                InputReveal { owner_pk: [2; 32], value: 1, salt: [0; 32] },
            ],
            signatures: vec![vec![]; 2],
            outputs: vec![
                OutputData { address: [0; 32], value: 8, salt: [0; 32] },
            ],
            salt: [0; 32],
        };
        assert!(!is_uniform_mix(&tx));
    }

    // ── End-to-end with consensus ───────────────────────────────────────

    #[test]
    fn coinjoin_passes_consensus_validation() {
        use crate::core::transaction::apply_transaction;
        use crate::core::mmr::UtxoAccumulator;

        let mut state = State {
            midstate: [0u8; 32],
            coins: UtxoAccumulator::new(),
            commitments: UtxoAccumulator::new(),
            depth: 0,
            target: [0xff; 32],
            height: 1,
            timestamp: 1000,
            commitment_heights: std::collections::HashMap::new(),
        };

        // Create 3 coins in the UTXO set
        let (seed_a, in_a, out_a) = make_participant(b"alice");
        let (seed_b, in_b, out_b) = make_participant(b"bob");
        let (seed_f, fee) = make_fee_participant(b"fee-donor");

        let pk_a = in_a.owner_pk;
        let pk_b = in_b.owner_pk;
        let pk_f = fee.owner_pk;
        let in_a_id = in_a.coin_id();
        let in_b_id = in_b.coin_id();
        let fee_id = fee.coin_id();

        state.coins.insert(in_a_id);
        state.coins.insert(in_b_id);
        state.coins.insert(fee_id);

        // Build the CoinJoin
        let mut session = MixSession::with_salt(8, 2, [0x42; 32]).unwrap();
        session.register(in_a, out_a).unwrap();
        session.register(in_b, out_b).unwrap();
        session.set_fee_input(fee).unwrap();

        let proposal = session.proposal().unwrap();

        // Mine commit PoW
        let mut nonce = 0u64;
        loop {
            let h = hash_concat(&proposal.commitment, &nonce.to_le_bytes());
            if u16::from_be_bytes([h[0], h[1]]) == 0x0000 {
                break;
            }
            nonce += 1;
        }

        // Apply Commit
        let commit_tx = Transaction::Commit {
            commitment: proposal.commitment,
            spam_nonce: nonce,
        };
        apply_transaction(&mut state, &commit_tx).unwrap();

        // Sign
        let seeds: Vec<([u8; 32], [u8; 32])> = vec![
            (seed_a, pk_a),
            (seed_b, pk_b),
            (seed_f, pk_f),
        ];
        let sigs: Vec<Vec<u8>> = proposal.inputs.iter().map(|input| {
            let (seed, _) = seeds.iter()
                .find(|(_, pk)| *pk == input.owner_pk)
                .unwrap();
            wots::sig_to_bytes(&wots::sign(seed, &proposal.commitment))
        }).collect();

        // Apply Reveal
        let reveal_tx = session.build_reveal(sigs).unwrap();
        apply_transaction(&mut state, &reveal_tx).unwrap();

        // Input coins spent
        assert!(!state.coins.contains(&in_a_id));
        assert!(!state.coins.contains(&in_b_id));
        assert!(!state.coins.contains(&fee_id));

        // Output coins created
        if let Transaction::Reveal { outputs, .. } = &reveal_tx {
            for o in outputs {
                assert!(state.coins.contains(&o.coin_id()));
            }
        }
    }

    // ── Three-participant session ───────────────────────────────────────

    #[test]
    fn three_participant_mix() {
        let mut session = MixSession::with_salt(16, 3, [0x99; 32]).unwrap();

        for i in 0..3u8 {
            let name = [i; 4];
            let seed = hash(&name);
            let pk = wots::keygen(&seed);
            let input = InputReveal { owner_pk: pk, value: 16, salt: hash(&[i + 100]) };
            let output = OutputData {
                address: hash(&[i + 200]),
                value: 16,
                salt: hash(&[i + 150]),
            };
            session.register(input, output).unwrap();
        }

        let (seed_f, fee) = make_fee_participant(b"fee3");
        session.set_fee_input(fee).unwrap();

        let p = session.proposal().unwrap();
        assert_eq!(p.inputs.len(), 4);  // 3 mix + 1 fee
        assert_eq!(p.outputs.len(), 3);

        let in_sum: u64 = p.inputs.iter().map(|i| i.value).sum();
        let out_sum: u64 = p.outputs.iter().map(|o| o.value).sum();
        assert_eq!(in_sum - out_sum, 1);
    }

    // ── Registration order doesn't affect proposal ──────────────────────

    #[test]
    fn proposal_independent_of_registration_order() {
        let (seed_a, in_a, out_a) = make_participant(b"alice");
        let (seed_b, in_b, out_b) = make_participant(b"bob");
        let (_, fee) = make_fee_participant(b"fee");

        let salt = [0x77; 32];

        // Order 1: alice then bob
        let mut s1 = MixSession::with_salt(8, 2, salt).unwrap();
        s1.register(in_a.clone(), out_a.clone()).unwrap();
        s1.register(in_b.clone(), out_b.clone()).unwrap();
        s1.set_fee_input(fee.clone()).unwrap();

        // Order 2: bob then alice
        let mut s2 = MixSession::with_salt(8, 2, salt).unwrap();
        s2.register(in_b, out_b).unwrap();
        s2.register(in_a, out_a).unwrap();
        s2.set_fee_input(fee).unwrap();

        let p1 = s1.proposal().unwrap();
        let p2 = s2.proposal().unwrap();

        assert_eq!(p1.commitment, p2.commitment);
        assert_eq!(
            p1.inputs.iter().map(|i| i.coin_id()).collect::<Vec<_>>(),
            p2.inputs.iter().map(|i| i.coin_id()).collect::<Vec<_>>(),
        );
        assert_eq!(
            p1.outputs.iter().map(|o| o.coin_id()).collect::<Vec<_>>(),
            p2.outputs.iter().map(|o| o.coin_id()).collect::<Vec<_>>(),
        );
    }

    // ── Denomination-1 mixing ───────────────────────────────────────────

    #[test]
    fn denomination_1_mix_needs_denomination_1_fee() {
        // Edge case: mixing denomination 1 coins. The fee input is *also*
        // denomination 1, so it looks like another mix participant on chain.
        // That's acceptable — the privacy set is N+1 instead of N.
        let mut session = MixSession::with_salt(1, 2, [0; 32]).unwrap();

        for i in 0..2u8 {
            let seed = hash(&[i]);
            let pk = wots::keygen(&seed);
            let input = InputReveal { owner_pk: pk, value: 1, salt: [i + 10; 32] };
            let output = OutputData { address: hash(&[i + 20]), value: 1, salt: [i + 30; 32] };
            session.register(input, output).unwrap();
        }

        let (_, fee) = make_fee_participant(b"fee-d1");
        session.set_fee_input(fee).unwrap();

        let p = session.proposal().unwrap();
        // 3 inputs of denom 1, 2 outputs of denom 1
        assert_eq!(p.inputs.iter().filter(|i| i.value == 1).count(), 3);
        assert_eq!(p.outputs.len(), 2);

        let in_sum: u64 = p.inputs.iter().map(|i| i.value).sum();
        let out_sum: u64 = p.outputs.iter().map(|o| o.value).sum();
        assert_eq!(in_sum - out_sum, 1);
    }

    // ── Accessor coverage ───────────────────────────────────────────────

    #[test]
    fn accessors() {
        let s = MixSession::new(16, 3).unwrap();
        assert_eq!(s.denomination(), 16);
        assert_eq!(s.participant_count(), 0);
        assert!(!s.has_fee_input());
        assert!(!s.is_ready());
    }
}




══════════════════════════════════════════════════════════════
 FILE: wallet/crypto.rs
══════════════════════════════════════════════════════════════

use aes_gcm::{
    aead::{Aead, KeyInit},
    Aes256Gcm, Nonce,
};
use anyhow::{bail, Result};
use argon2::Argon2;

const SALT_LEN: usize = 16;
const NONCE_LEN: usize = 12;

/// Derive a 32-byte key from password + salt via Argon2id.
///
/// ```
/// # use midstate::wallet::crypto::*;
/// // Internal doc test
/// ```
fn derive_key(password: &[u8], salt: &[u8]) -> [u8; 32] {
    let mut key = [0u8; 32];
    Argon2::default()
        .hash_password_into(password, salt, &mut key)
        .expect("Argon2id KDF failed");
    key
}

/// Encrypt plaintext with a password.
/// Output: salt (16) || nonce (12) || ciphertext+tag
pub fn encrypt(plaintext: &[u8], password: &[u8]) -> Result<Vec<u8>> {
    let salt: [u8; SALT_LEN] = rand::random();
    let nonce_bytes: [u8; NONCE_LEN] = rand::random();

    let key = derive_key(password, &salt);
    let cipher = Aes256Gcm::new_from_slice(&key).unwrap();
    let nonce = Nonce::from_slice(&nonce_bytes);

    let ciphertext = cipher
        .encrypt(nonce, plaintext)
        .map_err(|e| anyhow::anyhow!("encryption failed: {}", e))?;

    let mut out = Vec::with_capacity(SALT_LEN + NONCE_LEN + ciphertext.len());
    out.extend_from_slice(&salt);
    out.extend_from_slice(&nonce_bytes);
    out.extend_from_slice(&ciphertext);
    Ok(out)
}

/// Decrypt ciphertext with a password.
pub fn decrypt(data: &[u8], password: &[u8]) -> Result<Vec<u8>> {
    if data.len() < SALT_LEN + NONCE_LEN + 16 {
        bail!("wallet file too short or corrupted");
    }

    let salt = &data[..SALT_LEN];
    let nonce_bytes = &data[SALT_LEN..SALT_LEN + NONCE_LEN];
    let ciphertext = &data[SALT_LEN + NONCE_LEN..];

    let key = derive_key(password, salt);
    let cipher = Aes256Gcm::new_from_slice(&key).unwrap();
    let nonce = Nonce::from_slice(nonce_bytes);

    cipher
        .decrypt(nonce, ciphertext)
        .map_err(|_| anyhow::anyhow!("wrong password or corrupted wallet"))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn argon2_derivation_deterministic() {
        let pwd = b"my_secure_password";
        let salt = b"random_salt_1234";
        let key1 = derive_key(pwd, salt);
        let key2 = derive_key(pwd, salt);
        assert_eq!(key1, key2);
        assert_ne!(key1, [0u8; 32]);
    }

    #[test]
    fn round_trip() {
        let data = b"test wallet data";
        let password = b"hunter2";
        let encrypted = encrypt(data, password).unwrap();
        let decrypted = decrypt(&encrypted, password).unwrap();
        assert_eq!(data.as_slice(), &decrypted);
    }

    #[test]
    fn wrong_password() {
        let encrypted = encrypt(b"secret", b"correct").unwrap();
        assert!(decrypt(&encrypted, b"wrong").is_err());
    }
    #[test]
    fn truncated_data_fails() {
        let encrypted = encrypt(b"hello", b"pass").unwrap();
        assert!(decrypt(&encrypted[..5], b"pass").is_err());
    }

    #[test]
    fn empty_data_fails() {
        assert!(decrypt(&[], b"pass").is_err());
    }

    #[test]
    fn large_payload_round_trip() {
        let data = vec![0xABu8; 100_000];
        let password = b"strong_password_123";
        let encrypted = encrypt(&data, password).unwrap();
        let decrypted = decrypt(&encrypted, password).unwrap();
        assert_eq!(data, decrypted);
    }
}




══════════════════════════════════════════════════════════════
 FILE: wallet/mod.rs
══════════════════════════════════════════════════════════════

pub mod coinjoin;
pub mod crypto;
use crate::core::{hash_concat, compute_commitment, compute_coin_id, compute_address, decompose_value, wots, OutputData, InputReveal};
use crate::core::mss::{self, MssKeypair};
use anyhow::{bail, Result};
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};

/// Default wallet location: ~/.midstate/wallet.dat
pub fn default_path() -> PathBuf {
    dirs::home_dir()
        .unwrap_or_else(|| PathBuf::from("."))
        .join(".midstate")
        .join("wallet.dat")
}

/// Short display: first 8 hex chars + "…" + last 4 hex chars
pub fn short_hex(bytes: &[u8; 32]) -> String {
    let h = hex::encode(bytes);
    format!("{}…{}", &h[..8], &h[60..])
}

/// A receiving key (seed + public key). No value assigned yet.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct WalletKey {
    pub seed: [u8; 32],
    pub owner_pk: [u8; 32],
    pub address: [u8; 32],
    pub label: Option<String>,
}

/// A coin the wallet controls, with known value.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct WalletCoin {
    pub seed: [u8; 32],
    pub owner_pk: [u8; 32],
    pub address: [u8; 32],
    pub value: u64,
    pub salt: [u8; 32],
    pub coin_id: [u8; 32],
    pub label: Option<String>,
}

/// A commit that has been submitted but not yet revealed.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct PendingCommit {
    pub commitment: [u8; 32],
    pub salt: [u8; 32],
    pub input_coin_ids: Vec<[u8; 32]>,
    /// Full output data needed for the reveal transaction.
    pub outputs: Vec<OutputData>,
    /// (output_index, wots_seed) for change outputs we control.
    pub change_seeds: Vec<(usize, [u8; 32])>,
    pub created_at: u64,
    #[serde(default)]
    pub reveal_not_before: u64,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct HistoryEntry {
    pub inputs: Vec<[u8; 32]>,
    pub outputs: Vec<[u8; 32]>,
    pub fee: u64,
    pub timestamp: u64,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct WalletData {
    /// Receiving keys (not yet associated with a value).
    #[serde(default)]
    pub keys: Vec<WalletKey>,
    /// Coins with known values that we can spend.
    pub coins: Vec<WalletCoin>,
    #[serde(default)]
    pub mss_keys: Vec<MssKeypair>,
    pub pending: Vec<PendingCommit>,
    #[serde(default)]
    pub history: Vec<HistoryEntry>,
    #[serde(default)]
    pub last_scan_height: u64,
}

impl WalletData {
    fn empty() -> Self {
        Self {
            keys: Vec::new(),
            coins: Vec::new(),
            mss_keys: Vec::new(),
            pending: Vec::new(),
            history: Vec::new(),
            last_scan_height: 0,
        }
    }
}

pub struct Wallet {
    path: PathBuf,
    password: Vec<u8>,
    pub data: WalletData,
}

impl Wallet {
    pub fn create(path: &Path, password: &[u8]) -> Result<Self> {
        if path.exists() {
            bail!("wallet file already exists: {}", path.display());
        }
        let wallet = Self {
            path: path.to_path_buf(),
            password: password.to_vec(),
            data: WalletData::empty(),
        };
        wallet.save()?;
        Ok(wallet)
    }
/// All addresses the wallet watches for (keys + coin addresses).
pub fn watched_addresses(&self) -> Vec<[u8; 32]> {
    let mut addrs: Vec<[u8; 32]> = self.data.keys.iter().map(|k| k.address).collect();
    addrs.extend(self.data.mss_keys.iter().map(|k| k.master_pk));
    addrs.sort();
    addrs.dedup();
    addrs
}

/// Import a scanned coin, matching it to a wallet key. Returns true if new.
pub fn import_scanned(&mut self, address: [u8; 32], value: u64, salt: [u8; 32]) -> Result<Option<[u8; 32]>> {
    let coin_id = compute_coin_id(&address, value, &salt);

    if self.data.coins.iter().any(|c| c.coin_id == coin_id) {
        return Ok(None); // already have it
    }

    // Find matching key
    if let Some(pos) = self.data.keys.iter().position(|k| k.address == address) {
        let key = self.data.keys.remove(pos);
        self.data.coins.push(WalletCoin {
            seed: key.seed,
            owner_pk: key.owner_pk,
            address: key.address,
            value,
            salt,
            coin_id,
            label: key.label,
        });
        return Ok(Some(coin_id));
    }

    // MSS key match — keep key, just add coin
    if let Some(mss) = self.data.mss_keys.iter().find(|k| k.master_pk == address) {
        self.data.coins.push(WalletCoin {
            seed: mss.master_seed,
            owner_pk: mss.master_pk,
            address,
            value,
            salt,
            coin_id,
            label: Some(format!("received ({})", value)),
        });
        return Ok(Some(coin_id));
    }

    // Detect coins sent to an already-consumed WOTS address (key was used for a previous coin).
    // WOTS keys are one-time — spending the first coin would expose the key if a second coin
    // were also spent, so the wallet intentionally consumed the key on first import.
    if self.data.coins.iter().any(|c| c.address == address) {
        tracing::warn!(
            "Coin {} (value {}) sent to already-used WOTS address {}. \
             This coin is UNRECOVERABLE — the one-time key was consumed by an earlier coin. \
             The sender should be asked to resend to a fresh address.",
            hex::encode(coin_id), value, hex::encode(address)
        );
        return Ok(None);
    }

    Ok(None) // address not ours
}
    pub fn open(path: &Path, password: &[u8]) -> Result<Self> {
        if !path.exists() {
            bail!("wallet file not found: {}", path.display());
        }
        let encrypted = std::fs::read(path)?;
        let plaintext = crypto::decrypt(&encrypted, password)?;
        let data: WalletData = serde_json::from_slice(&plaintext)?;
        Ok(Self {
            path: path.to_path_buf(),
            password: password.to_vec(),
            data,
        })
    }

    pub fn save(&self) -> Result<()> {
        if let Some(parent) = self.path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        let plaintext = serde_json::to_vec(&self.data)?;
        let encrypted = crypto::encrypt(&plaintext, &self.password)?;
        std::fs::write(&self.path, encrypted)?;
        Ok(())
    }

    // ── Key generation ──────────────────────────────────────────────────────

    /// Generate a new receiving key. Returns the owner_pk to share with the sender.
    pub fn generate_key(&mut self, label: Option<String>) -> Result<[u8; 32]> {
        let seed: [u8; 32] = rand::random();
        let owner_pk = wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        self.data.keys.push(WalletKey { seed, owner_pk, address, label });
        self.save()?;
        Ok(address)
    }

    /// Generate a new MSS tree (reusable address).
    pub fn generate_mss(&mut self, height: u32, _label: Option<String>) -> Result<[u8; 32]> {
        let seed: [u8; 32] = rand::random();
        let keypair = mss::keygen(&seed, height)?;
        let root = keypair.master_pk;
        self.data.mss_keys.push(keypair);
        self.save()?;
        Ok(root)
    }

    // ── Coin management ─────────────────────────────────────────────────────

    /// Import a coin with known seed, value, and salt.
    pub fn import_coin(
        &mut self,
        seed: [u8; 32],
        value: u64,
        salt: [u8; 32],
        label: Option<String>,
    ) -> Result<[u8; 32]> {
        let owner_pk = wots::keygen(&seed);
        let address = compute_address(&owner_pk);
        let coin_id = compute_coin_id(&address, value, &salt);
        if self.data.coins.iter().any(|c| c.coin_id == coin_id) {
            bail!("coin already in wallet");
        }
        self.data.coins.push(WalletCoin {
            seed, owner_pk, address, value, salt, coin_id, label,
        });
        // Remove matching key from unused keys if present
        self.data.keys.retain(|k| k.address != address);
        self.save()?;
        Ok(coin_id)
    }

    /// Find a coin by coin_id.
    pub fn find_coin(&self, coin_id: &[u8; 32]) -> Option<&WalletCoin> {
        self.data.coins.iter().find(|c| &c.coin_id == coin_id)
    }

    /// Find an MSS key by master_pk.
    pub fn find_mss(&self, pk: &[u8; 32]) -> Option<&MssKeypair> {
        self.data.mss_keys.iter().find(|k| &k.master_pk == pk)
    }

    /// Resolve a coin reference (index, hex prefix, or full hex).
    pub fn resolve_coin(&self, reference: &str) -> Result<[u8; 32]> {
        if let Ok(idx) = reference.parse::<usize>() {
            if idx < self.data.coins.len() {
                return Ok(self.data.coins[idx].coin_id);
            }
        }
        let reference_lower = reference.to_lowercase();
        for c in &self.data.coins {
            if hex::encode(c.coin_id).starts_with(&reference_lower) {
                return Ok(c.coin_id);
            }
        }
        bail!("no matching coin found");
    }

    pub fn coins(&self) -> &[WalletCoin] {
        &self.data.coins
    }

    pub fn keys(&self) -> &[WalletKey] {
        &self.data.keys
    }

    pub fn mss_keys(&self) -> &[MssKeypair] {
        &self.data.mss_keys
    }

    pub fn coin_count(&self) -> usize {
        self.data.coins.len()
    }

    pub fn total_value(&self) -> u64 {
        self.data.coins.iter().map(|c| c.value).sum()
    }

    // ── Transaction building ────────────────────────────────────────────────

    /// Select coins whose total value >= needed. Returns selected coin_ids.
    pub fn select_coins(&self, needed: u64, live_coins: &[[u8; 32]]) -> Result<Vec<[u8; 32]>> {
        let mut selected = Vec::new();
        let mut total = 0u64;
        // Sort by value descending to minimize number of inputs
        let mut available: Vec<&WalletCoin> = self.data.coins.iter()
            .filter(|c| live_coins.contains(&c.coin_id))
            .collect();
        available.sort_by(|a, b| b.value.cmp(&a.value));

        for coin in available {
            if total >= needed { break; }
            selected.push(coin.coin_id);
            total += coin.value;
        }

        if total < needed {
            bail!("insufficient funds: have {}, need {}", total, needed);
        }
        Ok(selected)
    }

    /// Build outputs for a send: recipient outputs + change outputs.
    /// Returns (all_outputs, change_seeds).
    pub fn build_outputs(
        &mut self,
        recipient_address: &[u8; 32],
        recipient_denominations: &[u64],
        change_value: u64,
    ) -> Result<(Vec<OutputData>, Vec<(usize, [u8; 32])>)> {
        let mut outputs = Vec::new();
        let mut change_seeds = Vec::new();

        // Recipient outputs
        for &denom in recipient_denominations {
            let salt: [u8; 32] = rand::random();
            outputs.push(OutputData {
                address: *recipient_address,
                value: denom,
                salt,
            });
        }

        // Change outputs (decompose into power-of-2 denominations to self)
        if change_value > 0 {
            let change_denoms = decompose_value(change_value);
            for denom in change_denoms {
                let seed: [u8; 32] = rand::random();
                let owner_pk = wots::keygen(&seed);
                let address = compute_address(&owner_pk);
                let salt: [u8; 32] = rand::random();
                let idx = outputs.len();
                outputs.push(OutputData { address, value: denom, salt });
                change_seeds.push((idx, seed));
            }
        }

        Ok((outputs, change_seeds))
    }

    /// Prepare a commit for given inputs and outputs.
    pub fn prepare_commit(
        &mut self,
        input_coin_ids: &[[u8; 32]],
        outputs: &[OutputData],
        change_seeds: Vec<(usize, [u8; 32])>,
        privacy_delay: bool,
    ) -> Result<([u8; 32], [u8; 32])> {
        // Verify we own all inputs
        for coin_id in input_coin_ids {
            if self.find_coin(coin_id).is_none() {
                bail!("coin {} not in wallet", short_hex(coin_id));
            }
        }

        let output_coin_ids: Vec<[u8; 32]> = outputs.iter().map(|o| o.coin_id()).collect();
        let salt: [u8; 32] = rand::random();
        let commitment = compute_commitment(input_coin_ids, &output_coin_ids, &salt);

        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        let reveal_not_before = if privacy_delay {
            now + 10 + (rand::random::<u64>() % 41)
        } else {
            0
        };

        self.data.pending.push(PendingCommit {
            commitment,
            salt,
            input_coin_ids: input_coin_ids.to_vec(),
            outputs: outputs.to_vec(),
            change_seeds,
            created_at: now,
            reveal_not_before,
        });
        self.save()?;

        Ok((commitment, salt))
    }

    /// Build InputReveals and signatures for a pending commit.
    pub fn sign_reveal(&mut self, pending: &PendingCommit) -> Result<(Vec<InputReveal>, Vec<Vec<u8>>)> {
        let output_coin_ids: Vec<[u8; 32]> = pending.outputs.iter().map(|o| o.coin_id()).collect();
        let commitment = compute_commitment(
            &pending.input_coin_ids,
            &output_coin_ids,
            &pending.salt,
        );

        let mut input_reveals = Vec::new();
        let mut signatures = Vec::new();

        for coin_id in &pending.input_coin_ids {
            // Try WOTS coin
            if let Some(wc) = self.find_coin(coin_id).cloned() {
                input_reveals.push(InputReveal {
                    owner_pk: wc.owner_pk,
                    value: wc.value,
                    salt: wc.salt,
                });
                let sig = wots::sign(&wc.seed, &commitment);
                signatures.push(wots::sig_to_bytes(&sig));
            }
            // Try MSS key (coin's owner_pk matches an MSS master_pk)
            else if let Some(wc) = self.data.coins.iter().find(|c| &c.coin_id == coin_id) {
                // Found the coin data, but the seed might be for an MSS key
                if let Some(pos) = self.data.mss_keys.iter().position(|k| k.master_pk == wc.owner_pk) {
                    input_reveals.push(InputReveal {
                        owner_pk: wc.owner_pk,
                        value: wc.value,
                        salt: wc.salt,
                    });
                    let keypair = &mut self.data.mss_keys[pos];
                    if keypair.remaining() == 0 {
                        bail!("MSS key {} exhausted", short_hex(&wc.owner_pk));
                    }
                    let sig = keypair.sign(&commitment)?;
                    signatures.push(sig.to_bytes());
                } else {
                    bail!("key for {} not found", short_hex(coin_id));
                }
            } else {
                bail!("coin {} not found in wallet", short_hex(coin_id));
            }
        }

        self.save()?;
        Ok((input_reveals, signatures))
    }

    pub fn find_pending(&self, commitment: &[u8; 32]) -> Option<&PendingCommit> {
        self.data.pending.iter().find(|p| &p.commitment == commitment)
    }

    pub fn pending(&self) -> &[PendingCommit] {
        &self.data.pending
    }

    /// Complete a reveal: remove spent coins, add change coins.
    pub fn complete_reveal(&mut self, commitment: &[u8; 32]) -> Result<()> {
        let pending = self.data.pending.iter()
            .find(|p| &p.commitment == commitment)
            .ok_or_else(|| anyhow::anyhow!("pending commit not found"))?
            .clone();

        let spent_coin_ids = pending.input_coin_ids.clone();
        let fee: u64 = {
            let in_sum: u64 = spent_coin_ids.iter()
                .filter_map(|id| self.find_coin(id))
                .map(|c| c.value)
                .sum();
            let out_sum: u64 = pending.outputs.iter().map(|o| o.value).sum();
            in_sum.saturating_sub(out_sum)
        };

        // Remove spent coins
        self.data.coins.retain(|c| !spent_coin_ids.contains(&c.coin_id));

        // Add change coins
        for (idx, seed) in &pending.change_seeds {
            let out = &pending.outputs[*idx];
            let coin_id = out.coin_id();
            if !self.data.coins.iter().any(|c| c.coin_id == coin_id) {
                let owner_pk = wots::keygen(seed);
                self.data.coins.push(WalletCoin {
                    seed: *seed,
                    owner_pk,
                    address: out.address,
                    value: out.value,
                    salt: out.salt,
                    coin_id,
                    label: Some(format!("change ({})", out.value)),
                });
            }
        }

        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        self.data.history.push(HistoryEntry {
            inputs: spent_coin_ids,
            outputs: pending.outputs.iter().map(|o| o.coin_id()).collect(),
            fee,
            timestamp: now,
        });

        self.data.pending.retain(|p| &p.commitment != commitment);
        self.save()?;
        Ok(())
    }

    pub fn history(&self) -> &[HistoryEntry] {
        &self.data.history
    }

    /// Plan a private send: split into independent 2-in-1-out pairs.
    pub fn plan_private_send(
        &self,
        live_coins: &[[u8; 32]],
        recipient_address: &[u8; 32],
        denominations: &[u64],
    ) -> Result<Vec<(Vec<[u8; 32]>, Vec<OutputData>, Vec<(usize, [u8; 32])>)>> {
        // Each denomination gets its own independent transaction.
        // Each tx: 1+ inputs → 1 recipient output + change outputs.
        // Inputs must sum to > denomination.
        let mut used = std::collections::HashSet::new();
        let mut pairs = Vec::new();

        for &denom in denominations {
            // Find inputs covering denom + 1 (minimum fee)
            let needed = denom + 1;
            let mut selected = Vec::new();
            let mut total = 0u64;

            let mut available: Vec<&WalletCoin> = self.data.coins.iter()
                .filter(|c| live_coins.contains(&c.coin_id) && !used.contains(&c.coin_id))
                .collect();
            available.sort_by(|a, b| b.value.cmp(&a.value));

            for coin in available {
                if total >= needed { break; }
                selected.push(coin.coin_id);
                used.insert(coin.coin_id);
                total += coin.value;
            }

            if total < needed {
                bail!("insufficient funds for private send denomination {}", denom);
            }

            let change = total - denom - 1; // fee = 1
            let salt: [u8; 32] = rand::random();
            let mut outputs = vec![OutputData {
                address: *recipient_address,
                value: denom,
                salt,
            }];
            let mut change_seeds = Vec::new();

            if change > 0 {
                for cd in decompose_value(change) {
                    let seed: [u8; 32] = rand::random();
                    let pk = wots::keygen(&seed);
                    let addr = compute_address(&pk);
                    let cs: [u8; 32] = rand::random();
                    let idx = outputs.len();
                    outputs.push(OutputData { address: addr, value: cd, salt: cs });
                    change_seeds.push((idx, seed));
                }
            }

            pairs.push((selected, outputs, change_seeds));
        }

        Ok(pairs)
    }

    // ── CoinJoin mixing ─────────────────────────────────────────────────────

    /// Prepare a coin for CoinJoin mixing.
    ///
    /// Generates a fresh one-time address to receive the mixed output, and returns
    /// the `(InputReveal, OutputData, output_seed)` triple needed for registration
    /// with a [`coinjoin::MixSession`].
    ///
    /// The caller must hold `output_seed` until the mix completes, then pass it
    /// to [`complete_mix`] to import the received coin.
    pub fn prepare_mix_registration(
        &self,
        coin_id: &[u8; 32],
    ) -> Result<(InputReveal, OutputData, [u8; 32])> {
        let coin = self.find_coin(coin_id)
            .ok_or_else(|| anyhow::anyhow!("coin {} not in wallet", short_hex(coin_id)))?;

        let input = InputReveal {
            owner_pk: coin.owner_pk,
            value: coin.value,
            salt: coin.salt,
        };

        // Fresh one-time key for the output
        let output_seed: [u8; 32] = rand::random();
        let output_pk = wots::keygen(&output_seed);
        let output_address = compute_address(&output_pk);
        let output_salt: [u8; 32] = rand::random();

        let output = OutputData {
            address: output_address,
            value: coin.value,
            salt: output_salt,
        };

        Ok((input, output, output_seed))
    }

    /// Find and prepare a denomination-1 coin to pay the CoinJoin fee.
    ///
    /// Returns `(InputReveal, coin_id)` for the selected coin.
    pub fn prepare_mix_fee(
        &self,
        live_coins: &[[u8; 32]],
    ) -> Result<(InputReveal, [u8; 32])> {
        let coin = self.data.coins.iter()
            .find(|c| c.value == 1 && live_coins.contains(&c.coin_id))
            .ok_or_else(|| anyhow::anyhow!("no denomination-1 coin available for fee"))?;

        let input = InputReveal {
            owner_pk: coin.owner_pk,
            value: coin.value,
            salt: coin.salt,
        };
        Ok((input, coin.coin_id))
    }

    /// Sign a CoinJoin commitment for one of our coins.
    ///
    /// Returns the serialized signature bytes.
    pub fn sign_mix_input(
        &mut self,
        coin_id: &[u8; 32],
        commitment: &[u8; 32],
    ) -> Result<Vec<u8>> {
        // Try WOTS first
        if let Some(coin) = self.find_coin(coin_id).cloned() {
            if let Some(pos) = self.data.mss_keys.iter().position(|k| k.master_pk == coin.owner_pk) {
                let keypair = &mut self.data.mss_keys[pos];
                let sig = keypair.sign(commitment)?;
                self.save()?;
                return Ok(sig.to_bytes());
            }
            let sig = wots::sign(&coin.seed, commitment);
            return Ok(wots::sig_to_bytes(&sig));
        }
        bail!("coin {} not in wallet", short_hex(coin_id));
    }

    /// Complete a CoinJoin mix: remove spent coins and import the received output.
    pub fn complete_mix(
        &mut self,
        spent_coin_ids: &[[u8; 32]],
        output: &OutputData,
        output_seed: [u8; 32],
    ) -> Result<()> {
        self.data.coins.retain(|c| !spent_coin_ids.contains(&c.coin_id));

        let output_pk = wots::keygen(&output_seed);
        let coin_id = output.coin_id();
        if !self.data.coins.iter().any(|c| c.coin_id == coin_id) {
            self.data.coins.push(WalletCoin {
                seed: output_seed,
                owner_pk: output_pk,
                address: output.address,
                value: output.value,
                salt: output.salt,
                coin_id,
                label: Some(format!("mixed ({})", output.value)),
            });
        }

        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        self.data.history.push(HistoryEntry {
            inputs: spent_coin_ids.to_vec(),
            outputs: vec![coin_id],
            fee: 0, // fee donor pays, not us (unless we donated)
            timestamp: now,
        });

        self.save()?;
        Ok(())
    }
}

/// Deterministic coinbase seed derivation.
pub fn coinbase_seed(mining_seed: &[u8; 32], height: u64, index: u64) -> [u8; 32] {
    let height_key = hash_concat(mining_seed, &height.to_le_bytes());
    hash_concat(&height_key, &index.to_le_bytes())
}

/// Deterministic coinbase salt derivation (different domain from seed).
pub fn coinbase_salt(mining_seed: &[u8; 32], height: u64, index: u64) -> [u8; 32] {
    let height_key = hash_concat(mining_seed, &height.to_le_bytes());
    hash_concat(&height_key, &(index | 0x8000000000000000).to_le_bytes())
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::NamedTempFile;

    #[test]
    fn create_and_reopen() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let addr = w.generate_key(Some("test".into())).unwrap();
        assert_eq!(w.keys().len(), 1);
        assert_eq!(w.keys()[0].address, addr);

        let w2 = Wallet::open(&path, b"pass").unwrap();
        assert_eq!(w2.keys().len(), 1);
    }

    #[test]
    fn import_coin_and_find() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let seed: [u8; 32] = [0x42; 32];
        let salt: [u8; 32] = [0x11; 32];
        let coin_id = w.import_coin(seed, 16, salt, Some("test coin".into())).unwrap();

        assert_eq!(w.coin_count(), 1);
        let found = w.find_coin(&coin_id).unwrap();
        assert_eq!(found.value, 16);
    }

    #[test]
    fn total_value() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        w.import_coin([1u8; 32], 8, [2u8; 32], None).unwrap();
        w.import_coin([3u8; 32], 4, [4u8; 32], None).unwrap();
        assert_eq!(w.total_value(), 12);
    }

    #[test]
    fn short_hex_format() {
        let bytes = [0xab; 32];
        let s = short_hex(&bytes);
        assert_eq!(s, "abababab…abab");
    }

    // ── resolve_coin ────────────────────────────────────────────────────

    #[test]
    fn resolve_coin_by_index() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let seed = [0x42u8; 32];
        let salt = [0x11; 32];
        let coin_id = w.import_coin(seed, 16, salt, None).unwrap();
        assert_eq!(w.resolve_coin("0").unwrap(), coin_id);
    }

    #[test]
    fn resolve_coin_by_hex_prefix() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let coin_id = w.import_coin([0x42; 32], 8, [0x11; 32], None).unwrap();
        let prefix = &hex::encode(coin_id)[..8];
        assert_eq!(w.resolve_coin(prefix).unwrap(), coin_id);
    }

    #[test]
    fn resolve_coin_not_found() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let w = Wallet::create(&path, b"pass").unwrap();
        assert!(w.resolve_coin("99").is_err());
        assert!(w.resolve_coin("deadbeef").is_err());
    }

    // ── select_coins ────────────────────────────────────────────────────

    #[test]
    fn select_coins_minimal() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 4, [10; 32], None).unwrap();
        let c2 = w.import_coin([2; 32], 8, [20; 32], None).unwrap();
        let c3 = w.import_coin([3; 32], 16, [30; 32], None).unwrap();

        let live = vec![c1, c2, c3];
        // Need 9 → should select the 16-coin (largest first)
        let selected = w.select_coins(9, &live).unwrap();
        assert_eq!(selected.len(), 1);
        assert_eq!(selected[0], c3);
    }

    #[test]
    fn select_coins_insufficient() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 4, [10; 32], None).unwrap();
        assert!(w.select_coins(100, &[c1]).is_err());
    }

    #[test]
    fn select_coins_ignores_non_live() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 4, [10; 32], None).unwrap();
        let _c2 = w.import_coin([2; 32], 8, [20; 32], None).unwrap(); // not live

        let selected = w.select_coins(4, &[c1]).unwrap();
        assert_eq!(selected, vec![c1]);
    }

    // ── build_outputs ───────────────────────────────────────────────────

    #[test]
    fn build_outputs_with_change() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let dest = [0xAA; 32];
        let (outputs, change_seeds) = w.build_outputs(&dest, &[4, 2], 3).unwrap();

        // 2 recipient + decompose(3) = 1+2 = 2 change = 4 total
        let recipient_count = 2;
        let change_count = decompose_value(3).len(); // [1, 2]
        assert_eq!(outputs.len(), recipient_count + change_count);
        assert_eq!(change_seeds.len(), change_count);

        // First two are to recipient
        assert_eq!(outputs[0].address, dest);
        assert_eq!(outputs[0].value, 4);
        assert_eq!(outputs[1].address, dest);
        assert_eq!(outputs[1].value, 2);

        // Change values sum correctly
        let change_total: u64 = change_seeds.iter().map(|(idx, _)| outputs[*idx].value).sum();
        assert_eq!(change_total, 3);
    }

    #[test]
    fn build_outputs_no_change() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let (outputs, change_seeds) = w.build_outputs(&[0xBB; 32], &[8], 0).unwrap();
        assert_eq!(outputs.len(), 1);
        assert!(change_seeds.is_empty());
    }

    // ── coinbase_seed / coinbase_salt derivation ────────────────────────

    #[test]
    fn coinbase_seed_deterministic() {
        let ms = [0xAA; 32];
        assert_eq!(coinbase_seed(&ms, 100, 0), coinbase_seed(&ms, 100, 0));
    }

    #[test]
    fn coinbase_seed_varies_by_height() {
        let ms = [0xAA; 32];
        assert_ne!(coinbase_seed(&ms, 1, 0), coinbase_seed(&ms, 2, 0));
    }

    #[test]
    fn coinbase_seed_varies_by_index() {
        let ms = [0xAA; 32];
        assert_ne!(coinbase_seed(&ms, 1, 0), coinbase_seed(&ms, 1, 1));
    }

    #[test]
    fn coinbase_seed_differs_from_salt() {
        let ms = [0xAA; 32];
        assert_ne!(coinbase_seed(&ms, 1, 0), coinbase_salt(&ms, 1, 0));
    }

    // ── watched_addresses ───────────────────────────────────────────────

    #[test]
    fn watched_addresses_includes_keys_and_mss() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let addr1 = w.generate_key(None).unwrap();
        let mss_addr = w.generate_mss(4, None).unwrap();

        let watched = w.watched_addresses();
        assert!(watched.contains(&addr1));
        assert!(watched.contains(&mss_addr));
    }

    #[test]
    fn watched_addresses_deduped() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        w.generate_key(None).unwrap();
        w.generate_key(None).unwrap();
        let watched = w.watched_addresses();
        let mut sorted = watched.clone();
        sorted.sort();
        sorted.dedup();
        assert_eq!(watched.len(), sorted.len());
    }

    // ── import_scanned ──────────────────────────────────────────────────

    #[test]
    fn import_scanned_matches_key() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let addr = w.generate_key(Some("scan test".into())).unwrap();

        let salt = [0x55; 32];
        let value = 8u64;
        let result = w.import_scanned(addr, value, salt).unwrap();
        assert!(result.is_some());
        assert_eq!(w.coin_count(), 1);
        assert_eq!(w.keys().len(), 0); // key consumed
    }

    #[test]
    fn import_scanned_ignores_unknown_address() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let result = w.import_scanned([0xFF; 32], 8, [0; 32]).unwrap();
        assert!(result.is_none());
    }

    #[test]
    fn import_scanned_dedup() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let addr = w.generate_key(None).unwrap();
        let salt = [0x55; 32];

        w.import_scanned(addr, 8, salt).unwrap();
        // Second import same coin → None
        let result = w.import_scanned(addr, 8, salt).unwrap();
        assert!(result.is_none());
        assert_eq!(w.coin_count(), 1);
    }

    // ── generate_mss ────────────────────────────────────────────────────

    #[test]
    fn generate_mss_creates_key() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let root = w.generate_mss(4, Some("test mss".into())).unwrap();
        assert_ne!(root, [0u8; 32]);
        assert_eq!(w.mss_keys().len(), 1);
        assert_eq!(w.mss_keys()[0].height, 4);
    }

    // ── plan_private_send ───────────────────────────────────────────────

    #[test]
    fn plan_private_send_independent_pairs() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 8, [10; 32], None).unwrap();
        let c2 = w.import_coin([2; 32], 4, [20; 32], None).unwrap();
        let c3 = w.import_coin([3; 32], 16, [30; 32], None).unwrap();

        let live = vec![c1, c2, c3];
        let dest = [0xAA; 32];
        let pairs = w.plan_private_send(&live, &dest, &[4, 2]).unwrap();

        assert_eq!(pairs.len(), 2);
        // Each pair should have non-overlapping inputs
        let all_inputs: Vec<[u8; 32]> = pairs.iter()
            .flat_map(|(ins, _, _)| ins.clone())
            .collect();
        let mut deduped = all_inputs.clone();
        deduped.sort();
        deduped.dedup();
        assert_eq!(all_inputs.len(), deduped.len(), "inputs should not overlap between pairs");
    }

    #[test]
    fn plan_private_send_insufficient_funds() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 2, [10; 32], None).unwrap();
        assert!(w.plan_private_send(&[c1], &[0xAA; 32], &[4, 4]).is_err());
    }

    // ── CoinJoin helpers ────────────────────────────────────────────────

    #[test]
    fn prepare_mix_registration_produces_matching_denomination() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let coin_id = w.import_coin([1; 32], 8, [10; 32], None).unwrap();

        let (input, output, _seed) = w.prepare_mix_registration(&coin_id).unwrap();
        assert_eq!(input.value, 8);
        assert_eq!(output.value, 8);
        assert_eq!(input.coin_id(), coin_id);
    }

    #[test]
    fn prepare_mix_registration_unknown_coin_fails() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let w = Wallet::create(&path, b"pass").unwrap();
        assert!(w.prepare_mix_registration(&[0xFF; 32]).is_err());
    }

    #[test]
    fn prepare_mix_fee_finds_denomination_1() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 8, [10; 32], None).unwrap();
        let c2 = w.import_coin([2; 32], 1, [20; 32], None).unwrap();

        let (fee_input, fee_id) = w.prepare_mix_fee(&[c1, c2]).unwrap();
        assert_eq!(fee_input.value, 1);
        assert_eq!(fee_id, c2);
    }

    #[test]
    fn prepare_mix_fee_fails_without_denomination_1() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let c1 = w.import_coin([1; 32], 8, [10; 32], None).unwrap();
        assert!(w.prepare_mix_fee(&[c1]).is_err());
    }

    #[test]
    fn sign_mix_input_produces_valid_signature() {
        use crate::core::wots;

        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let seed = [0x42; 32];
        let coin_id = w.import_coin(seed, 8, [10; 32], None).unwrap();

        let commitment = crate::core::types::hash(b"test commitment");
        let sig_bytes = w.sign_mix_input(&coin_id, &commitment).unwrap();

        let coin = w.find_coin(&coin_id).unwrap();
        let sig = wots::sig_from_bytes(&sig_bytes).unwrap();
        assert!(wots::verify(&sig, &commitment, &coin.owner_pk));
    }

    #[test]
    fn sign_mix_input_unknown_coin_fails() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let commitment = [0; 32];
        assert!(w.sign_mix_input(&[0xFF; 32], &commitment).is_err());
    }

    #[test]
    fn complete_mix_removes_spent_adds_output() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let coin_id = w.import_coin([1; 32], 8, [10; 32], None).unwrap();
        let fee_id = w.import_coin([2; 32], 1, [20; 32], None).unwrap();
        assert_eq!(w.coin_count(), 2);

        let output_seed: [u8; 32] = [0x99; 32];
        let output_pk = crate::core::wots::keygen(&output_seed);
        let output_addr = crate::core::types::compute_address(&output_pk);
        let output = crate::core::OutputData {
            address: output_addr,
            value: 8,
            salt: [0xAA; 32],
        };

        w.complete_mix(&[coin_id, fee_id], &output, output_seed).unwrap();

        assert_eq!(w.coin_count(), 1);
        assert!(w.find_coin(&coin_id).is_none());
        assert!(w.find_coin(&fee_id).is_none());

        let new_coin = w.find_coin(&output.coin_id()).unwrap();
        assert_eq!(new_coin.value, 8);
        assert_eq!(new_coin.seed, output_seed);
    }

    #[test]
    fn complete_mix_persists() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        let mut w = Wallet::create(&path, b"pass").unwrap();
        let coin_id = w.import_coin([1; 32], 8, [10; 32], None).unwrap();

        let output_seed: [u8; 32] = [0x99; 32];
        let output_pk = crate::core::wots::keygen(&output_seed);
        let output_addr = crate::core::types::compute_address(&output_pk);
        let output = crate::core::OutputData {
            address: output_addr,
            value: 8,
            salt: [0xAA; 32],
        };

        w.complete_mix(&[coin_id], &output, output_seed).unwrap();
        let output_coin_id = output.coin_id();

        // Reopen
        let w2 = Wallet::open(&path, b"pass").unwrap();
        assert_eq!(w2.coin_count(), 1);
        assert!(w2.find_coin(&output_coin_id).is_some());
        assert_eq!(w2.history().len(), 1);
    }

    // ── wrong password ──────────────────────────────────────────────────

    #[test]
    fn open_wrong_password() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        Wallet::create(&path, b"correct").unwrap();
        assert!(Wallet::open(&path, b"wrong").is_err());
    }

    #[test]
    fn create_duplicate_path_fails() {
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap();

        Wallet::create(&path, b"pass").unwrap();
        assert!(Wallet::create(&path, b"pass").is_err());
    }
    
    #[test]
    fn test_mss_safety_recovery_persistence() {
        // 1. Setup: Create a wallet with an MSS key
        let file = NamedTempFile::new().unwrap();
        let path = file.path().to_path_buf();
        std::fs::remove_file(&path).unwrap(); // ensure it doesn't exist

        let mut w = Wallet::create(&path, b"password").unwrap();
        w.generate_mss(4, Some("test_mss".to_string())).unwrap();
        
        // Initial state: leaf index should be 0
        assert_eq!(w.mss_keys()[0].next_leaf, 0);

        // 2. Simulate the Fix:
        // The "network" tells us the index is actually 50.
        // We apply the fix logic: update internal state + safety margin.
        let remote_index = 50;
        let safety_margin = 20;
        let new_index = remote_index + safety_margin;

        // Apply fix directly to the data structure
        w.data.mss_keys[0].set_next_leaf(new_index);
        w.save().unwrap();

        // 3. Verify Persistence:
        // Close the wallet and reopen it from disk.
        let w_reloaded = Wallet::open(&path, b"password").unwrap();
        
        // The loaded wallet must have the updated index.
        assert_eq!(w_reloaded.mss_keys()[0].next_leaf, 70);
        
        // Ensure we didn't lose the key itself
        assert_eq!(w_reloaded.mss_keys().len(), 1);
        assert_eq!(w_reloaded.mss_keys()[0].height, 4);
    }
}


